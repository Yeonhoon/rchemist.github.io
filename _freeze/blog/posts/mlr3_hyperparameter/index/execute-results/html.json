{
  "hash": "783d816d91563f25c4bb3a0bae2e0822",
  "result": {
    "markdown": "---\ntitle: \"mlr3 하이퍼파라미터 최적화\"\ndescription: \"mlr3를 이용한 하이퍼파라미터 튜닝 학습\"\ndate: \"2023-03-01\"\ncategories: [R, mlr3, machine learning]\nimage: \"https://mlr3proba.mlr-org.com/logo.svg\"\n---\n\n\n::: callout-important\n이 글은 [mlr3book](https://mlr3book.mlr-org.com/)[^_mlr3_setup-1]을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 `mlr3`[^_mlr3_setup-2] 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다.\n:::\n\n[^_mlr3_setup-1]: https://mlr3book.mlr-org.com/\n\n[^_mlr3_setup-2]: https://mlr3.mlr-org.com/\n\n\n\n머신러닝 알고리즘은 보통 파라미터와 하이퍼파라미터를 포함하고 있습니다. 파라미터란 모델의 회귀계수나 가중치처럼 모델을 만들 때 필요한 매개변수입니다. 반면, 하이퍼파라미터는 사용자에 의해 구성됨으로써 파라미터가 어떻게 나올지를 결정합니다.\n\n대표적인 하이퍼파라미터의 예시로는 랜덤포레스트 알고리즘에서 나무의 개수를 정한다던가, 신경망의 학습률을 조정하는 것 등이 있습니다.\n\n하이퍼파라미터는 어떻게 설정하는지에 따라 모델의 성능을 향상시킬 수도, 그 반대가 될 수도 있습니다. 따라서 하이퍼파라미터를 최적화함으로써, 주어진 태스크에 대해 최적의 알고리즘 모델을 개발하는 것이 필요합니다.\n\n어쩌면 최적의 모델을 구성하는 것이 하나의 러너에 하이퍼 파라미터를 다르게 부여하는 벤치마크 실험을 통해 모델을 선택하는 것과 같다고 생각할 수 있습니다. 예를 들어 랜덤포레스트 모델들을 구성하는 나무의 개수를 다르게 정의하여 성능을 비교해본다고 해봅시다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nbmr <- benchmark(\n  benchmark_grid(\n    tasks= tsk(\"penguins_simple\"),\n    learners = list(\n      lrn(\"classif.ranger\", num.trees = 1, id=\"1 tree\"),\n      lrn(\"classif.ranger\", num.trees = 10, id=\"10 tree\"),\n      lrn(\"classif.ranger\", num.trees = 100, id=\"100 tree\")\n    ),\n    resamplings = rsmp(\"cv\", folds=3)\n  )\n)\n\nautoplot(bmr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n결과를 봤을 때, 나무가 100개로 구성된 랜덤포레스트 모델의 성능이 가장 좋은 것으로 나타났습니다. 다만 이렇게 임의적으로 시행착오를 거쳐 하이퍼파라미터를 조정해주는 것은 많은 시간이 필요한 것은 물론, 종종 편향되고 재생산성이 떨어집니다.\n\n지금까지 개발되어온 정교한 하이퍼파라미터 최적화 방법은 종료(termination) 시점까지 반복적으로 다양한 파라미터를 검토 후, 최적의 하이퍼파라미터 구성을 내놓는 효율적이고 로버스트한 결과를 출력합니다.\n\n# 모델 튜닝하기\n\nmlr3tuning 패키지를 통해 mlr3 생태계에서 하이퍼파라미터 최적화를 수행할 수 있습니다.\n\n-   TuningInstanceSingleCrit, TuningInstanceMultiCrit: 튜닝 인스턴스를 만들 때 사용.\n\n-   Tuner: 최적의 알고리즘을 불러오고 설정할 때 사용.\n\n## 러너의 학습공간 설정\n\n각각의 러너들은 하이퍼파라미터 세트를 갖고 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_svm <- lrn(\"classif.svm\")\nlrn_svm$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\nKey: <id>\n                 id    class lower upper nlevels\n             <char>   <char> <num> <num>   <num>\n 1:       cachesize ParamDbl  -Inf   Inf     Inf\n 2:   class.weights ParamUty    NA    NA     Inf\n 3:           coef0 ParamDbl  -Inf   Inf     Inf\n 4:            cost ParamDbl     0   Inf     Inf\n 5:           cross ParamInt     0   Inf     Inf\n 6: decision.values ParamLgl    NA    NA       2\n 7:          degree ParamInt     1   Inf     Inf\n 8:         epsilon ParamDbl     0   Inf     Inf\n 9:          fitted ParamLgl    NA    NA       2\n10:           gamma ParamDbl     0   Inf     Inf\n11:          kernel ParamFct    NA    NA       4\n12:              nu ParamDbl  -Inf   Inf     Inf\n13:           scale ParamUty    NA    NA     Inf\n14:       shrinking ParamLgl    NA    NA       2\n15:       tolerance ParamDbl     0   Inf     Inf\n16:            type ParamFct    NA    NA       2\n                                                                                      default\n                                                                                       <list>\n 1:                                                                                        40\n 2:                                                                                          \n 3:                                                                                         0\n 4:                                                                                         1\n 5:                                                                                         0\n 6:                                                                                     FALSE\n 7:                                                                                         3\n 8:                                                                                       0.1\n 9:                                                                                      TRUE\n10: <NoDefault>\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n11:                                                                                    radial\n12:                                                                                       0.5\n13:                                                                                      TRUE\n14:                                                                                      TRUE\n15:                                                                                     0.001\n16:                                                                          C-classification\n    parents  value\n     <list> <list>\n 1:               \n 2:               \n 3:  kernel       \n 4:    type       \n 5:               \n 6:               \n 7:  kernel       \n 8:               \n 9:               \n10:  kernel       \n11:               \n12:    type       \n13:               \n14:               \n15:               \n16:               \n```\n:::\n:::\n\n\n## Hyperparameter tuning {#sec-hyperparameter}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf$param_set\nrequire(paradox)\nlrn_rf <- lrn('classif.ranger',\n              max.depth=to_tune(10,50),\n              mtry = to_tune(5,20),\n              num.trees = to_tune(30,100)\n              )\n\n# ti: TuningInstance\ninstance <- ti(\n  task = task_sonar,\n  learner = lrn_rf,\n  resampling = rsmp('cv',folds=5),\n  measures = msrs(c('classif.sensitivity',\n                    'classif.specificity')),\n  terminator = trm('evals',n_evals=100)\n)\ninstance\ntuner = tnr('random_search')\n\ntuner$optimize(instance)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf$param_set$values <-  instance$result_learner_param_vals[[1]]\n\nlrn_rf\n```\n:::\n\n\n### AutoTuner\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_xgb <- lrn('classif.xgboost',\n               eta = to_tune(1e-4,1e-2),\n               gamma=to_tune(1e-3,1e-2),\n               max_depth=to_tune(10,50),\n               predict_type='prob'\n               )\n\nat <- auto_tuner(\n  method=tnr('random_search'),\n  learner = lrn_xgb,\n  resampling = rsmp('cv',folds=5),\n  measure = msr('classif.auc'),\n  term_evals = 30\n)\n\nat$train(task_sonar)\n```\n:::\n\n\n### Nested Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nouter_resampling <- rsmp('cv',folds=3)\n\nrr <- resample(task_sonar, at, outer_resampling, store_models = T)\n\nextract_inner_tuning_results(rr)\nrr$score(measures)\nrr$aggregate(measures)\n```\n:::\n\n\n## 5. Feature selection\n\n### 5.1. Introduction\n\nfeature selection의 장점\n\n1.  과적합(overfitting) 감소로 인한 성능 향상\n2.  불필요한 feature에 의존하지 않는 안정된(robust) 모델\n3.  간단함으로 인한 해석의 용이함\n4.  잠재적으로 값비싼 feature 수집 불필요\n\n### 5.2. Filters\n\n#### 5.2.1 Filter value 계산\n\n`mlr3filters::mlr_filters` dictionary 통해 또는 `mlr3filters::flt()` 를 이용해 filter 선택 가능.\n\nFilter 클래스에는 `$calculate()` 메소드가 존재하는데, filter value와 등수를 내림차순 정렬하여 보여준다.\n\n예를 들어 information gain filter를 이용하는 경우,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nfilter <- flt('information_gain')\ntask <- tsk('penguins')\ntask\nfilter$calculate(task)\nfilter\n```\n:::\n\n\n일부 filter들은 hyperparameters가 존재하는데, learner에서 param_set을 변경해주는 것처럼 간단히 변경 가능합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter_cor <- flt('correlation')\nfilter_cor$param_set$values <- list(method='spearman')\nfilter_cor$param_set\n```\n:::\n\n\n#### 5.2.2. Feature importance filters\n\nimportance 가 있는 모든 모델에서 사용 가능한 filter입니다. `ranger`와 같은 일부 learner에서는 learner를 만들 때 지정을 해줘야 합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf <- lrn('classif.ranger', importance='impurity')\n\n# remove missing values\ntask$filter(which(complete.cases(task$data())))\n\nfilter_imp <- flt('importance', learner=lrn_rf)\nfilter_imp$calculate(task)\nfilter_imp\n```\n:::\n\n\n#### 5.2.3. Embedded methods\n\nEmbedded methods는 Learner들로 하여금 예측에 중요한 변수들을 선택하는 방법입니다. 많은 learner들이 이 기능을 갖고 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\n\nstopifnot('selected_features' %in% learner$properties)\n\nlearner$train(task)\nlearner$selected_features()\n\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\nfilter\n```\n:::\n\n\nmodel에 의해 선택된 feature의 점수만 1, 나머지는 0 (dropped features)\n\n#### 5.2.4. Filter-based feature selection\n\nfilter를 통해 각 feature들의 점수가 계산이 되었다면, 다음 모델링 단계에서 feature를 선택하여 학습을 시켜주어야 합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\n\nkeep <- names(which(filter$scores==1))\ntask$select(keep) # column을 선택하기 때문에 select\ntask$feature_names\n```\n:::\n\n\n위의 예시에서는 selected_features를 했기 때문에 0과 1로 구분이 되었지만, 연속형 점수에 대해 filtering 을 할 때는 다음과 같은 방법이 있습니다.\n\n-   위에서 top N개의 feature 선택하는 경우\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# top 3개 선택\nkeep <- names(head(filter$scores,3))\ntask$select(keep)\n```\n:::\n\n\n-   score가 k 보다 큰 경우\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# information gain이 0.5보다 큰 경우\nkeep <- names(which(filter$scores>0.5))\ntask$select(keep)\n```\n:::\n\n\n### 5.3 Wrapper methods\n\n모델의 성능을 최적화하는 feature들을 반복적으로 선택합니다. feature들에 순위를 매기는 대신, 일부 feature들만을 사용하여 학습한 뒤, 선택된 성능 지표에 따라 평가하게 됩니다.\n\n#### 5.3.1. 간단한 예시\n\nmlr3에서는 `FSelector`를 이용하여 위의 방법을 수행합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3fselect)\ninstance <- fselect(\n  method='sequential',\n  task= tsk('penguins'),\n  learner = lrn('classif.rpart'),\n  resampling = rsmp('holdout'),\n  measures = msr('classif.acc')\n)\n```\n:::\n\n\n성능 비교를 위한 feature들의 모든 subset을 확인하기 위해선 아래의 코드로 확인 가능합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(instance$archive) \n```\n:::\n\n\n최적의 feature 들을 확인하기 위해서는\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstance$result_feature_set\n```\n:::\n\n\n#### 5.3.2 FSelectInstance\n\n#### 5.3.3. Fselector 클래스\n\n`mlr3fselect::FSelector`에는 다양한 종류의 feature 선택 알고리즘이 존재합니다.\n\n-   Random Search ([`mlr3fselect::FSelectorRandomSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_random_search.html))\n-   Exhaustive Search ([`mlr3fselect::FSelectorExhaustiveSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_exhaustive_search.html))\n-   Sequential Search ([`mlr3fselect::FSelectorSequential`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_sequential.html))\n-   Recursive Feature Elimination ([`mlr3fselect::FSelectorRFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html))\n-   Design Points ([`mlr3fselect::FSelectorDesignPoints`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_design_points.html))\n-   Genetic Search ([`mlr3fselect::FSelectorGeneticSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_genetic_search.html))\n-   Shadow Variable Search ([`mlr3fselect::FSelectorShadowVariableSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html))\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}