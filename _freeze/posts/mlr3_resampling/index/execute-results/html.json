{
  "hash": "025c50a2ec6e75565a9b6233d1e1cc2f",
  "result": {
    "markdown": "---\ntitle: 'mlr3 리샘플링 벤치마킹'\ndescription: \"여러 모델 동시 학습 및 성능비교\"\ndate: \"2023-02-24\"\ncategories: [mlr3, R, machine learning]\nimage: \"https://mlr3proba.mlr-org.com/logo.svg\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"data.table\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInstalling data.table [1.14.8] ...\n\tOK [linked cache]\n```\n:::\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(mlr3)\nlibrary(mlr3verse)\nlibrary(mlr3viz)\n```\n:::\n\n\n# Resampling\n\n특정한 데이터로 훈련된 머신러닝 모델은 새로운 데이터에도 일반화시킬 수 있어야 합니다. 다시 말해 학습 데이터로 훈련된 모델이 좋은 성능을 낸다면, 다른 데이터를 통해서도 좋은 성능이 나와야 한다는 것입니다.\n\n모델의 성능의 일반화를 평가하기 위해선 우선 모델의 성능을 평가할 지표를 선택해야 합니다. 목적에 따라 정확도가 될 수도 있고, auc 가 될 수도 있겠죠.\n\n적절한 평가지표를 선택했다면, 일반화 평가를 위해 사용 가능한 데이터를 어떻게 사용할지를 정해주어야 합니다. 이 때 모델 학습에 사용된 데이터는 사용되어서는 안됩니다. 동일한 데이터로 검증해봤자 동일한 성능이 나올테니까요. 훈련데이터서는 좋은 성능을 보여주었지만, 새로운 데이터에서는 예측 성능이 나빠질 수도 있구요 (과적합, Overfitting).\n\n따라서, 학습에 사용된 training 데이터를 사용하지 않고 모델의 성능을 테스트하는 것이 일반적입니다. 그러나 학습에 사용된 데이터와 동일한 피처, 동일한 형태를 갖고 있는 새로운 데이터가 없는 것이 일반적입니다. 이 문제를 해결하기 위해, 기존에 학습했던 데이터의 일부분만을 사용하여 평가에 사용하게 되었습니다. 그래서 전체 데이터에서 training 데이터, test 데이터를 나눈 뒤, train 데이터로만 학습을 하고, test 데이터는 새로운 데이터인 것처럼 간주하고 학습된 모델의 성능을 평가하는 것입니다.\n\n가장 흔한 전략은 홀드아웃(holdout) 전략입니다. 데이터에서 특정 비율로 랜덤하게 데이터를 나누어 training 데이터와 test 데이터로 나누는 것이죠.\n\n이상적으로는 training 데이터가 굉장히 커서, 학습 시 데이터의 특성을 대표할 수 있는 것입니다. test 데이터더 신뢰할 수 있는 일반화를 할 수 있도록 큰 데이터가 필요할테구요. 그렇지만 문제는 역시 사용할 수 있는 데이터가 제한됐다는 것입니다.\n\n이 문제를 해결할 수 있는 전략이 바로 Resampling 전략입니다. Resampling 전략은 가능한 모든 데이터를 여러 개의 training과 test 데이터로 반복적으로 나누어 학습을 하고, 모델의 성능의 평균을 계산하는 전략입니다. 반복하는 횟수가 많으면 많을수록 모델의 성능에 대한 분산이 감소할테고, 그러면 더욱 신뢰할 수 있는 성능을 확인할 수 있을 것입니다.\n\n## Resampling 종류\n\n`mlr3` 에서 실행가능한 모든 리샘플링 전략은 `mlr_resamplings` 딕셔너리를 통해 확인가능합니다. 홀드아웃, 교차 검증(CV), 부트스트랩 등이 포함되어있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(mlr_resamplings)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|key         |label                         |params           | iters|\n|:-----------|:-----------------------------|:----------------|-----:|\n|bootstrap   |Bootstrap                     |ratio  , repeats |    30|\n|custom      |Custom Splits                 |                 |    NA|\n|custom_cv   |Custom Split Cross-Validation |                 |    NA|\n|cv          |Cross-Validation              |folds            |    10|\n|holdout     |Holdout                       |ratio            |     1|\n|insample    |Insample Resampling           |                 |     1|\n|loo         |Leave-One-Out                 |                 |    NA|\n|repeated_cv |Repeated Cross-Validation     |folds  , repeats |   100|\n|subsampling |Subsampling                   |ratio  , repeats |    30|\n\n</div>\n:::\n:::\n\n\n`params` 열을 보면 resampling을 위한 파라미터들이 나와있습니다. 예를 들어 `holdout`은 `ratio`를 통해 어떤 비율로 train, test를 나눌 것인지, `cv`는 `folds`를 통해 몇 개로 데이터를 나눌 것인지를 설정해줄 수 있죠.\n\n## Resampling 객체 생성\n\n리샘플링 객체를 만들어봅시다. 우선 holdout을 이용해 리샘플링을 진행하겠습니다. 리샘플링 객체는 `rsmp()`를 통해 만들 수 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampling <- rsmp(\"holdout\")\nresampling\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n```\n:::\n:::\n\n\n생성한 리샘플링 객체를 확인했을 때, `Instantiated: FALSE`라고 되어있습니다. 아직 리샘플링을 수행하지 않았기 때문입니다.\n\n또한 holdout 의 ratio를 정해주지 않았기 문에 2/3가 초기값으로 설정되어있습니다. 즉 데이터의 3분의 2는 훈련에, 3분의 1은 검증에 쓰이게 됩니다. 새로운 리샘플링 객체를 만들 때 holdout 비율을 설정하거나 기존의 객체의 파라미터 값을 수정할 수 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampling <- rsmp(\"holdout\", ratio=0.8)\nresampling$param_set$values <- list(ratio=0.5)\n```\n:::\n\n\nholdout은 성능을 일반화하는 과정을 한 번밖에 수행하지 않습니다. 따라서 좀더 신뢰성 있는 성능 측정을 위해, 가장 많이 사용되는 리샘플링 방법 중 하나인 교차 검증(`cv`)를 사용하도록 하겠습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresampling <- rsmp(\"cv\", folds=10)\n```\n:::\n\n\n## Instantiation\n\n데이터가 들어가 있는 태스크에 대해 리샘플링을 수행하기 위해 리샘플링 객체의 `$instantiate()` 메소드를 이용해야합니다. 메소드 안에 태스크를 넣어주면 리샘플링이 적용됐다는 의미에서 `Instantiated: TRUE`가 출력됩니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk(\"sonar\")\nresampling$instantiate(task)\nresampling\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResamplingCV>: Cross-Validation\n* Iterations: 10\n* Instantiated: TRUE\n* Parameters: folds=10\n```\n:::\n:::\n\n\n## 실행\n\n리샘플리의 실행은 `resample()` 을 사용합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlearner <- lrn(\"classif.rpart\", predict_type=\"prob\")\nrr <- resample(task, learner, resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nINFO  [18:04:35.776] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 1/10)\nINFO  [18:04:35.854] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 2/10)\nINFO  [18:04:35.890] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 3/10)\nINFO  [18:04:35.920] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 4/10)\nINFO  [18:04:35.954] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 5/10)\nINFO  [18:04:35.986] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 6/10)\nINFO  [18:04:36.019] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 7/10)\nINFO  [18:04:36.050] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 8/10)\nINFO  [18:04:36.081] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 9/10)\nINFO  [18:04:36.131] [mlr3] Applying learner 'classif.rpart' on task 'sonar' (iter 10/10)\n```\n:::\n\n```{.r .cell-code}\nrr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 10 iterations\n* Task: sonar\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n:::\n\n\n리샘플링 실행이 완료되었고, `rr`이라는 객체 안에 리샘플링 결과가 저장되어있습니다. 리샘플링을 통한 모델의 성능 평가는 `$score()`와 `$aggregate()` 메소드를 이용합니다.\n\n`$score()`와 `$aggregate()` 모두 `Measure` 객체를 이용해 성능을 측정합니다. 기본적으로 성능을 측정할 때는 검증을 위한 데이터셋을 활용하게 됩니다.\n\n우선 정확도(`classif.acc`)를 통해 모델의 정확도를 평가해보겠습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc <- rr$score(msr(\"classif.acc\"))\nacc[,.(iteration, classif.acc)]\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| iteration| classif.acc|\n|---------:|-----------:|\n|         1|   0.7619048|\n|         2|   0.6666667|\n|         3|   0.6190476|\n|         4|   0.7142857|\n|         5|   0.7619048|\n|         6|   0.7619048|\n|         7|   0.8571429|\n|         8|   0.7619048|\n|         9|   0.7500000|\n|        10|   0.7000000|\n\n</div>\n:::\n:::\n\n\n`$score()`를 통해 모델의 성능을 살펴보니 cv에서 설정한 10번의 반복 별로 성능이 출력되는 것을 알 수 있습니다.\n\n다음으로 `$aggregate()`를 이용해 모델의 성능을 살펴보겠습니다. `$aggregate()`는 반복 때마다 계산된 성능의 평균 값을 계산하여 출력합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.acc \n  0.7354762 \n```\n:::\n:::\n\n\n여러 개의 평가 지표로 모델의 성능을 평가할 수도 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeasures <- msrs(c(\"classif.acc\",\n                   \"classif.sensitivity\",\n                   \"classif.specificity\",\n                   \"classif.auc\"))\n\nrr$aggregate(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        classif.acc classif.sensitivity classif.specificity         classif.auc \n          0.7354762           0.7388967           0.7439880           0.7691579 \n```\n:::\n:::\n\n\n## 결과 시각화\n\n`mlr3viz`의 `autoplot()`을 통해 리샘플링 결과를 시각화할 수 있습니다.\n\n\n::: {.cell layout=\"[[45,-10,45]]\" output-location='fragment'}\n\n```{.r .cell-code}\nrequire(mlr3viz)\nautoplot(rr, measure = msr(\"classif.acc\"), type=\"boxplot\")\n```\n\n::: {.cell-output-display}\n![Boxplot](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\nautoplot(rr, measure = msr(\"classif.acc\"), type=\"histogram\")\n```\n\n::: {.cell-output-display}\n![Histogram](index_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n## 4. Benchmarking\n\n벤치마킹은 다른 러너 또는 여러 태스크들을 각기 다른 리샘플링 방법을 이용해 학습시킬 수 있는 방법입니다. 지금까지는 하나의 태스크에 하나의 러너, 하나의 리샘플링을 해주었다면, 벤치마킹을 이용해서는 여러 가지의 학습을 한번에 수행할 수 있는 것이지요.\n\n이 벤치마킹은 여러 가지 러너와 리샘플링을 비교함으로써 성능별로 좋은 러너를 나열하여 쉽게 비교할 수 있다는 장점이 있습니다.\n\n### 벤치마킹 설계하기\n\n### 벤치마크 실험하기\n\n### 벤치마크 객체 전환하기\n\n### 1) Design\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3learners)\ndesign <- benchmark_grid(\n  tasks = tsk('sonar'),\n  learners = lrns(c('classif.log_reg','classif.ranger'),\n                  predict_type='prob',\n                  predict_sets=c('train','test')),\n  resamplings = rsmps('cv',folds=5)\n)\ndesign\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n|task                              |learner                           |resampling                        |\n|:---------------------------------|:---------------------------------|:---------------------------------|\n|<environment: 0x0000022a6af044a0> |<environment: 0x0000022a6b17f480> |<environment: 0x0000022a64090610> |\n|<environment: 0x0000022a6af044a0> |<environment: 0x0000022a6bea6d28> |<environment: 0x0000022a64090610> |\n\n</div>\n:::\n:::\n\n\n### 2) Execution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbmr <- benchmark(design, store_models = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nINFO  [18:04:37.540] [mlr3] Running benchmark with 10 resampling iterations\nINFO  [18:04:37.546] [mlr3] Applying learner 'classif.log_reg' on task 'sonar' (iter 1/5)\nINFO  [18:04:37.613] [mlr3] Applying learner 'classif.log_reg' on task 'sonar' (iter 2/5)\nINFO  [18:04:37.679] [mlr3] Applying learner 'classif.log_reg' on task 'sonar' (iter 3/5)\nINFO  [18:04:37.746] [mlr3] Applying learner 'classif.log_reg' on task 'sonar' (iter 4/5)\nINFO  [18:04:37.834] [mlr3] Applying learner 'classif.log_reg' on task 'sonar' (iter 5/5)\nINFO  [18:04:37.900] [mlr3] Applying learner 'classif.ranger' on task 'sonar' (iter 1/5)\nINFO  [18:04:38.741] [mlr3] Applying learner 'classif.ranger' on task 'sonar' (iter 2/5)\nINFO  [18:04:38.865] [mlr3] Applying learner 'classif.ranger' on task 'sonar' (iter 3/5)\nINFO  [18:04:38.993] [mlr3] Applying learner 'classif.ranger' on task 'sonar' (iter 4/5)\nINFO  [18:04:39.118] [mlr3] Applying learner 'classif.ranger' on task 'sonar' (iter 5/5)\nINFO  [18:04:39.243] [mlr3] Finished benchmark\n```\n:::\n\n```{.r .cell-code}\nmeasures <- list(\n  msr('classif.acc',id='accuracy'),\n  msr('classif.auc',id='AUC'),\n  msr('classif.prauc',id='PRC')\n)\n\nbmr$aggregate(measures)\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| nr|resample_result                   |task_id |learner_id      |resampling_id | iters|  accuracy|       AUC|       PRC|\n|--:|:---------------------------------|:-------|:---------------|:-------------|-----:|---------:|---------:|---------:|\n|  1|<environment: 0x0000022a5f0d3ec0> |sonar   |classif.log_reg |cv            |     5| 0.7406504| 0.7650445| 0.7683154|\n|  2|<environment: 0x0000022a5f0f1c68> |sonar   |classif.ranger  |cv            |     5| 0.8415796| 0.9235181| 0.9333841|\n\n</div>\n:::\n:::\n\n\n### 3) Plotting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(bmr, type='roc')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}