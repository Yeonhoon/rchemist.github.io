{
  "hash": "94fe54f4442b666c586967c9a4846f38",
  "result": {
    "markdown": "---\ntitle: \"mlr3 하이퍼파라미터 튜닝\"\ndescription: \"mlr3를 이용한 하이퍼파라미터 튜닝 학습\"\ndate: \"\"\ncategories: [R, mlr3, machine learning]\nimage: \"https://mlr3proba.mlr-org.com/logo.svg\"\n---\n\n\n::: callout-important\n이 글은 [mlr3book](https://mlr3book.mlr-org.com/)[^_mlr3_setup-1]을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 `mlr3`[^_mlr3_setup-2] 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다.\n:::\n\n[^_mlr3_setup-1]: https://mlr3book.mlr-org.com/\n\n[^_mlr3_setup-2]: https://mlr3.mlr-org.com/\n\n\n\n## 4. Hyperparameter tuning {#sec-hyperparameter}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf$param_set\nrequire(paradox)\nlrn_rf <- lrn('classif.ranger',\n              max.depth=to_tune(10,50),\n              mtry = to_tune(5,20),\n              num.trees = to_tune(30,100)\n              )\n\n# ti: TuningInstance\ninstance <- ti(\n  task = task_sonar,\n  learner = lrn_rf,\n  resampling = rsmp('cv',folds=5),\n  measures = msrs(c('classif.sensitivity',\n                    'classif.specificity')),\n  terminator = trm('evals',n_evals=100)\n)\ninstance\ntuner = tnr('random_search')\n\ntuner$optimize(instance)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf$param_set$values <-  instance$result_learner_param_vals[[1]]\n\nlrn_rf\n```\n:::\n\n\n### AutoTuner\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_xgb <- lrn('classif.xgboost',\n               eta = to_tune(1e-4,1e-2),\n               gamma=to_tune(1e-3,1e-2),\n               max_depth=to_tune(10,50),\n               predict_type='prob'\n               )\n\nat <- auto_tuner(\n  method=tnr('random_search'),\n  learner = lrn_xgb,\n  resampling = rsmp('cv',folds=5),\n  measure = msr('classif.auc'),\n  term_evals = 30\n)\n\nat$train(task_sonar)\n```\n:::\n\n\n### Nested Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nouter_resampling <- rsmp('cv',folds=3)\n\nrr <- resample(task_sonar, at, outer_resampling, store_models = T)\n\nextract_inner_tuning_results(rr)\nrr$score(measures)\nrr$aggregate(measures)\n```\n:::\n\n\n## 5. Feature selection\n\n### 5.1. Introduction\n\nfeature selection의 장점\n\n1.  과적합(overfitting) 감소로 인한 성능 향상\n2.  불필요한 feature에 의존하지 않는 안정된(robust) 모델\n3.  간단함으로 인한 해석의 용이함\n4.  잠재적으로 값비싼 feature 수집 불필요\n\n### 5.2. Filters\n\n#### 5.2.1 Filter value 계산\n\n`mlr3filters::mlr_filters` dictionary 통해 또는 `mlr3filters::flt()` 를 이용해 filter 선택 가능.\n\nFilter 클래스에는 `$calculate()` 메소드가 존재하는데, filter value와 등수를 내림차순 정렬하여 보여준다.\n\n예를 들어 information gain filter를 이용하는 경우,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nfilter <- flt('information_gain')\ntask <- tsk('penguins')\ntask\nfilter$calculate(task)\nfilter\n```\n:::\n\n\n일부 filter들은 hyperparameters가 존재하는데, learner에서 param_set을 변경해주는 것처럼 간단히 변경 가능합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter_cor <- flt('correlation')\nfilter_cor$param_set$values <- list(method='spearman')\nfilter_cor$param_set\n```\n:::\n\n\n#### 5.2.2. Feature importance filters\n\nimportance 가 있는 모든 모델에서 사용 가능한 filter입니다. `ranger`와 같은 일부 learner에서는 learner를 만들 때 지정을 해줘야 합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrn_rf <- lrn('classif.ranger', importance='impurity')\n\n# remove missing values\ntask$filter(which(complete.cases(task$data())))\n\nfilter_imp <- flt('importance', learner=lrn_rf)\nfilter_imp$calculate(task)\nfilter_imp\n```\n:::\n\n\n#### 5.2.3. Embedded methods\n\nEmbedded methods는 Learner들로 하여금 예측에 중요한 변수들을 선택하는 방법입니다. 많은 learner들이 이 기능을 갖고 있습니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\n\nstopifnot('selected_features' %in% learner$properties)\n\nlearner$train(task)\nlearner$selected_features()\n\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\nfilter\n```\n:::\n\n\nmodel에 의해 선택된 feature의 점수만 1, 나머지는 0 (dropped features)\n\n#### 5.2.4. Filter-based feature selection\n\nfilter를 통해 각 feature들의 점수가 계산이 되었다면, 다음 모델링 단계에서 feature를 선택하여 학습을 시켜주어야 합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\n\nkeep <- names(which(filter$scores==1))\ntask$select(keep) # column을 선택하기 때문에 select\ntask$feature_names\n```\n:::\n\n\n위의 예시에서는 selected_features를 했기 때문에 0과 1로 구분이 되었지만, 연속형 점수에 대해 filtering 을 할 때는 다음과 같은 방법이 있습니다.\n\n-   위에서 top N개의 feature 선택하는 경우\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# top 3개 선택\nkeep <- names(head(filter$scores,3))\ntask$select(keep)\n```\n:::\n\n\n-   score가 k 보다 큰 경우\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# information gain이 0.5보다 큰 경우\nkeep <- names(which(filter$scores>0.5))\ntask$select(keep)\n```\n:::\n\n\n### 5.3 Wrapper methods\n\n모델의 성능을 최적화하는 feature들을 반복적으로 선택합니다. feature들에 순위를 매기는 대신, 일부 feature들만을 사용하여 학습한 뒤, 선택된 성능 지표에 따라 평가하게 됩니다.\n\n#### 5.3.1. 간단한 예시\n\nmlr3에서는 `FSelector`를 이용하여 위의 방법을 수행합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr3fselect)\ninstance <- fselect(\n  method='sequential',\n  task= tsk('penguins'),\n  learner = lrn('classif.rpart'),\n  resampling = rsmp('holdout'),\n  measures = msr('classif.acc')\n)\n```\n:::\n\n\n성능 비교를 위한 feature들의 모든 subset을 확인하기 위해선 아래의 코드로 확인 가능합니다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.table(instance$archive) \n```\n:::\n\n\n최적의 feature 들을 확인하기 위해서는\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstance$result_feature_set\n```\n:::\n\n\n#### 5.3.2 FSelectInstance\n\n#### 5.3.3. Fselector 클래스\n\n`mlr3fselect::FSelector`에는 다양한 종류의 feature 선택 알고리즘이 존재합니다.\n\n-   Random Search ([`mlr3fselect::FSelectorRandomSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_random_search.html))\n-   Exhaustive Search ([`mlr3fselect::FSelectorExhaustiveSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_exhaustive_search.html))\n-   Sequential Search ([`mlr3fselect::FSelectorSequential`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_sequential.html))\n-   Recursive Feature Elimination ([`mlr3fselect::FSelectorRFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html))\n-   Design Points ([`mlr3fselect::FSelectorDesignPoints`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_design_points.html))\n-   Genetic Search ([`mlr3fselect::FSelectorGeneticSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_genetic_search.html))\n-   Shadow Variable Search ([`mlr3fselect::FSelectorShadowVariableSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html))\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}