[
  {
    "objectID": "gallery/gallery/exponential graph/index.html",
    "href": "gallery/gallery/exponential graph/index.html",
    "title": "지수함수를 통한 복리 비교",
    "section": "",
    "text": "require(ggplot2)\ndf <- data.frame(x=seq(1,50))\nx <- 1:50\np <- ggplot(df, aes(x=x))\nfor(i in 1:8){\n  p = local({\n    j <- i\n    y <- (1+0.01*j)^x\n    p + geom_line(aes(y=y, color=as.character(j)),\n                  linewidth=1) +\n      annotate(\"text\",\n               label = 0.01*j,\n               size=2,\n               x=50, y=(1+0.01*j)^50,\n               hjust = -.3)\n  }\n  )\n}\n\np + scale_color_brewer(\n  palette = \"Spectral\") +\n  theme_classic()+\n  scale_x_continuous(limits=c(0,55))+\n  theme(legend.position = \"None\") +\n  labs(x=\"Year\", y=\"Total\")"
  },
  {
    "objectID": "gallery/gallery/ggplot2_forestplot/index.html",
    "href": "gallery/gallery/ggplot2_forestplot/index.html",
    "title": "ggplot2를 활용한 forestplot 그리기",
    "section": "",
    "text": "ggplot2 를 이용한 예쁜 forest plot 만들기\n\ndf_data <- data.frame(Cancer=c(\"Brain\", \"Colorectal\", \"Kidney clear cell carcinoma\", \"Kidney renal papillary carcinoma\"),\n                      OR=c(1.03, 0.98, 1.27, 1.22),\n                      OR_lower=c(0.97, 0.62, 1.16, 1.03),\n                      OR_upper=c(1.09, 1.55, 1.38, 1.45)\n                      )\n\n### Visualize\n\nif (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')   # Load ggplot2 library\n\n\n\np <- ggplot(df_data, aes(x=Cancer, y=OR, ymin=OR_lower, ymax=OR_upper)) + \n  geom_linerange(size=8, colour=\"#a6d8f0\") +\n  geom_hline(aes(x=0, yintercept=1), lty=1) +\n  geom_point(size=3, shape=21, fill=\"#008fd5\", colour = \"white\", stroke = 1) +\n  scale_y_continuous(limits = c(0.5, 2)) +\n  coord_flip() +\n  ggtitle(\"Odds ratio for Gene of Interest\") +\n  theme_minimal()\np"
  },
  {
    "objectID": "gallery/gallery/ml_model_performance/index.html",
    "href": "gallery/gallery/ml_model_performance/index.html",
    "title": "머신러닝 모델별 퍼포먼스 그래프",
    "section": "",
    "text": "아래와 같이 모델별로 머신러닝 성능을 정리한 표가 있다고 해보자.\n\n이 데이터를 통해 머신러닝 모델별, 그리고 질병별 성능을 아래와 같이 시각화해보자. X축에는 모델이, Y축에는 성능이 와야 하고, 각 성능 지표별로 다른 선으로 구성할 것이다. 그리고 각 질병별로 이런 그래프가 나열되어야 한다.\n\n우선 데이터 시각화를 위한 데이터프레임을 만들어야 한다. 저 표 형태 그대로는 데이터프레임으로 저장할 수 없다. 따라서 데이터 프레임 형태를 잘 구성하는 게 중요하다.\n특정 범주별로 색상을 구분해주는 그래프를 만들기 위해선 X축과 범례에 들어갈 범주 데이터, 그리고 수치에 해당하는 숫자 데이터를 각각 다른 열로 구성해야 한다.\n우선 각 모델별 성능별 점수가 질병별로 필요하므로, 질병 열을 구성해준다. 위의 표에서 질병 * 모델 * 성능을 봤을 때, 하나의 행이 질병 하나의 모델별 성능별 점수인 것을 알 수 있다. 따라서 질병에 따라 모델, 성능, 점수가 길게 늘어지는 long data가 되도록 구성한다.\n\nrequire(data.table)\nrequire(ggplot2)\nrequire(ggthemes)\ndf <- data.table(\n  disease = rep(c(\"Death\",\"Heart disease\",\"Stroke\",\"Cancer\",\"Hypertension\",\"Diabetes\"), each=12),\n  model = rep(rep(c(\"Mod1\",\"Mod2\",\"Mod3\",\"Mod4\"),each=3),6),\n  category = rep(c(\"AUC\",\"Accuracy\",\"F1\"), 4*6),\n  score = c(0.863, 0.725, 0.106, 0.873, 0.753, 0.116, 0.877, 0.802, 0.135, 0.887, 0.793, 0.135,\n            0.718,  0.617,  0.213,  0.731,  0.653, 0.224,   0.731,  0.605,  0.216,  0.734,  0.614,  0.218,\n            0.790,  0.684,  0.223,  0.794,  0.681, 0.223,   0.794,  0.713,  0.234,  0.795,  0.705,  0.233,\n            0.774,  0.654,  0.123,  0.775,  0.669, 0.126,   0.776,  0.665,  0.125,  0.783,  0.690,  0.131,\n            0.698,  0.615,  0.434,  0.720,  0.595, 0.449,   0.769,  0.662,  0.492,  0.770,  0.665,  0.493,\n            0.706,  0.592,  0.324,  0.725,  0.615, 0.336,   0.806,  0.699,  0.400,  0.809,  0.726,  0.413\n            )\n)\ndf |> head(12)\n\n\n\n  \n\n\n\n하나의 질병 당 모델 네 종류, 그리고 모델 성능 세 종류가 필요하다. 그렇기 떄문에 하나의 질병을 12번씩 반복하여 만들어주면 된다.\n다음으로 원하는 순서대로 출력하기 위해, 질병의 순서와 모델 성능의 순서를 정해준다.\n\ndf$disease <- factor(df$disease, levels=c(\"Death\",\"Heart disease\",\"Stroke\",\"Cancer\",\"Hypertension\",\"Diabetes\"))\ndf$category <- factor(df$category, levels=c(\"AUC\",\"Accuracy\",\"F1\"))\n\n그리고 나서 질병 * 모델 * 성능 지표 별 점수를 시각화해준다.\n\n df |> \n  ggplot(aes(x=model, y=score, color=category, group=category))+\n  geom_point(aes(shape=category)) + \n  geom_line(aes(lty=category)) + \n  facet_grid(~disease) +\n  scale_x_discrete(labels=c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"))+\n  scale_y_continuous(limits = c(0,1),\n                     breaks = seq(0,1,0.2),\n                     expand = c(0,0)) + \n  scale_shape_discrete(labels=c(\"AUC\",\"Accuracy\",\"F1-score\"),\n                       name = NULL)+\n  scale_color_tableau(labels=c(\"AUC\",\"Accuracy\",\"F1-score\"),\n                       name = NULL)+\n  scale_linetype_discrete(labels=c(\"AUC\",\"Accuracy\",\"F1-score\"),\n                          name = NULL)+\n  geom_text(aes(label=format(score,3)), vjust=-1, size=1.5) +\n  theme_few() +\n  theme(legend.position = \"top\",\n        legend.direction = \"horizontal\",\n        axis.title = element_blank(),\n        legend.text = element_text(size=7),\n        axis.text = element_text(size=4),\n        strip.text = element_text(size=6, face = \"bold\")) +\n  ggtitle(\"(A) Male (n=64,389)\")\n\n\n\n\n질병은 facet_grid()에 포함시켜 옆으로 나열하게끔 시각화 하였다."
  },
  {
    "objectID": "gallery/index.html",
    "href": "gallery/index.html",
    "title": "Gallery",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\n지수함수를 통한 복리 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n머신러닝 모델별 퍼포먼스 그래프\n\n\n\nMachine learning\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nggplot2를 활용한 forestplot 그리기\n\n\n\nR\n\n\nggplot2\n\n\nforestplot\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rchemist 블로그",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nmlr3 피처 선택\n\n\n\n\n\n\n\nR\n\n\nmlr3\n\n\nmachine learning\n\n\n\n\n머신러닝에서 최적의 피처 선택하기\n\n\n\n\n\n\nApr 28, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgroup analysis (feat. forester)\n\n\n\n\n\n\n\nR\n\n\nSubgroup analysis\n\n\nPublish\n\n\nforester\n\n\n\n\nR을 활용한 하위그룹 분석\n\n\n\n\n\n\nApr 22, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\n성향점수 매칭 (PSM)\n\n\n\n\n\n\n\nR\n\n\nPSM\n\n\nmatchit\n\n\ncobalt\n\n\n\n\nR을 활용한 성향점수 매칭하는 방법\n\n\n\n\n\n\nApr 19, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nSankey plot 그리기\n\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\n\n\nggalluvial을 통한 Sankey plot 그리기\n\n\n\n\n\n\nApr 18, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n상관관계 시각화\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nvisualization\n\n\n\n\nggplot2, ggcorrplot 를 통해 상관관계 히트맵 만들기\n\n\n\n\n\n\nApr 17, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nMICE를 이용한 multiple imputation\n\n\n\n\n\n\n\nR\n\n\nimputation\n\n\nmice\n\n\n\n\nmultiple imputation을 활용한 분석방법\n\n\n\n\n\n\nApr 11, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주요 패키지의 성능비교\n\n\n\n\n\n\n\nR\n\n\ngroup_by\n\n\ntest\n\n\ndplyr\n\n\ndata.table\n\n\n\n\n주요 패키지들의 그룹별 계산 속도 비교\n\n\n\n\n\n\nApr 10, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIMIC-IV 분석 준비\n\n\n\n\n\n\n\ndataset\n\n\nMIMIC-IV\n\n\nPostgreSQL\n\n\n\n\nMIMIC-IV 데이터 DB에 저장하기\n\n\n\n\n\n\nApr 5, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\n통계 테이블 끝판왕 gtsummary\n\n\n\n\n\n\n\nR\n\n\ntable\n\n\ngtsummary\n\n\n\n\n분석결과 보고를 위한 최고의 테이블 패키지\n\n\n\n\n\n\nApr 3, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nGT\n\n\n\n\n\n\n\nR\n\n\ntable\n\n\ngt\n\n\n\n\n원하는대로 테이블 꾸며주는 패키지\n\n\n\n\n\n\nApr 1, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 하이퍼파라미터 최적화\n\n\n\n\n\n\n\nR\n\n\nmlr3\n\n\nmachine learning\n\n\n\n\nmlr3를 이용한 하이퍼파라미터 튜닝 학습\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n학습관점에서 비교하는 dplyr과 data.table\n\n\n\n\n\n\n\ndata.table\n\n\ndplyr\n\n\nR\n\n\n\n\n처음 배우는 사람에게 추천하는 패키지\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 리샘플링 벤치마킹\n\n\n\n\n\n\n\nmlr3\n\n\nR\n\n\nmachine learning\n\n\n\n\n여러 모델 동시 학습 및 성능비교\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nstringr을 이용한 문자 추출하기\n\n\n\n\n\n\n\nR\n\n\nregex\n\n\nstringr\n\n\n\n\n정규표현식을 사용한 규칙 찾기\n\n\n\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot2 facet label 설정\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nfacet\n\n\n\n\nggplot에서 facet을 사용할 때 label을 변경하는 방법에 대해 알아봅시다.\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 1.14.9 업데이트\n\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\n\n\ndata.table 1.14.9 버전 업데이트 내용 살펴보기.\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 기초\n\n\n\n\n\n\n\nmlr3\n\n\nR\n\n\nmachine learning\n\n\n\n\nmlr3에 대한 소개 및 mlr3를 사용하기 위한 필수문법에 대해 소개합니다.\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nImputation의 종류\n\n\n\n\n\n\n\nR\n\n\nimputation\n\n\nmice\n\n\n\n\nmice를 이용한 multiple imputation\n\n\n\n\n\n\nFeb 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggsignif: 통계적 유의성 시각화\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nvisualization\n\n\nggsignif\n\n\n\n\n그래프에 통계적 유의성(p-value) 출력\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot 세부 조정: 축 조정\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nvisualization\n\n\n\n\n그래프와 축 간격을 조정하거나 tick 간격을 변경하는 방법\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataExplorer을 활용한 EDA\n\n\n\n\n\n\n\nEDA\n\n\nR\n\n\n\n\nDataExplorer를 통한 탐색적 데이터 분석\n\n\n\n\n\n\nFeb 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidyr로 Pivoting하기\n\n\n\n\n\n\n\nR\n\n\ndplyr\n\n\ntidyr\n\n\n\n\ntidyr을 이용해 데이터의 형태를 바꾸는 pivoting에 대해 알아봅시다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr 심화\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\nacross()로 column 동시 처리\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot에서 두 번째 y축 그리기\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nVisualization\n\n\naxis\n\n\n\n\nggplot으로 그래프에 두 개의 y축을 활용하는 방법을 배우기\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr 기초 문법 이해하기\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\ndplyr 필수함수 및 문법 익히기\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr group_by()\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\ndplyr를 활용한 그룹 별 계산\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR에서 색상 다루기\n\n\n\n\n\n\n\ncolor\n\n\nR\n\n\nvisualization\n\n\n\n\n데이터 시각화를 진행할 때, 그래프에 적절한 색상을 선택하는 방법을 살펴봅시다.\n\n\n\n\n\n\nJan 25, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 심화\n\n\n\n\n\n\n\ndata.table\n\n\nR\n\n\n\n\n특수 기호, 조인, 피봇 등 data.table에서 다루는 심화내용을 살펴봅시다.\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot boxplot 그리기\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nVisualization\n\n\nboxplot\n\n\n\n\nggplot으로 boxplot 그리는 방법\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR 기초 이해\n\n\n\n\n\n\n\nR\n\n\ndata.frame\n\n\n\n\nR 사용을 위한 필수 개념 및 함수을 배워봅시다.\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nQuarto 사용법\n\n\nQuarto, a substitute of Rmarkdown\n\n\n\n\nQuarto\n\n\nR\n\n\n\n\nBrief intorduction of Quarto\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 기초 문법\n\n\n\n\n\n\n\ndata.table\n\n\nR\n\n\n\n\ndata.table 문법, 연산자, 함수\n\n\n\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservable.js\n\n\n\n\n\n\n\nOJS\n\n\nVisualization\n\n\njavascript\n\n\n\n\nInteractive chart with ojs\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny에서 DB 다루기\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울 R 밋업 자료\n\n\n\n\n\n\n\n\n\n\nR을 활용한 머신러닝\n\n\n\n\n\n\nJan 1, 2022\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그래프 범례(legend) 통합하기\n\n\n\n\n\n두 가지 이상의 변수 통합\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\nKaplan-Meier 곡선\n\n\n\n\n\n\n\nR\n\n\nsurvival\n\n\nvisualization\n\n\n\n\nggsurvplot을 통한 생존분석 시각화\n\n\n\n\n\n\nJan 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/data.table_1.14.9_update/index.html",
    "href": "blog/posts/data.table_1.14.9_update/index.html",
    "title": "data.table 1.14.9 업데이트",
    "section": "",
    "text": "data.table은 R에서 데이터를 빠르고 메모리 효율적으로 분석하도록 도와주는 패키지입니다. 현재 CRAN에 공개되어 있는 data.table 의 버전은 1.14.8 버전입니다 (2023-02-21 기준).\ngithub에서 개발 중인 (거의 개발이 마무리되어가고 있습니다) data.table 1.14.9 버전은 꽤 많은 내용들이 업데이트되었는데요. 업데이트 사항들을 살펴보니 꽤 흥미로운 요소들이 많았습니다.\n이번 글에서는 data.table 1.14.9 버전에서 유용하게 사용할 수 있는 추가 기능들을 소개해드리고자 합니다."
  },
  {
    "objectID": "blog/posts/data.table_1.14.9_update/index.html#패키지-업데이트",
    "href": "blog/posts/data.table_1.14.9_update/index.html#패키지-업데이트",
    "title": "data.table 1.14.9 업데이트",
    "section": "패키지 업데이트",
    "text": "패키지 업데이트\n현재 개발 중인 1.14.9 버전을 사용하기 위해선 CRAN이 아니라, github에서 개발 중인 패키지를 다운로드 받아야 합니다.\n\n# remotes::install_github('Rdatatable/data.table')\ndata.table::update_dev_pkg()\n\nR data.table package is up-to-date at 88039186915028ab3c93ccfd8e22c0d1c3534b1a (1.14.9)\n\nlibrary(data.table)\n\n아래와 같은 메시지가 뜨면 정상적으로 설치된 것 입니다.\ndata.table 1.14.9 IN DEVELOPMENT built 2023-02-21 01:19:16 UTC; user using 12 threads (see ?getDTthreads).  Latest news: r-datatable.com"
  },
  {
    "objectID": "blog/posts/data.table_1.14.9_update/index.html#주요-업데이트-내용",
    "href": "blog/posts/data.table_1.14.9_update/index.html#주요-업데이트-내용",
    "title": "data.table 1.14.9 업데이트",
    "section": "주요 업데이트 내용",
    "text": "주요 업데이트 내용\n새로운 버전에서 추가됐거나 변경된 점들 중, (주관적인) 유용한 부분에 대해 살펴보겠습니다!\n\n데이터 유형 출력\n이전까지 data.table 유형의 데이터를 출력했을 때, 각 열의 데이터 유형을 알 수 없었습니다. 그래서 str() 등을 통해 확인해야만 했었죠.\n이제 data.table도 tibble처럼 열별로 데이터 유형을 쉽게 확인할 수 있습니다.\n\n\n\n\niris |> as.data.table() |> head()\n\n# 이전\n#    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n# 1:          5.1         3.5          1.4         0.2  setosa\n# 2:          4.9         3.0          1.4         0.2  setosa\n# 3:          4.7         3.2          1.3         0.2  setosa\n# 4:          4.6         3.1          1.5         0.2  setosa\n# 5:          5.0         3.6          1.4         0.2  setosa\n# 6:          5.4         3.9          1.7         0.4  setosa\n\n# 1.14.9\n#    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#           <num>       <num>        <num>       <num>  <fctr>\n# 1:          5.1         3.5          1.4         0.2  setosa\n# 2:          4.9         3.0          1.4         0.2  setosa\n# 3:          4.7         3.2          1.3         0.2  setosa\n# 4:          4.6         3.1          1.5         0.2  setosa\n# 5:          5.0         3.6          1.4         0.2  setosa\n# 6:          5.4         3.9          1.7         0.4  setosa\n\n\n\n\n\n\n\nTip\n\n\n\n이전 버전에서도 data.table 의 열 유형을 확인하는 것이 가능합니다. options()를 통해 아래와 같이 설정해주시면 됩니다.\n\noptions(datatable.print.class=TRUE)\n\n\n\n\n\nDT()\nDT(i, j, by, …) 라는 형태의 함수가 추가되었습니다. data.table의 함수적 형태라고 볼 수 있겠습니다.\n기존에는 [i, j, by,...] 처럼 [] 을 이용했는데, 이를 위해서는 데이터가 항상 data.table 이어야 했습니다. 하지만 DT() 를 이용한다면 data.table이 아니어도 data.table 문법을 활용할 수 있게 되었습니다.\n\nmtcars |> DT(hp>100, .(mean_mpg = mean(mpg)), by=.(cyl))\n\n\n\n  \n\n\n\n\n\nenv\n프로그래밍 인터페이스가 새롭게 추가되었습니다. 파라미터들을 통해 계산에 사용할 column, 새롭게 정의할 column의 이름은 물론 함수의 이름까지 활용할 수 있게 되었습니다. 이 기능은 env 인자를 통해 활용 가능합니다.\n\ndt <- data.table(x = 1:5, y = 11:15, z = c('A','A','B','C','B'))\n\ndt[,.(out_colname = fun(in_colname, fun_arg1 = fun_arg1_val)), by=group,\n   env = list(\n     out_colname = \"mean_x\",\n     fun = \"mean\",\n     in_colname = \"x\",\n     fun_arg1 = \"na.rm\",\n     fun_arg1_val = T,\n     group = \"z\"\n   )]\n\n\n\n  \n\n\n\nenv 를 활용한 프로그래밍 인터페이스는 주로 사용자 정의 함수를 활용할 때 사용할 것 같습니다. 사용자 정의 함수에서 파라미터들을 문자열로 넣어주기 때문에, env는 굉장히 편리하게 사용할 수 있을 것으로 보입니다.\n\n\n%notin%\ndata.table의 infix 연산자 (%%가 들어간 연산자) 중 새로운 녀석이 추가되었습니다. 바로 %notin%입니다. 기존에 있던 %in% 연산자는 A %in% B 형태로 쓰여, 해당 변수(열)에 특정 값이 있는지 확인할 수 있었습니다. %in% 의 부정형을 사용하기 위해서는 변수 앞에 !을 붙여줘야 했었죠. 이제는 %notin%을 통해 더 직관적으로 그 의미를 이해할 수 있게 되었습니다.\n\n# 기존 방식\nas.data.table(iris)[!Species %in% 'setosa',.N, by=.(Species)]\n\n\n\n  \n\n\n# 1.14.9 버전\nas.data.table(iris)[Species %notin% 'setosa',.N, by=.(Species)]\n\n\n\n  \n\n\n\n\n\nlet()\ncolumn을 reference에서 변경하기 위해 사용하던 := 의 별명함수(alias)가 생겼습니다. 바로 let() 입니다.\n\nDT = data.table(A = 1:5)\n\n# 기존 방식\nDT[, B := 6:10]\n\n# 새로운 방식\nDT[, let(C = LETTERS[1:5],\n         D = letters[1:5])]\nDT\n\n\n\n  \n\n\n\nlet()의 등장으로 =와 := 을 헷갈려 하시는 분들의 어려움을 해소하는 데 도움이 될 것 같습니다.\n\n\nfifelse()\nfifelse()를 사용할 때 NA와 다른 유형의 데이터가 생성될 수 있게끔 변경되었습니다. 기존에는 fifelse()에서 true, false 인자에 들어가는 값에 NA가 들어가면 에러 메시지가 떴습니다. 하지만 1.14.9 버전부터는 해당 에러메시지가 뜨지 않고, 정상적으로 잘 출력됩니다.\n\nas.data.table(airquality)[,fifelse(Solar.R < 300, NA, Solar.R)]\n\n# 이전 버전\n# Error in fifelse(Solar.R < 300, NA, Solar.R) : \n#   'yes' is of type logical but 'no' is of type integer. Please make sure that both arguments have the same type.\n\n# 1.14.9 버전\n# [1]  NA  NA  NA 313  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA 334 307  NA 322  NA\n#  [21]  NA 320  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n#  [41] 323  NA  NA  NA 332 322  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n#  [61]  NA  NA  NA  NA  NA  NA 314  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n#  [81]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n# [101]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n# [121]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n# [141]  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA\n\n\n\n.I의 사용\n.I를 by에 사용함으로써, 행 단위(rowwise) 계산이 가능해졌습니다.\n예전에 여러 열들을 행별로 계산하기 위해서는 rowSums()나 rowMeans() 등을 활용했었는데 이제 더 간단하게 계산할 수 있게 되었습니다.\n\nlibrary(ggplot2)\n\n# 이전 활용\na=as.data.table(diamonds)[,.(sum = rowSums(.SD)),.SDcols=depth:z]\n\n# 1.14.9 버전\nb=as.data.table(diamonds)[,.(sum = sum(.SD)),.SDcols=depth:z, by=.I]\n\nmicrobenchmark()를 이용해 두 코드의 성능을 확인해보니, rowSums()를 활용하는 게 더 빠른 것으로 나타났습니다.\n\nmicrobenchmark::microbenchmark(\n  list =  list(\n    previous = a,\n    post = b\n  ),\n  times = 10\n)\n\n\n\n  \n\n\n\n\n지금까지 data.table 1.14.9 버전의 주요한 내용들에 대해 살펴보았습니다. 더 많은 내용은 아래 레퍼런스를 통해 살펴보시기 바랍니다!"
  },
  {
    "objectID": "blog/posts/data.table_1.14.9_update/index.html#레퍼런스",
    "href": "blog/posts/data.table_1.14.9_update/index.html#레퍼런스",
    "title": "data.table 1.14.9 업데이트",
    "section": "레퍼런스",
    "text": "레퍼런스\n\nr-datatable.com1\ndata.table changelog2"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html",
    "href": "blog/posts/mlr3_hyperparameter/index.html",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "",
    "text": "Important\n\n\n\n이 글은 mlr3book1을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 mlr32 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다.\n머신러닝 알고리즘은 보통 파라미터와 하이퍼파라미터를 포함하고 있습니다. 파라미터란 모델의 회귀계수나 가중치처럼 모델을 만들 때 필요한 매개변수입니다. 반면, 하이퍼파라미터는 사용자에 의해 구성됨으로써 파라미터가 어떻게 나올지를 결정합니다.\n대표적인 하이퍼파라미터의 예시로는 랜덤포레스트 알고리즘에서 나무의 개수를 정한다던가, 신경망의 학습률을 조정하는 것 등이 있습니다.\n하이퍼파라미터는 어떻게 설정하는지에 따라 모델의 성능을 향상시킬 수도, 그 반대가 될 수도 있습니다. 따라서 하이퍼파라미터를 최적화함으로써, 주어진 태스크에 대해 최적의 알고리즘 모델을 개발하는 것이 필요합니다.\n어쩌면 최적의 모델을 구성하는 것이 하나의 러너에 하이퍼 파라미터를 다르게 부여하는 벤치마크 실험을 통해 모델을 선택하는 것과 같다고 생각할 수 있습니다. 예를 들어 랜덤포레스트 모델들을 구성하는 나무의 개수를 다르게 정의하여 성능을 비교해본다고 해봅시다.\n결과를 봤을 때, 나무가 100개로 구성된 랜덤포레스트 모델의 성능이 가장 좋은 것으로 나타났습니다. 다만 이렇게 임의적으로 시행착오를 거쳐 하이퍼파라미터를 조정해주는 것은 많은 시간이 필요한 것은 물론, 종종 편향되고 재생산성이 떨어집니다.\n지금까지 개발되어온 정교한 하이퍼파라미터 최적화 방법은 종료(termination) 시점까지 반복적으로 다양한 파라미터를 검토 후, 최적의 하이퍼파라미터 구성을 내놓는 효율적이고 로버스트한 결과를 출력합니다."
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#러너의-학습공간-설정",
    "href": "blog/posts/mlr3_hyperparameter/index.html#러너의-학습공간-설정",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "러너의 학습공간 설정",
    "text": "러너의 학습공간 설정\n각각의 러너들은 search space라고 하는 학습공간을 갖습니다. 이 학습공간은 러너들의 하이퍼파라미터 세트를 설정해줄 수 있는 공간이라고 이해할 수 있습니다.\ne1071 패키지에 있는 서포트 벡터 머신(SVM)으로 예시를 들어보겠습니다. sonar 데이터를 태스크로 활용하여 SVM 모델을 최적화해봅시다.\n\nlrn_svm <- lrn(\"classif.svm\", type=\"C-classification\", kernel = \"radial\")\n\n러너의 파라미터 정보는 $param_set 필드에 저장이 됩니다. 여기에는 파라미터의 이름, 클래스의 종류, 레벨, 튜닝 범위 등이 포함되어 있습니다.\n\nas.data.table(lrn_svm$param_set)\n\n\n\n  \n\n\n\n범위가 무한대인 파라미터의 경우 이론상으로는 모든 경우의 수를 고려할 수 있으나, 이는 현실적으로 불가능하죠. 여기서는 일부 하이퍼파라미터를 설정해주도록 하겠습니다.\n숫자형 (numeric) 하이퍼파라미터의 경우 반드시 하한과 상한의 범위를 지정해줘야 합니다. 이 때 to_tune() 함수를 이용해주면 됩니다.\n\nlrn_svm = lrn(\"classif.svm\",\n            cost = to_tune(1e-1, 1e5),\n            gamma = to_tune(1e-1, 1),\n            type = \"C-classification\",\n            kernel = \"radial\")\n\nlrn_svm\n\n<LearnerClassifSVM:classif.svm>\n* Model: -\n* Parameters: cost=<RangeTuneToken>, gamma=<RangeTuneToken>,\n  type=C-classification, kernel=radial\n* Packages: mlr3, mlr3learners, e1071\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric\n* Properties: multiclass, twoclass\n\n\n학습공간은 주로 머신러닝 모델 설계자의 경험을 바탕으로 구성되는 것이 일반적입니다. 학습공간과 관련된 자세한 사항은 아래에서 추가적으로 다루도록 하겠습니다."
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#터미네이터-terminator",
    "href": "blog/posts/mlr3_hyperparameter/index.html#터미네이터-terminator",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "터미네이터 (Terminator)",
    "text": "터미네이터 (Terminator)\n앞서 말한 것처럼 이론적으로는 각 러너들의 모든 학습공간을 탐색하며 성능이 가장 뛰어난 모델을 만들 수 있습니다. 하지만 수학적으로, 그리고 현실적으로는 모든 학습공간을 탐색하기란 불가능합니다. 따라서 특정 알고리즘의 하이퍼파라미터 튜닝 과정 도중, 언제 종료할지에 대한 기준 역시 필요합니다.\nmlr3tuning 패키지에서는 이것을 Terminator 클래스를 통해 구현해놓았습니다. 터미네이터의 종류는 다음과 같습니다.\n\nTerminator 종류\n\n\n\n\n\n\n\n종 류\n설명\n사용예시와 초기 파라미터 설정\n\n\n\n\n평가 횟수\n특정 탐색횟수가 되면 종료\ntrm(\"evals\", n_evals= 500)\n\n\n구동 시간\n특정 탐색시간이 되면 종료\ntrm(\"run_time\", sec= 100)\n\n\n성능 수준\n특정 성능에 도달하면 종료\ntrm(\"perf_reached\", level= .1)\n\n\n시간\n특정 현실시간이 되면 종료\ntrm(\"clock_time\", n_evals= 500)\n\n\n정체\n특정 반복동안 개선이 없으면 종료\ntrm(\"stagnation\", iters= 5, threshold= 1e-5)\n\n\n조합\n여러 터미네이터 조합\ntrm(\"combo\", terminators = list(run_time_100, evals_200), any = TRUE)\n\n\n\n이 중 가장 많이 활용되는 것은 trm(\"evals\", n_evals= 500) 와 trm(\"run_time\", sec= 100) 입니다. trm(\"combo\")는 여러 가지의 터미너이터를 조합하여 종료기준을 설정합니다. any 또는 all을 통해 여러 조건을 하나만 만족해도 종료할지, 모든 기준을 만족해야 할지 설정할 수 있습니다."
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#ti-튜닝-인스턴스",
    "href": "blog/posts/mlr3_hyperparameter/index.html#ti-튜닝-인스턴스",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "ti: 튜닝 인스턴스",
    "text": "ti: 튜닝 인스턴스\n튜닝 인스턴스는 어떤 모델을 최적화하기 위해 필요한 모든 정보를 갖는 일종의 환경입니다. 어떤 데이터를 어떤 알고리즘을 통해 학습시킬 건지, 어떤 검증전략을 통해 어떤 성능을 기준으로 파라미터들을 평가할 것인지 등의 정보를 담게 됩니다.\n이러한 튜닝 인스턴스는 ti() 함수를 통해 사용자가 직접 설정하거나 tune()을 이용해 자동으로 설정할 수 있습니다. 우선은 ti()를 통해 튜닝 인스턴스를 설정하는 방법부터 알아보겠습니다.\n튜닝 인스턴스에는 학습시킬 데이터를 갖고 있는 태스크(task), 러너(learner), 리샘플링 (resampling), 성능 측정(measure), 그리고 터미네이터(terminator)가 사용됩니다.\n\nresampling = rsmp(\"cv\", folds=3)\n\nmeasure = msr(\"classif.acc\")\n\nlearner = lrn(\"classif.svm\",\n  cost = to_tune(1e-1, 1e5),\n  gamma = to_tune(1e-1, 1),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nterminator = trm(\"evals\")\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds=3),\n  measures = measure,\n  terminator = terminator\n)\n\ninstance\n\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.svm_on_sonar>\n* Search Space:\n       id    class lower upper nlevels\n   <char>   <char> <num> <num>   <num>\n1:   cost ParamDbl   0.1 1e+05     Inf\n2:  gamma ParamDbl   0.1 1e+00     Inf\n* Terminator: <TerminatorEvals>"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#tuner",
    "href": "blog/posts/mlr3_hyperparameter/index.html#tuner",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "Tuner",
    "text": "Tuner\n튜닝 인스턴스를 만들었다면, 이제 어떻게 튜닝을 할 것인지를 정해야 합니다. mlr3tuning 에는 다양한 Tuner 클래스가 존재합니다.\n\nmlr3tuning에서 사용가능한 튜닝 알고리즘\n\n\nTuner\nFunction call\nPackage\n\n\n\n\nRandom Search\ntnr(\"random_search\")\nmlr3tuning\n\n\nGrid Search\ntnr(\"grid_search\")\nmlr3tuning\n\n\nIterative Racing\ntnr(\"irace\")\nmlr3tuning\n\n\nCMA-ES\ntnr(\"cmaes\")\nmlr3tuning\n\n\nBayesian Optimization\ntnr(\"mbo\")\nmlr3mbo\n\n\nHyperband\ntnr(\"hyperband\")\nmlr3hyperband\n\n\nGeneralized Simulated Annealing\ntnr(\"gensa\")\nmlr3tuning\n\n\nNonlinear Optimization\ntnr(\"nloptr\")\nmlr3tuning\n\n\n\ngrid search와 random search는 가장 기본적이면서도 많이 사용되는 튜닝 알고리즘입니다. grid search는 학습공간에서 설정한 범위 내의 모든 하이퍼파라미터를 평가하는 반면, random search는 랜덤하게 학습공간을 탐색하여 하이퍼파라미터들을 평가합니다.\ngrid search와 random search는 나이브(naive)한 알고리즘으로 평가받는데, 이는 하이퍼파라미터들을 평가할 때, 이전 평가값들을 무시하고 새롭게 하이퍼파라미터들을 구성하기 때문입니다.\n반면에 Covariance Matrix Adaptation Evolution Strategy (CMA-ES)나 베이지안 최적화와 같은 보다 정교한 알고리즘들은 모델 기반 최적화라고도 볼립니다. 이 알고리즘들은 하이퍼파라미터 평가 과정 중 이전의 평가된 구성으로부터 학습하여 더 좋은 하이퍼파라미터 조합을 더욱 빠르게 찾아냅니다.\n앞서 우리가 만들었던 SVM 알고리즘을 튜너를 활용해 하이퍼파라미터를 탐색해보도록 하겠습니다. tnr() 함수를 이용해 튜닝을 수행하는데, batch_size를 설정하여 한 번에 몇 개의 하이퍼파라미터 구성을 평가할지 설정할 수 있습니다. 이 때 설정해주는 resolution의 경우 하이퍼파라미터 조합의 숫자와 연관이 있습니다. 예를 들어 위의 튜닝 인스턴스에서 범위를 정해준 하이퍼파라미터는 두 개 (cost, gamma)입니다. resolution을 5로 설정할 경우 \\(5^2=25\\), 즉 25번의 하이퍼파라미터 조합이 구성되는 것입니다.\n\ntuner = tnr(\"grid_search\", resolution = 5, batch_size = 10)\ntuner\n\n<TunerGridSearch>: Grid Search\n* Parameters: resolution=5, batch_size=10\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#튜닝-수행하기",
    "href": "blog/posts/mlr3_hyperparameter/index.html#튜닝-수행하기",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "튜닝 수행하기",
    "text": "튜닝 수행하기\n이제 하이퍼파라미터 튜닝을 실시해보겠습니다. 튜너$optimize(튜닝인스턴스)의 구조로 튜닝을 수행해줍니다.\n\ntuner$optimize(instance)"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#tune-을-이용한-빠른-튜닝",
    "href": "blog/posts/mlr3_hyperparameter/index.html#tune-을-이용한-빠른-튜닝",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "tune 을 이용한 빠른 튜닝",
    "text": "tune 을 이용한 빠른 튜닝\n앞서 ti()를 이용해 튜닝 인스턴스를 설정하고, 터미네이터, 튜너까지 직접 설정해주었는데요. tune() 함수를 이용하면 이러한 튜닝 인스턴스 설정 과정을 간소화할 수 있습니다.\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 10),\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.acc\")\n)\n\ninstance$result"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#튜닝-결과-분석하기",
    "href": "blog/posts/mlr3_hyperparameter/index.html#튜닝-결과-분석하기",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "튜닝 결과 분석하기",
    "text": "튜닝 결과 분석하기\nti이나 tune 에 관계없이 하이퍼파라미터 튜닝 이후에는 모든 파라미터 구성이 $archive 필드에 포함되어있습니다.\n\nas.data.table(instance$archive)[,.(cost, gamma, classif.acc)]\n\n\n\n  \n\n\n\n결과를 보면 25개의 조합별로 성능 점수(classif.acc, 정확도)가 나와있습니다. 또한 튜닝결과에서는 하이퍼파라미터의 조합별 성능 뿐만 아니라 에러나 경고, 학습된 시간, 모델 학습 시간 등의 정보도 확인할 수 있습니다.\n\nas.data.table(instance$archive)[,\n  .(timestamp, runtime_learners, errors, warnings)]\n\n\n\n  \n\n\n\n마지막으로 하이퍼파라미터 튜닝 결과를 mlr3viz를 이용해 시각화할 수 있습니다.\n\nautoplot(instance, type=\"surface\")\n\n\n\n\ncost와 gamma 의 조합에 따른 성능의 정도가 색상으로 나타났습니다. 정확도가 높을수록 히트맵의 색상이 연하다는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#튜닝된-모델-활용하기",
    "href": "blog/posts/mlr3_hyperparameter/index.html#튜닝된-모델-활용하기",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "튜닝된 모델 활용하기",
    "text": "튜닝된 모델 활용하기\n튜닝인스턴스에는 튜닝된 결과에 대한 모든 정보를 포함하고 있습니다. instance에 저장된 최적의 파라미터를 활용하여 새로운 러너의 파라미터로 설정해주겠습니다.\n\nsvm_tuned = lrn(\"classif.svm\")\nsvm_tuned$param_set$values = instance$result_learner_param_vals\n\n이제 새로운 러너를 학습하고 결과를 예측할 수 있게 되었습니다.\n\nsvm_tuned$train(tsk(\"sonar\"))\nsvm_tuned$model\n\n\nCall:\nsvm.default(x = data, y = task$truth(), type = \"C-classification\", \n    kernel = \"radial\", gamma = 0.00316227766016838, cost = 1e+05, \n    probability = (self$predict_type == \"prob\"))\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1e+05 \n\nNumber of Support Vectors:  93"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#autotuner를-활용한-중첩-리샘플링",
    "href": "blog/posts/mlr3_hyperparameter/index.html#autotuner를-활용한-중첩-리샘플링",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "AutoTuner를 활용한 중첩 리샘플링",
    "text": "AutoTuner를 활용한 중첩 리샘플링\nmlr3에선 AutoTuner를 활용한다면 중첩리샘플링도 쉽게 수행이 가능합니다. 코드를 통해 중첩 리샘플링을 살펴보겠습니다.\nAutoTuner 내부에서 수행하는 리샘플링은 inner resampling으로 4-fold 교차 검증을 수행합니다. 반면에 at를 대상으로 수행하는 resample()의 경우 outer resampling으로 3-fold 교차 검증을 수행합니다.\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 10),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\"),\n)\n\ntask = tsk(\"sonar\")\nouter_resampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n\nrr\n\n<ResampleResult> with 3 resampling iterations\n task_id        learner_id resampling_id iteration warnings errors\n   sonar classif.svm.tuned            cv         1        0      0\n   sonar classif.svm.tuned            cv         2        0      0\n   sonar classif.svm.tuned            cv         3        0      0\n\n\nresample()에서 store_models = TRUE 로 설정할 경우, inner tuning 된 AutoTuner 모델들이 저장됩니다. 이를 통해 inner tuning에 대한 정보를 알 수 있습니다.\nextract_inner_tuning_results()는 최적의 하이퍼파라미터 구성을 보여주고, extract_inner_tuning_archives()는 모든 하이퍼파리미터 기록을 보여줍니다.\n\nextract_inner_tuning_results(rr)\n\n\n\n  \n\n\n\n\nextract_inner_tuning_archives(rr)"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#성능-비교",
    "href": "blog/posts/mlr3_hyperparameter/index.html#성능-비교",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "성능 비교",
    "text": "성능 비교\n이제, inner tunning과 outer tuning 간의 성능 비교를 통해 모델 과적합 여부를 살펴보겠습니다.\nextract_inner_tuning_results(rr)[,\n  .(iteration, cost, gamma, classif.ce)] # inner\nrr$score()[,.(iteration, classif.ce)] # outer\n외부 리샘플링의 일부 결과에서 확연히 낮은 성능이 나온다는 것은 최적화된 하이퍼파라미터가 데이터에 과적합되었다는 것을 의미합니다. 따라서 튜닝된 모델은 요약계산된 성능으로 보고하는 것이 중요합니다.\n\nrr$aggregate()\n\nclassif.ce \n 0.2255349 \n\n\n마지막으로 중첩 리샘플링을 과도하게 많이 설정하는 경우, 컴퓨터 성능을 많이 필요로 하게 됩니다. 예를 들어 위의 예시에서는 내부 리샘플링 4, 외부 리샘플링 3, 하이퍼파라미터 2개, resolution 5로 인해 \\(3*4*5*5=300\\), 즉 300번의 모델 훈련 검증을 수행하게 됩니다. 따라서 내부 리샘플링에서는 홀드아웃 등과 같은 방법을 사용하거나 병렬화를 사용하도록 하는 것을 권장합니다.\n\n\n\n\n\n\n\n\nR6 Class\nSugar function\n요약\n\n\n\n\nTuner\ntnr()\n최적화 알고리즘 결정\n\n\nTerminator\ntrm()\n튜닝 알고리즘 종료 시점 기준 설정\n\n\nTuningInstanceSingleCrit or TuningInstanceMultiCrit\nti()\n튜닝 세팅 저장 및 결과 저장\n\n\nAutoTuner\nauto_tuner()\n튜닝과정 자동화\n\n\n-\nextract_inner_tuning_results()\n중첩 리샘플링의 내부 튜닝결과 추출\n\n\n-\nextract_inner_tuning_archives()\n중첩 리샘플링의 내부 튜닝 아카이브 추출"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#클래스-함수-정리",
    "href": "blog/posts/mlr3_hyperparameter/index.html#클래스-함수-정리",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "클래스 함수 정리",
    "text": "클래스 함수 정리"
  },
  {
    "objectID": "blog/posts/mlr3_hyperparameter/index.html#참고자료",
    "href": "blog/posts/mlr3_hyperparameter/index.html#참고자료",
    "title": "mlr3 하이퍼파라미터 최적화",
    "section": "참고자료",
    "text": "참고자료\nhttps://mlr3book.mlr-org.com/optimization.html"
  },
  {
    "objectID": "blog/posts/data.table_advanced/index.html",
    "href": "blog/posts/data.table_advanced/index.html",
    "title": "data.table 심화",
    "section": "",
    "text": "data.table 기초 포스트 에서는 대용량의 데이터를 빠르게 처리할 수 있는 data.table 패키지에 대해 배워봤습니다.\n이번 시간에는 data.table 패키지를 조금 더 효율적으로 사용할 수 있는 방법, 그리고 data.table을 이용하여 데이터를 원하는대로 붙이고 변형하는 방법에 대해 알아보겠습니다."
  },
  {
    "objectID": "blog/posts/data.table_advanced/index.html#특수-기호",
    "href": "blog/posts/data.table_advanced/index.html#특수-기호",
    "title": "data.table 심화",
    "section": "1. 특수 기호",
    "text": "1. 특수 기호\ndata.table 패키지에는 유용하게 사용할 수 있는 특수 기호들이 있습니다. 이 특수한 기호들은 다른 패키지에서는 사용할 수 없는 것들로, data.table 형태의 데이터를 다룰 때만 사용이 가능합니다.\n이번에 배울 data.table의 특수 기호는 크게 3가지입니다. 바로 .SD, .N, .I 입니다. 이 특수기호들은 모두 data.table의 j(column 부분)에서 쓰입니다.\n\n1) lapply + .SD\n.SD는 Subset Data의 약자입니다. 말그대로 데이터의 일부분을 선택하기 위한 특수기호(special symbols)입니다.\n.SD와 함께 사용되는 것이 있습니다. 바로 .SDcols 입니다. .SDcols를 통해 데이터 중 원하는 column을 선택할 수 있습니다.\n만약 .SDcols 없이 .SD만 사용한다면 데이터의 모든 column을 선택한다는 뜻입니다.\n\nrequire(data.table)\nrequire(NHANES)\ndt <- as.data.table(NHANES)\ndt[,head(.SD)]\n\n\n\n  \n\n\n\n반면 .SDcols와 함께 .SD를 사용한다면 .SDcols에 입력한 column만 선택합니다.\n\ndt[,str(.SD),.SDcols=c('Gender','Age')]\n\nClasses 'data.table' and 'data.frame':  10000 obs. of  2 variables:\n $ Gender: Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 1 2 2 1 1 1 ...\n $ Age   : int  34 34 34 4 49 9 8 45 45 45 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n - attr(*, \".data.table.locked\")= logi TRUE\n\n\nNULL\n\n\n.SDcols에 column을 선택하는 방법은 크게 세 가지가 있습니다.\n\ncolumn 이름을 갖는 vector 만들어서 사용\n아래의 예시처럼 찾고자 하는 column 이름 c()로 묶은 vector 형태로 넣어줄 수 있습니다.\n\ndt[,.SD,.SDcols=c('Gender','Age','Race1','Education')]\n\n\n\n  \n\n\n# OR\ntarget <- c('Gender','Age','Race1','Education')\ndt[,.SD,.SDcols=target] |> head()\n\n\n\n  \n\n\n\npatterns()를 통한 column 규칙 찾기\npatterns() 함수를 이용해 해당 문자열을 갖는 모든 column을 찾을 수 있습니다.\n\ndt[,.SD,.SDcols=patterns('Alcohol')] |> head()\n\n\n\n  \n\n\n\n:을 이용해 연속적인 column을 찾기\n.SDcols에 적용할 column이 연이어 붙어있는 경우, :을 이용하여 찾을 수 있습니다.\n\ndt[,.SD,.SDcols=Gender:Age] |> head()\n\n\n\n  \n\n\n\n\n한편 .SD (.SDcols)와 자주 사용되는 함수는 lapply()입니다.\nlapply()는 list + apply의 약자로, list에 동일한 함수를 적용할 때 사용되는 함수입니다.\nlapply()는 크게 아래의 구조로 이루어져 있습니다.\n\nlapply(\n  X, # 함수를 적용할 부분\n  FUN, # 선택된 X에 적용할 함수\n  ... # 추가 인자, e.g., na.rm=T\n)\n\nX 에는 함수를 적용할 column이름 또는 벡터가 오게 됩니다. 이 챕터에서는 주로 data.table에 lapply()를 적용하기 때문에 column이름이 오게 됩니다.\nFUN 에는 X에서 선택된 column들에 동일하게 적용할 함수를 작성합니다. FUN 부분은 각 column이 호출되어야 하는 횟수에 따라 사용하는 방법이 달라집니다.\n보통 function(x) 또는 \\(x) 를 이용해 적용시킬 함수를 입력해줍니다.\n\nlapply(1:5, function(x) ifelse(is.na(x),mean(x,na.rm=T),x))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\nlapply(1:5, \\(x) mean(x,na.rm=T))\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\n만약 평균을 구하늖 ㅏㅁ수처럼 mean() 처럼 각 column이 한번만 와도 되는 상황이라면 function()을 생략하고 함수의 이름만 사용할 수 있습니다. 결측치가 있는 경우 … 부분에 추가적으로 na.rm=T 를 사용할 수 있습니다.\n\nlapply(1:5, mean, na.rm=T)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nlapply()와 .SD를 사용할 때는 .SD와 .SDcols를 통해 선택되는 column에 동일한 함수를 적용하게 됩니다. 일일이 column마다 함수를 적용할 필요가 없으니, 입력해야 하는 코드도 줄어들 뿐만 아니라 계산에 필요한 시간도 훨씬 단축됩니다.\n\ndt[,lapply(.SD, # 함수를 적용할 column \n           mean, # functcion 부분\n           na.rm=T # 추가 인자 부분\n           ),\n   .SDcols=c('Age','BMI') # .SD 중 선택되는 column이름\n   ]\n\n\n\n  \n\n\n\n또한 lapply()와 .SD는 여러 column들의 평균, 표준편차 등의 요약통계량을 계산하는 경우, 동시에 column들의 유형을 numeric에서 character로 변환하는 경우 등에 자주 사용됩니다.\n\ndt[,lapply(.SD, mean, na.rm=T), .SDcols=target]\n\n\n\n  \n\n\ndt[,lapply(.SD, as.factor), .SDcols=target] |> head()\n\n\n\n  \n\n\n\nlapply()와 .SD를 활용하여 여러 column들을 변경한 후, 데이터에 저장하는 것은 chapter 3에서 배웠던 := 를 이용합니다.\n여러 column을 동시에 변경하여 저장할 때는 := 왼쪽 부분에 column 이름이 있는 vector를 ()로 감싸주면 됩니다.\n예를 들어 나이와 BMI의 NA를 각 column의 중앙값으로 채워넣고자 하는 경우,\n\ntarget <- c('Age','BMI')\n\ndt[,(target):=lapply(.SD, \\(x) ifelse(is.na(x), median(x,na.rm=T),x)),.SDcols=target]\n\n\n\n2) .N\n.N은 데이터의 수, 다시 말해 row의 개수를 확인하는 특수기호입니다. .N은 주로 by와 함께 됩니다. by를 통해 특정 집단의 분포별로 몇 명이 있는지, 또는 몇 건의 데이터가 있는지 확인합니다.\n예를 들면 인종(race)에 따른 데이터의 수를 확인하고 싶을 때는 아래와 같이 사용합니다.\n\ndt[,.N, by=Race1]\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n사실 그룹별 응답 수는 table() 함수를 이용해도 구할 수 있습니다. 그러나 .N을 이용하게 되면 각 범주별 빈도수를 data.table 문법 내에서 구할 수 있기 때문에 활용성에 있어 table()보다 더 뛰어나다고 할 수 있습니다.\n\ndt[,c(.N, lapply(.SD,mean)),by=.(Race1),.SDcols='Age']\n\n\n\n  \n\n\n\n\n\n\n\n3) .I\n.I는 j 부분에서 행을 다루기 특수기호입니다.\n보통 .I는 특정 조건을 만족하는 row의 위치를 찾을 때 사용합니다. 특히 by가 있는 경우, 다시 말해 범주별로 row의 위치를 찾을 때 많이 사용합니다.\n집단별 row가 필요하지 않다면, j가 아닌 i 부분에서 행을 선택할 수 있습니다. 그러나 i 부분만 입력하게 될 경우, 집단 별 조건을 확인할 수 없습니다. 왜냐하면 by를 사용하기 위해서는 j가 선행되어야 하기 때문입니다.\n따라서 i 부분에서 바로 row를 filtering 하는 것이 아니라, j 부분에서 집단별로 조건을 만족시키는 행의 번호를 찾아서 사용합니다.\n또한 .I를 사용하는 경우 행의 번호를 확인할 수 있기 때문에, 그 행의 번호에 해당하는 모든 column을 사용할 수 있습니다.\n예를 들어, 인종별 첫 번째 행을 선택하는 경우는 다음과 같이 실행할 수 있습니다.\n\ndt[dt[,.I[1L],by=Race1]$V1]\n\n\n\n  \n\n\n\n이번에는 인종 별로 첫 번째 행이 아니라 Height가 최대인 (키가 가장 큰) 데이터를 추출해보겠습니다.\n\ndt[dt[,.I[max(Height,na.rm=T)],by=.(Race1)]$V1]\n\n\n\n  \n\n\ndt[dt[,.I[which.max(Height)],by=.(Race1)]$V1]"
  },
  {
    "objectID": "blog/posts/data.table_advanced/index.html#데이터-병합",
    "href": "blog/posts/data.table_advanced/index.html#데이터-병합",
    "title": "data.table 심화",
    "section": "2. 데이터 병합",
    "text": "2. 데이터 병합\nR에서 데이터를 묶는 방법은 크게 두 종류가 있습니다. 하나는 bind() 이고, 다른 하나는 merge() 또는 join() 입니다.\n\n1) bind\nbind 계열의 함수는 두 개 이상의 데이터를 합치는 함수입니다. merge는 뒤에서 언급하겠지만, bind는 merge와 달리 특정한 기준이 되는 column이 필요하지 않습니다.\nbind에는 rbind(), cbind()가 있습니다.\n\na. rbind()\nrbind()는 row-bind의 약자로, 두 가지 이상의 데이터의 행을 묶는 함수입니다. 행을 묶는 것이기 때문에 데이터가 아래로 추가됩니다.\n\nrbind(1:3,c('a','b','c'))\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"2\"  \"3\" \n[2,] \"a\"  \"b\"  \"c\" \n\n\nrbind()를 통해 합치고자 하는 데이터의 길이가 맞지 않는 경우, 자동으로 부족한 부분을 채웁니다.\n\nrbind(c(1:3),c(1:4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    1\n[2,]    1    2    3    4\n\n\n만약 column 이름을 갖는 matrix나 data.table 형태를 rbind()하고자 하는 경우는 합치고자 하는 데이터의 column이름이 같아야 합니다.\n\nrequire(data.table)\na <- data.table(num=1:5,\n                str = letters[1:5])\nb <- data.table(num=6:10,\n                str = LETTERS[1:5])\nrbind(a,b)\n\n\n\n  \n\n\n\n만약 두 데이터의 column 이름이 같지 않은 경우, 에러 메시지가 뜨게 됩니다.\n\na <- data.table(num=1:5,\n                str=letters[1:5])\nb <- data.table(num=6:10,\n                str2=LETTERS[1:5])\nrbind(a,b)\n\n# 에러메시지\n# Column 2 ['str2'] of item 2 is missing in item 1. Use fill=TRUE to fill with NA (NULL for list columns), or use.names=FALSE to ignore column names.\n\n이럴 때는 column 이름을 무시하거나, 이름이 같지 않은 column을 NA로 채워줄 수 있습니다.\n\nrbind(a,b, use.names=F)\n\n\n\n  \n\n\n\n\nrbind(a,b, fill=T)\n\n\n\n  \n\n\n\n\n\nb. cbind()\ncbind()는 column-bind의 약자로, 두 가지 이상의 데이터의 열을 묶는 함수입니다. 열을 묶기 때문에 데이터가 옆으로 추가됩니다.\n\na <- data.table(num=1:5,\n                str = letters[1:5])\nb <- data.table(num=6:10,\n                str2 = LETTERS[1:5])\ncbind(a,b)\n\n\n\n  \n\n\n\n\n\n\n2) merge\nmerge()는 두가지의 데이터를 특정 column을 기준으로 합치는 함수입니다. bind 계열의 함수와 다르게 기준이 되는 column이 반드시 필요합니다.\nmerge()는 두 가지 데이터 중 어떤 방식으로 합치느냐에 따라 크게 3가지로 분류할 수 있습니다.\n\na. inner join\ninner join은 두 가지 데이터의 기준이 되는 column 에서 공통된 값들만 갖는 데이터를 합치는 방법입니다.\ndata.table에서 inner join 하는 방법은 아래와 같습니다.\n\nmerge(dt1, dt2, by=\"id\")\n\n\n\nb. left / right join\nleft/right join은 두 가지 데이터에서 왼(오른)쪽 데이터의 column을 기준으로 데이터를 합치는 방법입니다.\ndata.table에서 left join과 right join을 하는 방법은 아래와 같습니다.\n\nmerge(dt1, dt2, by, all.x=T) # left join\nmerge(dt1, dt2, by, all.y=T) # right join\n\nall.x 인자에 TRUE를 주면 left join을, all.y는 right join을 의미합니다.\n\n\nc. outer join\nouter join은 두 가지 데이터의 모든 column을 기준으로 데이터를 합치는 방법입니다.\n공통되지 않은 값을 갖는 경우는 NA로 채워넣어집니다.\ndata.table에서 outer join을 하는 방법은 아래와 같습니다.\n\nmerge(dt1, dt2, all.x)\n\nmerge의 경우 공통된 이름의 column을 기준으로 두 데이터를 합치는 방법이라고 했습니다. 그런데 만약 기준이 되는 column의 이름이 다르다면 어떻게 해야 할까요? 이때는 by.x와 by.y 인자를 사용합니다.\n\nmerge(by.x, by.y)\n\n기존에는 by인자에 공통된 column명을 넣으면 됐지만, 두 가지 데이터의 column이 다르기 때문에 각각 by.x와 by.y에 기준이 되는 column 이름을 넣어주는 것입니다."
  },
  {
    "objectID": "blog/posts/data.table_advanced/index.html#section",
    "href": "blog/posts/data.table_advanced/index.html#section",
    "title": "data.table 심화",
    "section": "",
    "text": "Tip\n\n\n\n두 가지 column을 기준으로 합치고자 할 때, 이름이 다른 경우 아래와 같이 사용합니다.\n\nmerge(dt1, dt2, by.x=c('a','b'), by.y=c('A','B'))\n\n\n\n.EACHI: X와 Y를 합칠 때, 요소별 합쳐진 개수 확인\n\nX = data.table(x = c(1,1,1,2,2,5,6), y = 1:7, key = \"x\")\nY = data.table(x = c(2,6), z = letters[2:1], key = \"x\")\nX[Y,.N, by=x]; #X[Y,.N, by=y];\n\n\n\n  \n\n\nX[Y, .N, by=.EACHI]"
  },
  {
    "objectID": "blog/posts/data.table_advanced/index.html#pivoting",
    "href": "blog/posts/data.table_advanced/index.html#pivoting",
    "title": "data.table 심화",
    "section": "3. Pivoting",
    "text": "3. Pivoting\npivoting이란 column이 여러 개 좌우로 붙어있던 것을 위아래로 길게 늘이거나, 데이터를 좌우로 넓게 펼치는 것을 의미합니다.\npivot: 축을 기준으로 데이터를 회전시키는 것을 의미합니다. 데이터 측면에서 축은 기준이 되는 column을 의미합니다. 예를 들면 환자의 식별자, 일자 등이 있습니다.\npivoting 개념에서 사용되는 데이터 유형 두 가지에 대해 살펴보겠습니다.\n\nWide data\n넓은(wide) 데이터는 이름 그대로 양 옆으로 넓은 데이터를 의미합니다. 양 옆으로 넓다는 것은 column의 개수가 많다는 것을 의미합니다. 지금까지 교재에서 다루었던 거의 대부분의 데이터가 column들이 옆으로 붙어있는 형태는 wide 데이터입니다.\nLong data\n긴 (long) 데이터는 넓은 데이터와 다르게, 반복되는 column을 기준으로 여러 변수(variable)와 값(value)으로 구성된 데이터입니다.\n이 때 반복되는 column이란 반복되어 기록된 값들입니다. 예를 들면 환자가 입원해있는 동안의 기록이나 일자별 날씨 같이 반복적으로 기록되어 있는 데이터 등이 있습니다.\nairquality는 Month와 Day 별 오존량, 풍량, 기온 등이 기록된 데이터입니다. 여기서 Month와 Day가 바로 반복되는 column입니다.\n\nairquality |> head()\n\n\n\n  \n\n\n\nlong 데이터는 이렇게 반복되는 column들을 기준으로 데이터를 variable과 value로 길게 확장시켜 놓은 것입니다.\n\n데이터 분석을 수행하는 우리 입장에서는 wide한 데이터가 보기에 더 편리합니다. column마다 어떤 데이터가 있는지 확인할 수 있고, 각 column 별로 계산을 수행할 수 있기 때문입니다.\n그러나 컴퓨터 입장에서는, 즉 계산하는 측면에서는 long data가 더 효율적입니다. 그렇기 때문에 동일한 column에 대해 연산을 할 때, long data가 훨씬 빠르게 계산됩니다.\n\n1) melt\nmelt는 column들이 옆으로 나열되어 있는 옆으로 넓은(wide)한 데이터를 위아래로 길게(long) 바꾸는 것을 의미합니다.\n\n\n\n\n\n\nTip\n\n\n\nmelt는 어떤 것을 녹이는 의미를 갖고 있습니다. 넓게 퍼진 데이터를 녹여 길게 만든다고 이해하시면 됩니다.\n\n\n이렇게 옆으로 나열된 column들을 특정한 축을 기준으로 데이터를 variable과 value라는 column으로 녹입니다. 이렇게 되면 long data의 축에 오는 데이터들은 계속 반복되고, variable과 value만 변경되는 데이터로 변환됩니다.\nmelt 함수에서는 기준이 되는 column의 이름과 길게 변환할 column을 지정해줄 수 있습니다.\n\nair_dt <- as.data.table(airquality)\nmelt(air_dt,\n     id.vars=c('Month','Day'),\n     measure.vars=c('Ozone','Temp'),\n     na.rm = T # TRUE일 경우 NA인 값은 제외\n     ) |> head()\n\n\n\n  \n\n\n\n만약 measure.vars에 아무런 인자를 주지 않는다면, melt를 수행했을 때, 기준 column을 제외한 모든 column들이 long 형태로 변경됩니다.\n\nair_melt <- melt(air_dt,\n     id.vars=c('Month','Day'),\n     na.rm = T # TRUE일 경우 NA인 값은 제외\n     )\nair_melt |> head()\n\n\n\n  \n\n\n\nmelt의 measure.vars에는 앞서 배웠던 patterns을 통해 비슷한 패턴을 갖는 column명을 선택할 수도 있습니다.\n\nmelt(dt,\n     id.vars = c('Gender','Age','Race1'),\n     measure.vars = patterns('HH') # HH가 들어가는 column 선택\n     ) |> head()\n\n\n\n  \n\n\n\n여러 개의 패턴을 지정해 두 가지 이상으로 정리할 수도 있습니다.\n\nhouse_dt <- data.table(\n  family = 1:5,\n  dob_child1 = c('1998-11-26','1996-06-22','2002-07-11','2004-10-10','2000-12-05'),\n  dob_child2 = c('2000-01-29',NA,'2004-04-05','2009-08-27','2005-02-28'),\n  name_child1 = c('Susan','Mark','Sam','Craig','Parker'),\n  name_child2 = c('Jose',NA,'Seth','Khai','Gracie')\n)\n\nhouse_dt |> \n    melt(\n        id.vars = 'family',\n        measure.vars = patterns(\"^dob\",\"^name\"),\n        value.name = c(\"dob\",'name')) |> head()\n\n\n\n  \n\n\n\n\n\n2) dcast\n이렇게 옆으로 나열된 column들을 특정한 축을 기준으로 데이터를 variable과 value라는 column으로 녹입니다.\n\n\n\n\n\n\nTip\n\n\n\ncast는 어떤 형태로 데이터를 굳히는 것을 의미합니다. 기존에 melt로 녹아있던 데이터를 casting 하는 data+casting (dcast)라고 이해하시면 됩니다.\n\n\ndcast는 long data에서 축 column에서 특정 값들을 기준으로 넓게 펼칠 수 있습니다. dcast에서는 formula 형태로 기준이 되는 축과 wide하게 변경해줄 column을 선택해주면 됩니다. 이 때 formula 형태라는 것은 ~ 을 기준으로 기준 축과 column\n\ndcast(data=air_melt,\n      formula = Month + Day ~ variable, # 기준 column ~ column 이름으로 들어갈 column\n      fill = 0, # NA인 경우 채워 넣어줄 값을 선택할 수 있습니다.\n      value.var = 'value' # 데이터로 들어갈 값들.\n        ) |> head()\n\n\n\n  \n\n\n\ndcast에는 fun.aggregate라는 인자가 존재합니다. 이 인자의 경우 만약 기준이 되는 column의 하나의 값에 여러 데이터(row)가 존재하는 경우에 사용합니다.\nairquality 데이터를 예로 들어보겠습니다. 앞서 보여드린 air_melt의 경우 Month와 Day의 두 column을 기준으로 melt했기 때문에 기준 column에 공통되는 값들이 없었습니다 (5/1~ 9/27 까지 일일 데이터이므로).\n이번에는 Day만 기준 column으로 놓고 melt를 한 데이터를 다시 dcast 해보겠습니다.\n\nair_melt2 <- melt(air_dt, id.vars = 'Day',measure.vars = c('Ozone','Solar.R','Wind','Temp'))\n\nair_melt2 |> head()\n\n\n\n  \n\n\n\nair_melt와 달리 air_melt2에서는 Day별로 여러 개의 데이터가 존재하게 됩니다 (Day가 1인 경우, 5월 1일, 6월 1일 ~9월 1일 등의 데이터가 있기 때문입니다).\n이 데이터를 dcast하게 된다면 Aggregate function missing, defaulting to 'length' 아래와 같은 메시지가 뜹니다.\n\ndcast(air_melt2, Day ~ variable, value.var='value') |> head()\n\n\n\n  \n\n\n\n이 메시지가 뜬 이유는 다음과 같습니다. Day라는 고유한 값은 1~31까지 밖에 없는데, air_melt2에서는 Day별로 여러 건의 데이터가 있었습니다. 그렇기 때문에 dcast에서는 Day별 요약 또는 집계를 수행합니다. 이 때 fun.aggregate라는 인자에 어떻게 요약할 것인지 지정해주지 않았기 때문에 초기값인 length()가 적용된 것입니다.\n따라서 평균이나 중앙값, 최소, 최대 등 요약함수를 넣어주면 Day별로 여러 건의 데이터를 요약한 값으로 데이터가 펼쳐지게 됩니다.\n\ndcast(air_melt2, Day ~ variable, \n      value.var='value',\n      fun.aggregate = mean) |> head()\n\n\n\n  \n\n\n\n만약 여러 건의 데이터 중 첫 번째를 사용하고 싶은 경우에는 아래와 같이 사용할 수 있습니다.\n\ndcast(air_melt2, Day ~ variable, \n      value.var='value',\n      fun.aggregate = function(x)x[1]) |> head()\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n가장 일반적으로 사용하는 데이터는 wide 형태의 데이터입니다.\n그러나 분석하고자 하는 유형, 필요한 데이터의 모양에 따라 데이터의 형태를 변환해줘야 할 때가 있습니다.\n\n\n\n함께 보면 좋은 자료\n\ndata.table 기초"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_colwise/index.html",
    "href": "blog/posts/2023-01-26-dplyr_colwise/index.html",
    "title": "dplyr 심화",
    "section": "",
    "text": "데이터 분석을 수행할 때, 여러 열에 대해 동일한 작업을 해주는 경우가 종종 있습니다. 그러나 동일한 코드를 복사해서 붙여넣기 하는 것은 굉장히 번거로운 일이고, 때로는 실수의 원인이 되기도 합니다.\n이번 글에서는 dplyr에서 여러 개의 열에 대해 동일한 작업을 수행할 수 있는 across() 에 대해 살펴보도록 하겠습니다.\ndplyr에 있는 storms 데이터를 통해 예시를 들어보겠습니다. 예를 들어 wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter 변수의 평균을 구한다고 해보겠습니다. 이 경우, 4가지 열에 대해 각각 mean(na.rm=T)를 입력해야 합니다.\n4개 정도면 괜찮지 않은가 생각할 수도 있지만, 만약 변수가 더 많아진다면? 훨씬 번거로울 수 밖에 없겠죠.\n\nlibrary(dplyr)\nstorms |> \n  group_by(name, month) |> \n  summarise(mean_wind = mean(wind, na.rm=T),\n            mean_pressure = mean(pressure,na.rm=T),\n            mean_tropical = mean(tropicalstorm_force_diameter, na.rm=T),\n            mean_hurricane_force = mean(hurricane_force_diameter, na.rm=T))\n\n\n\n  \n\n\n\n하지만 across() 를 사용할 경우, 위의 코드는 다음과 같이 입력할 수 있습니다.\n\nstorms |> \n  group_by(name,month) |> \n  summarise(across(wind:hurricane_force_diameter, mean, na.rm=T))"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_colwise/index.html#기본-사용법",
    "href": "blog/posts/2023-01-26-dplyr_colwise/index.html#기본-사용법",
    "title": "dplyr 심화",
    "section": "기본 사용법",
    "text": "기본 사용법\n본격적으로 across() 사용법에 대해 알아봅시다.\nacross() 에는 핵심적인 두 가지의 인자를 받을 수 있습니다.\n\n.cols: 함수를 적용시킬 열들을 입력합니다. 열의 위치, 이름, 유형을 통해 열들을 선택할 수 있습니다.\n.fns: 열들에 적용시킬 함수를 입력합니다. purrr 패키지 스타일의 식 ~.x/2와 같은 형태로도 입력 가능합니다.\n\nacross()는 주로 summarise() 와 함께 사용해 여러 열에 동일한 함수를 적용시켜줍니다.\n\n# 열의 유형으로 선택하기\nstarwars |> \n  summarise(across(where(is.character), n_distinct))\n\n\n\n  \n\n\n# 열의 이름으로 선택\nstarwars |> \n  group_by(species) |> \n  filter(n()>1) |> \n  summarise(across(c(sex,gender,homeworld), n_distinct))\n\n\n\n  \n\n\n# purrr 방식 함수 적용\nstarwars |> \n  group_by(species) |> \n  filter(n()>1) |> \n  summarise(across(is.numeric, ~mean(.x, na.rm=T)))"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_colwise/index.html#여러-함수-적용하기",
    "href": "blog/posts/2023-01-26-dplyr_colwise/index.html#여러-함수-적용하기",
    "title": "dplyr 심화",
    "section": "여러 함수 적용하기",
    "text": "여러 함수 적용하기\nacross()를 이용해 여러 열에 두 가지 이상의 함수를 적용할 수 있습니다.\n\nmin_max <- list(\n  min = ~min(.x, na.rm=T),\n  max = ~max(.x ,na.rm=T)\n)\n\nstarwars |> \n  group_by(species) |> \n  summarise(across(is.numeric,min_max))\n\n\n\n  \n\n\n\n여러 개의 함수를 적용한 경우, .names 를 통해 결과 데이터에서 출력되는 열의 이름을 변경합니다.\n\nstarwars |> \n  summarise(across(is.numeric, min_max, .names = \"{.fn}_{.col}\")) |> \n  relocate(starts_with('min'))"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_colwise/index.html#다른-함수와의-활용",
    "href": "blog/posts/2023-01-26-dplyr_colwise/index.html#다른-함수와의-활용",
    "title": "dplyr 심화",
    "section": "다른 함수와의 활용",
    "text": "다른 함수와의 활용\n\n1. mutate()\n\nmin_max_scale <- function(x){\n  m <- min(x, na.rm=T)\n  M <- max(x, na.rm=T)\n  return((x-m)/(M-m))\n}\n\ndf <- tibble(x=1:4, y=rnorm(4))\ndf |> mutate(\n  across(is.numeric, min_max_scale)\n)\n\n\n\n  \n\n\n\n\n\n2. distinct() , count()\ncount(), distinct() 와 같은 함수는 summarise() 를 생략할 수 있습니다.\ndistinct(): unique한 값 찾기\n\nstarwars |> distinct(across(contains('color')))\n\n\n\n  \n\n\n\ncount(): 수를 셀 때 사용하는 함수입니다. across()와 함께 사용할 경우, 조건에 해당하는 열들의 조합별로 수를 셉니다.\n\nstarwars |> count(across(contains('color')), sort = T)\n\n\n\n  \n\n\n\n\n\n3. filter()\nfilter()와 across()는 바로 사용할 수 없습니다. 사실 filter() 에서는 across()가 아닌 다른 함수를 통해 조건을 만족하는 값들을 출력해야 합니다.\n\nif_any() : 열들 중 하나의 열만 조건을 충족하면 선택합니다.\n\nstarwars |> \n  filter(if_any(everything(),~!is.na(.x)))\n\n\n\n  \n\n\n\nif_all() : 열들 중 모든 열들이 조건을 충족해야 선택합니다.\n\n\nstarwars |> \n  filter(if_all(everything(),~!is.na(.x)))"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_colwise/index.html#across-vs-_if-_at-_all",
    "href": "blog/posts/2023-01-26-dplyr_colwise/index.html#across-vs-_if-_at-_all",
    "title": "dplyr 심화",
    "section": "across() vs _if(), _at(), _all()",
    "text": "across() vs _if(), _at(), _all()\n_if(), _at(), _all()은 dplyr 이전 버전에서 쓰이던 함수들로, across() 처럼 여러 열에 대해 동시에 작업을 하기 위해 사용하는 함수들입니다.\nacross() 가 더 좋은 이유는 다음과 같습니다.\n\n여러 열들에 대해 특정 함수를 사용하여 요약할 수 있습니다.\n각 함수별로 _if(), _at(), _all() 이 존재했습니다. across()는 이런 함수들의 기능을 아우르기 때문에, 사용해야 할 함수의 숫자를 줄여줍니다.\n\nacross()와 _if(), _at(), _all() 을 대응시켜보면 다음과 같습니다. 예를 들어 mutate()를 통해 열들을 변화시키고자 할 경우,\n\n_if() numeric인 열들에 대해 평균을 계산하는 경우\n\nstarwars |> mutate_if(is.numeric, mean, na.rm=T) \n\n\n\n  \n\n\nstarwars |> mutate(across(is.numeric,mean, na.rm=T))\n\n\n\n  \n\n\n\n_at() 특정한 조건 만족하는 열들\n\n# 최빈값 출력 사용자 지정 함수\nMode <- function(x){\n  y <- names(which.max(table(x)))\n  return(y)\n}\nstarwars |> \n  mutate_at(vars(ends_with('color')),Mode) |> \n  select(ends_with('color'))\n\n\n\n  \n\n\nstarwars |> \n  mutate(across(ends_with('color'),Mode)) |> \n  select(ends_with('color'))\n\n\n\n  \n\n\n\n_all() 모든 열에 대해 함수 적용\n\ndf <- tibble(x=2, y=4, z=8)\ndf |> mutate_all(~.x/y)\n\n\n\n  \n\n\ndf |> mutate(across(everything(),~.x/y))\n\n\n\n  \n\n\n\n_all() 함수는 across() 안에 everything()을 사용하여 구현이 가능합니다.\n\n_all(), _if(), _at() 가 적용되던 함수들은 mutate() 뿐만 아니라 select(), summarise() 등에도 동일하게 적용할 수 있었습니다.\n이처럼 across() 하나만으로 _all(), _if(), _at() 함수들을 모두 구현 가능하기 때문에, 굳이 이전 버전의 함수들을 사용하지 않아도 될 것 같습니다."
  },
  {
    "objectID": "blog/posts/data.table_over_dplyr/index.html",
    "href": "blog/posts/data.table_over_dplyr/index.html",
    "title": "학습관점에서 비교하는 dplyr과 data.table",
    "section": "",
    "text": "R로 데이터 핸들링을 할 때, dplyr와 data.table을 많이 활용합니다. 저 역시 두 가지를 모두 사용하고 있습니다. 저는 R 입문자의 입장에서 봤을 때, 이 두 패키지를 일본어와 중국어에 빗대어 설명드리고 싶습니다.\n\n“일본어는 처음엔 쉬운데, 나중엔 갈수록 어려워져.\n반대로 중국어는 처음엔 어려운데, 나중엔 쉬워져.”\n\n중국어를 배울 때 자주 들었던 말입니다. 어문계열 출신이다보니 중국어를 배울 때 항상 일본어와 비교를 하는 말을 들었습니다. 일본어는 어순도 우리나라와 비슷하고, 애니메이션 등의 영향으로 많이 익숙하죠. 반면에 중국어는 어순도 한국어와 다르고, 무엇보다 성조도 있죠. 그래서 중국어에 대한 진입장벽이 일본어보다 더 높다고들 얘기했던 기억이 납니다.\n저는 dplyr와 data.table의 관계가 일본어와 중국어와 비슷하다고 생각합니다.\ndplyr는 처음엔 쉽습니다. dplyr는 함수 중심의 코드로 구성이 되어있습니다(Functional programming). 몇 개만 알면 기본적인 분석은 다 해낼 수 있습니다. 파이프 연산자를 이용해 코드를 읽어나가는 것도 더 쉽게 느낄 수 있습니다. 하지만 추가적인 작업을 하기 위해서는 여러 함수를 더 익혀야 합니다. 알아둬야하는 함수가 많아지고, 새로운 문법도 늘어납니다.\n반면에 data.table은 처음에 문법을 익히기가 어렵습니다. 기초 문법을 익힐 때, 어느 정도 난이도가 있습니다. 하지만 이 문법을 잘 이해하고 나면, 이것들을 잘 활용하기만 하면 끝입니다."
  },
  {
    "objectID": "blog/posts/data.table_over_dplyr/index.html#data.table을-추천하는-이유",
    "href": "blog/posts/data.table_over_dplyr/index.html#data.table을-추천하는-이유",
    "title": "학습관점에서 비교하는 dplyr과 data.table",
    "section": "data.table을 추천하는 이유",
    "text": "data.table을 추천하는 이유\n그렇기 때문에 처음 데이터 핸들링 패키지를 선택하라고 하면 dplyr보다는 data.table을 추천드립니다. dplyr보다 data.table을 추천드리는지 이유는 아래와 같습니다.\n\n1. 간결함\ndplyr보다 data.table의 코드가 더 간단합니다. 간단하다는 것은 입력하는 코드가 적은 것은 물론, 숙지해야 할 문법 (또는 함수)도 더 단순합니다.\ndplyr는 원하는 기능을 함수이름으로 설정해 놓았습니다. 그렇기 때문에 함수 자체를 익히는 것은 크게 어렵지 않습니다. 문제는 외워야 할 함수가 너무 많다는 것입니다. select(), mutate(), filter(), summarise(), filter(), recode(), case_when() 등등.. 익혀야 할 기본 함수들이 굉장히 많죠.\n또한 여러 가지 변수를 동시에 다루는 작업이라면 더욱 복잡해집니다. ~ 가 붙는 코드, .이 붙는 코드들이 생겨나면서 처음 배우는 사람들에게는 코드를 이해하는 데 어려움이 생깁니다.\n반면 data.table 안에서 기본적인 동작들은 함수가 아닌 문법을 통해 해결할 수 있습니다. [] 안에서 행과 열, 그룹을 다루는 문법을 처음에 잘 익혀두면 손쉽게 사용이 가능합니다. 또한 사용하는 함수의 종류도 크게 많지 않아 dplyr보다 쉽게 데이터를 핸들링할 수 있습니다.\n\n\n2. 패키지 의존성\ndplyr는 tidyverse라는 생태계에 속한 패키지입니다. 추가적인 동작을 사용하기 위해선 tidyr, broom 등과 같은 패키지를 설치해야만 합니다. 어느 패키지에 어떤 함수가 있는지 알아야 하고, 필요한 패키지를 추가적으로 설치해야 합니다. 그 패키지들도 설치하려면 다른 패키지가 필요해서 필요한 패키지 이외에도 여러 패키지들을 다운로드받아야 합니다.\n반면에 data.table은 data.table 패키지 하나만 설치하면 끝이죠. 이것저것 복잡하게 패키지를 다운받을 필요없이, data.table 하나의 패키지만 설치하면 웬만한 작업은 모두 수행할 수 있습니다."
  },
  {
    "objectID": "blog/posts/data.table_over_dplyr/index.html#왜-dplyr를-더-많이-쓸까",
    "href": "blog/posts/data.table_over_dplyr/index.html#왜-dplyr를-더-많이-쓸까",
    "title": "학습관점에서 비교하는 dplyr과 data.table",
    "section": "왜 dplyr를 더 많이 쓸까?",
    "text": "왜 dplyr를 더 많이 쓸까?\n그럼에도 불구하고, 일반적인 R 사용자들은 dplyr를 더 많이 쓰는 것 같습니다. 여러 이유가 있겠지만, 가장 중요한 것은 아마 UI일 것입니다.\n앞서 말씀드린 것처럼 data.table의 문법은 처음에 이해하기에 쉽지 않습니다. [], i, j, by 등등 처음 데이터 분석을 배우는 사람의 입장에서는 이것들을 바로 이해하는 게 쉽지 않겠죠.\ndplyr는 처음 봤을 때, 굉장히 쉽습니다. 영어 단어 몇 개만 이해하면 바로 써먹을 수 있습니다. %>%를 통해 여러 함수들을 연결시킴으로써 직관적으로 코드가 눈에 들어올테니까요.\n또한 Posit (구 RStudio) 사에서는 공식적으로 dplyr가 있는 tidyverse를 밀어주는 느낌이 없잖아 있습니다. tidyverse와 관련된 내용의 강의를 제공하고 있습니다. 처음 R을 접하는 사람들에게 dplyr로 유입되게끔 유도를 하는 것이지요.\n그렇기 때문에 dplyr의 사용자가 더 많을 것이고, 커뮤니티도 더 활성화가 되어있지 않나 생각합니다."
  },
  {
    "objectID": "blog/posts/data.table_over_dplyr/index.html#결론",
    "href": "blog/posts/data.table_over_dplyr/index.html#결론",
    "title": "학습관점에서 비교하는 dplyr과 data.table",
    "section": "결론",
    "text": "결론\n최근 dplyr를 다시 사용해야 할 일이 있어서 dplyr를 공부하다가 생각을 정리하고자 작성한 글입니다.\n비록 저 역시 data.table 패키지를 알기 전, dplyr를 먼저 접하게 되었습니다. (위에서 말한 것처럼) 커뮤니티가 온통 dplyr 세상이었거든요. 하지만 저는 주류와는 다른 선택을 하고 싶었고, 그래서 data.table을 찾아 공부했던 것 같습니다. 실제로 써보니 data.table이 만족도가 더 높았구요.\n입문자에게 data.table을 추천하기는 했지만, 나중에는 dplyr를 배워서, 둘 다 사용할 수 있는 것도 괜찮지 않을까요? ㅎㅎ"
  },
  {
    "objectID": "blog/posts/2023-01-25-colors/index.html",
    "href": "blog/posts/2023-01-25-colors/index.html",
    "title": "R에서 색상 다루기",
    "section": "",
    "text": "0. Intro\n색상은 시각화에서 가장 중요한 역할을 한다고 해도 과언이 아닙니다. 그래프의 색상에 따라 전달하고자 하는 의미, 전달되는 방법이 크게 달라질 수 있습니다.\n이번 글에서는 R에서 데이터 시각화를 위해 색상을 고르는 방법에 대해 살펴보겠습니다.\n\n\n1. colors()\nR에는 기본적으로 내장된 색상 이름이 657가지나 됩니다. 어떤 종류의 색상이 있는지 살펴보기 위해서는 colors() 를 실행해보면 됩니다.\n\ncolors()\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\"         \n\n\n657가지의 색상 중 원하는 색의 이름으로 그 색을 사용할 수 있습니다.\n우선 R에서 가장 많이 활용되는 시각화 패키지 ggplot2에 있는 diamonds 데이터로 예시를 들어보겠습니다.\n\nlibrary(ggplot2)\nggplot(diamonds,\n       aes(x=cut))+\n  geom_bar(fill=colors()[1:5])\n\n\n\n\n\ncolors()[1:5]\n\n[1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n[5] \"antiquewhite2\"\n\n\ncolors()의 1~5번째 값을 확인해보니, white, aliceblue, antiquewhite 등이었습니다. 이 색상들을 활용하여 각각의 막대 그래프 색을 지정해줬습니다.\n물론 하나의 색상만으로 막대그래프의 색상을 동일하게 지정해줄 수도 있습니다.\n\nlibrary(ggplot2)\nggplot(diamonds,\n       aes(x=cut))+\n  geom_bar(fill=colors()[657])\n\n\n\n\n각각의 색상들은 모두 자신만의 16자리의 값을 갖고 있습니다. 이 16자리 형태의 값들은 HEX color라고 불리며, 웹페이지에서 색상을 나타내기 위한 규격입니다.\n예시로는 #FFFFFF를 들 수 있겠습니다. HEX color는 맨 앞에 #이 오고, 숫자나 문자 6자리가 오게됩니다. 첫 두 자리는 적색 계열을, 다음 두 글자는 녹색, 마지막 두 글자는 청색 계열을 나타냅니다. 각각의 색상은 모두 00부터 FF까지의 값을 갖습니다.\n이처럼 각각의 색상별로 고유한 HEX 코드가 존재하지만, 우리에게는 색상의 이름으로 선택하는 것이 훨씬 쉽기 때문에, colors()를 이용해서 원하는 색상을 선택하여 시각화를 할 수 있습니다.\n\nggplot(diamonds, aes(x=color)) +\n  geom_bar(fill=c('red','orange','yellow','green','blue','navy', 'purple'))\n\n\n\n\n\n\n2. RColorBrewer()\n앞서 colors()를 이용해서는 표현하고자 하는 범주만큼 색상을 일일일 입력해줘야 했습니다.\n이러한 방법은 원하는 색상을 직접 선택할 수 있다는 장점이 있으나, 아무래도 번거롭다는 단점이 있습니다.\n그래서 R에서 저장된 색상모음들을 이용하는 것을 통해 번거로움을 해소할 수 있습니다. 바로 RColorBrewer()라는 패키지를 이용하는 방법입니다.\n\n# 패키지 설치\ninstall.packages('RColorBrewer')\n\nInstalling RColorBrewer [1.1-3] ...\n    OK [linked cache in 0.3 milliseconds]\n* Installed 1 package in 2.3 seconds.\n\n# 패키지 불러오기\nlibrary(RColorBrewer)\n\n패키지를 잘 불러왔다면, display.brewer.all()을 이용해 사용 가능한 모든 색상 팔레트를 확인할 수 있습니다.\n\ndisplay.brewer.all()\n\n\n\n\n\n\n팔레트에는 크게 세 종류가 있습니다.\n\n연속형 팔레트(Sequential palettes): 위 사진에서 첫 번째 그룹에 해당하는 팔레트입니다. 특정 색상이 점점 진해지는 특징을 갖고 있습니다. 순서대로 나열된 데이터를 시각화하기에 적합합니다.\n정성 팔레트(Qualitative palettes): 위 사진에서 두 번째 그룹에 해당하는 팔레트입니다. 클래스 간의 크기 차이를 의미하지 않으며, 색상은 클래스 간의 주요 시각적 차이를 만드는 데 사용됩니다. 질적 체계는 명목 또는 범주형 데이터를 표현하는 데 가장 적합합니다.\n다범주 팔레트(Diverging palettes): 위 사진에서 세 번째 그룹에 해당하는 팔레트입니다. 데이터가 갖는 범위의 양 끝~ 중간 범위의 값과 끝값을 똑같이 강조할 수 있습니다. 양 끝으로 갈수록 색이 진해지고, 중간으로 갈수록 색이 연해지므로, 양 끝의 값을 강조하는 데 사용할 수 있습니다.\n\nggplot을 이용할 때 팔레트를 이용하기 위해서는 scale_fill_brewer() 또는 scale_color_brewer() 를 이용해야 합니다.\n\nggplot(diamonds, aes(x=cut, fill=color)) +\n  geom_bar() + \n  scale_fill_brewer(palette = 'Spectral',direction = -1)\n\n\n\n\ndiamonds에서 color 변수는 범주별 등급을 나타냅니다. 양 극단의 color을 강조시키기 위해 때문에, 다범주 팔레트중 하나인 Spectral을 사용하였습니다.\n\n\n3. 패키지 내장 팔레트\n다른 팔레트를 이용하는 방법은 ggplot과 함께 사용할 수 있는 다른 패키지들을 이용하는 것입니다.\n\nlibrary(ggthemes)\nlibrary(ggsci)\n\n대표적으로는 ggthemes, ggsci와 같은 패키지가 있습니다. scale_fill_ 또는 scale_color_로 시작하는 함수를 이용해 다양한 팔레트를 선택할 수 있습니다.\n\nggplot(diamonds, aes(x=price, y=carat, color=cut)) +\n  geom_point() + \n  ggthemes::scale_color_pander()\n\n\n\n\n이 때 각 팔레트마다 가능한 범주의 수가 정해져 있으니, 잘 확인해보고 사용하는 것이 필요합니다.\n\n\n참고자료\n\nhttps://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3\nhttps://r-graph-gallery.com/38-rcolorbrewers-palettes.html"
  },
  {
    "objectID": "blog/posts/ggplot_combine_legend/index.html",
    "href": "blog/posts/ggplot_combine_legend/index.html",
    "title": "그래프 범례(legend) 통합하기",
    "section": "",
    "text": "library(ggplot2)\ndataA <- data.frame(\n  population=rep(c(\"High\", \"Low\"), time=5),\n  year=rep(c(\"2020_H\",\"2020_L\",\"2020_O\",\"2021_L\",\"2021_O\"), each=2),\n  x=c(10,20,30,40,50,60,70,80,90,100),\n  y=c(15,25,35,45,55,75,100,120,125,145)\n)\ndataA$group <- paste(dataA$year,dataA$population, sep=\" \")\n\ncolors <- c(\"Dark red\",\"Dark blue\", \"Orange\", \"purple\", \"dark green\")\nshapes <- c(21,24)\n\nggplot(data=dataA, aes(x=x, y=y))+\n  stat_smooth(method='lm', linetype=1, se=FALSE, formula=y~x, linewidth=0.5,\n              color=\"darkred\") +\n  geom_point(aes(fill=group, shape=group), size=4) +\n  scale_fill_manual(values= rep(colors,each=2),\n                    labels = dataA$group,\n                    name= \"Year & Pop.\") +\n  scale_shape_manual(values= rep(shapes, 5),\n                     labels = dataA$group,\n                     name= \"Year & Pop.\") +\n  scale_x_continuous(breaks=seq(0,100,20),limits=c(0,100)) +\n  scale_y_continuous(breaks=seq(0,150,50),limits=c(0,150)) +\n  # guides(shape=\"none\") +\n  theme_classic(base_size=16, base_family=\"serif\")+\n  theme(legend.position=c(0.85, 0.3),\n        legend.title=element_blank(),\n        legend.key=element_rect(color=alpha(\"grey\",.05), fill=alpha(\"grey\",.05)),\n        legend.background= element_rect(fill=alpha(\"grey\",.05)),\n        axis.line=element_line(linewidth=0.5, colour=\"black\"))"
  },
  {
    "objectID": "blog/posts/GT/index.html",
    "href": "blog/posts/GT/index.html",
    "title": "GT",
    "section": "",
    "text": "데이터프레임 등을 table로 변환하여 출력이 가능합니다.\n조건에 따라 각 셀이나 글자의 색을 변경할 수 있습니다."
  },
  {
    "objectID": "blog/posts/GT/index.html#table-구조",
    "href": "blog/posts/GT/index.html#table-구조",
    "title": "GT",
    "section": "Table 구조",
    "text": "Table 구조\ngt에서의 테이블 구조는 다음과 같습니다.\n\n\n\n\n\n\n\n\ngt 로 테이블을 만들기 위해 gt 패키지 내부에 있는 gtcars 데이터를 활용하도록 하겠습니다.\ngtcars는 mtcars처럼 차량 데이터이지만, 좀더 최근의 슈퍼카 데이터로 구성되어 있습니다.\n\nlibrary(gt)\ngtcar_dt <- as.data.table(gtcars)[,.(ctry_origin, mfr, model,year, trim, msrp)]\ngtcar_dt <- gtcar_dt[gtcar_dt[,.I[1:2],by=.(ctry_origin)]$V1]\ngtcar_dt[,id := seq_len(.N)]\ngtcar_dt |> \n  gt(rowname_col = \"id\") |> \n  tab_header(\n    title = md(\"**Super cars**\"),\n    subtitle = md(\"Find your *Dream* car!\")\n  ) |> \n  tab_footnote(\n    footnote = \"Manufacturer\",\n    locations = cells_column_labels(columns = mfr)\n  ) |> \n  tab_footnote(\n    footnote = \"Units: $\",\n    locations = cells_column_labels(msrp)\n  ) |> \n  tab_source_note(\n    source_note = md(\"URL: https://gt.rstudio.com/articles/intro-creating-gt-tables.html\")\n  )\n\n\n\n\n\n  \n    \n      Super cars\n    \n    \n      Find your Dream car!\n    \n    \n      \n      ctry_origin\n      mfr1\n      model\n      year\n      trim\n      msrp2\n    \n  \n  \n    1\nUnited States\nFord\nGT\n2017\nBase Coupe\n447000\n    2\nUnited States\nChevrolet\nCorvette\n2016\nZ06 Coupe\n88345\n    3\nItaly\nFerrari\n458 Speciale\n2015\nBase Coupe\n291744\n    4\nItaly\nFerrari\n458 Spider\n2015\nBase\n263553\n    5\nJapan\nAcura\nNSX\n2017\nBase Coupe\n156000\n    6\nJapan\nNissan\nGT-R\n2016\nPremium Coupe\n101770\n    7\nUnited Kingdom\nBentley\nContinental GT\n2016\nV8 Coupe\n198500\n    8\nUnited Kingdom\nAston Martin\nDB11\n2017\nBase Coupe\n211195\n    9\nGermany\nBMW\n6-Series\n2016\n640 I Coupe\n77300\n    10\nGermany\nBMW\ni8\n2016\nMega World Coupe\n140700\n  \n  \n    \n      URL: https://gt.rstudio.com/articles/intro-creating-gt-tables.html\n    \n  \n  \n    \n      1 Manufacturer\n    \n    \n      2 Units: $"
  },
  {
    "objectID": "blog/posts/GT/index.html#stub",
    "href": "blog/posts/GT/index.html#stub",
    "title": "GT",
    "section": "Stub",
    "text": "Stub\nstub은 행의 이름과 연관된 항목들입니다. 예를 들어, gtcars에서는 각 자동차 브랜드들의 국적을 그룹으로 나누어 표시할 수 있겠죠. 행을 그룹으로 묶어 분류를 해보도록 하겠습니다.\n\ngtcar_dt |> \n  gt(rowname_col = \"id\") |> \n  tab_stubhead(label=\"No\") |> \n  tab_row_group(\n    label = \"USA\",\n    rows = 1:2\n  ) |> \n  tab_row_group(\n    label = \"Italy\",\n    rows = 3:4\n  ) |>  as_raw_html()\n\n\n\n  \n  \n  \n    \n    \n      No\n      ctry_origin\n      mfr\n      model\n      year\n      trim\n      msrp\n    \n  \n  \n    \n      Italy\n    \n    3\nItaly\nFerrari\n458 Speciale\n2015\nBase Coupe\n291744\n    4\nItaly\nFerrari\n458 Spider\n2015\nBase\n263553\n    \n      USA\n    \n    1\nUnited States\nFord\nGT\n2017\nBase Coupe\n447000\n    2\nUnited States\nChevrolet\nCorvette\n2016\nZ06 Coupe\n88345\n    \n      \n    \n    5\nJapan\nAcura\nNSX\n2017\nBase Coupe\n156000\n    6\nJapan\nNissan\nGT-R\n2016\nPremium Coupe\n101770\n    7\nUnited Kingdom\nBentley\nContinental GT\n2016\nV8 Coupe\n198500\n    8\nUnited Kingdom\nAston Martin\nDB11\n2017\nBase Coupe\n211195\n    9\nGermany\nBMW\n6-Series\n2016\n640 I Coupe\n77300\n    10\nGermany\nBMW\ni8\n2016\nMega World Coupe\n140700\n  \n  \n  \n\n\n\n\n위의 예시에서는 tab_row_group()을 이용해 행 그룹의 이름과 행의 번호를 지정해주었습니다. 보다 편리하게 이 작업을 수행하기 위해 group_by()를 이용해 행들을 묶을 수 있습니다.\n\nlibrary(dplyr)\ngtcar_dt |> \n  group_by(ctry_origin) |> \n  arrange(mfr, desc(msrp)) |> \n  gt(rowname_col = \"id\")\n\n\n\n\n\n  \n    \n    \n      \n      mfr\n      model\n      year\n      trim\n      msrp\n    \n  \n  \n    \n      Japan\n    \n    5\nAcura\nNSX\n2017\nBase Coupe\n156000\n    6\nNissan\nGT-R\n2016\nPremium Coupe\n101770\n    \n      United Kingdom\n    \n    8\nAston Martin\nDB11\n2017\nBase Coupe\n211195\n    7\nBentley\nContinental GT\n2016\nV8 Coupe\n198500\n    \n      Germany\n    \n    10\nBMW\ni8\n2016\nMega World Coupe\n140700\n    9\nBMW\n6-Series\n2016\n640 I Coupe\n77300\n    \n      United States\n    \n    2\nChevrolet\nCorvette\n2016\nZ06 Coupe\n88345\n    1\nFord\nGT\n2017\nBase Coupe\n447000\n    \n      Italy\n    \n    3\nFerrari\n458 Speciale\n2015\nBase Coupe\n291744\n    4\nFerrari\n458 Spider\n2015\nBase\n263553\n  \n  \n  \n\n\n\n\n\ngtcar_dt |> \n  arrange(mfr, desc(msrp)) |> \n  mutate(car = paste(mfr, model)) |> \n  select(-mfr, -model, -id) |> \n  group_by(ctry_origin) |> \n  gt(rowname_col = \"car\")\n\n\n\n\n\n  \n    \n    \n      \n      year\n      trim\n      msrp\n    \n  \n  \n    \n      Japan\n    \n    Acura NSX\n2017\nBase Coupe\n156000\n    Nissan GT-R\n2016\nPremium Coupe\n101770\n    \n      United Kingdom\n    \n    Aston Martin DB11\n2017\nBase Coupe\n211195\n    Bentley Continental GT\n2016\nV8 Coupe\n198500\n    \n      Germany\n    \n    BMW i8\n2016\nMega World Coupe\n140700\n    BMW 6-Series\n2016\n640 I Coupe\n77300\n    \n      United States\n    \n    Chevrolet Corvette\n2016\nZ06 Coupe\n88345\n    Ford GT\n2017\nBase Coupe\n447000\n    \n      Italy\n    \n    Ferrari 458 Speciale\n2015\nBase Coupe\n291744\n    Ferrari 458 Spider\n2015\nBase\n263553"
  },
  {
    "objectID": "blog/posts/GT/index.html#column-labels",
    "href": "blog/posts/GT/index.html#column-labels",
    "title": "GT",
    "section": "Column labels",
    "text": "Column labels\n\ntable의 열 다루기\n\nColumn 묶기\n\nair_dt <- airquality |> head(10)\n\nair_tbl <- air_dt |> \n  gt() |> \n  tab_header(\n    title = \"New York Air Quality Measurements\"\n  ) |> \n  tab_spanner(\n    label=\"Measurement\",\n    columns = c(Ozone, Solar.R, Wind, Temp)\n  ) |> \n  tab_spanner(\n    label=\"Time\",\n    columns = c(Month, Day)\n  )\nair_tbl\n\n\n\n\n\n  \n    \n      New York Air Quality Measurements\n    \n    \n    \n      \n        Measurement\n      \n      \n        Time\n      \n    \n    \n      Ozone\n      Solar.R\n      Wind\n      Temp\n      Month\n      Day\n    \n  \n  \n    41\n190\n7.4\n67\n5\n1\n    36\n118\n8.0\n72\n5\n2\n    12\n149\n12.6\n74\n5\n3\n    18\n313\n11.5\n62\n5\n4\n    NA\nNA\n14.3\n56\n5\n5\n    28\nNA\n14.9\n66\n5\n6\n    23\n299\n8.6\n65\n5\n7\n    19\n99\n13.8\n59\n5\n8\n    8\n19\n20.1\n61\n5\n9\n    NA\n194\n8.6\n69\n5\n10"
  },
  {
    "objectID": "blog/posts/GT/index.html#column-이동-및-이름-수정",
    "href": "blog/posts/GT/index.html#column-이동-및-이름-수정",
    "title": "GT",
    "section": "Column 이동 및 이름 수정",
    "text": "Column 이동 및 이름 수정\n\nair_tbl <- air_tbl |> \n  cols_move_to_start(\n    columns = c(Month, Day)\n  ) |> \n  cols_label(\n    Ozone = html(\"Ozone,<br>ppbV\"),\n    Solar.R = html(\"Solar R.,<br>cal/m<sup>2</sup>\"),\n    Wind = html(\"Wind,<br>mph\"),\n    Temp = html(\"Temp,<br>&deg;F\")\n  )\n\nair_tbl\n\n\n\n\n\n  \n    \n      New York Air Quality Measurements\n    \n    \n    \n      \n        Time\n      \n      \n        Measurement\n      \n    \n    \n      Month\n      Day\n      Ozone,ppbV\n      Solar R.,cal/m2\n      Wind,mph\n      Temp,°F\n    \n  \n  \n    5\n1\n41\n190\n7.4\n67\n    5\n2\n36\n118\n8.0\n72\n    5\n3\n12\n149\n12.6\n74\n    5\n4\n18\n313\n11.5\n62\n    5\n5\nNA\nNA\n14.3\n56\n    5\n6\n28\nNA\n14.9\n66\n    5\n7\n23\n299\n8.6\n65\n    5\n8\n19\n99\n13.8\n59\n    5\n9\n8\n19\n20.1\n61\n    5\n10\nNA\n194\n8.6\n69"
  },
  {
    "objectID": "blog/posts/GT/index.html#column-정렬",
    "href": "blog/posts/GT/index.html#column-정렬",
    "title": "GT",
    "section": "Column 정렬",
    "text": "Column 정렬\n\nair_tbl |> \n  cols_align(\n    align = \"center\",\n    columns = c(Month, Day)\n  )\n\n\n\n\n\n  \n    \n      New York Air Quality Measurements\n    \n    \n    \n      \n        Time\n      \n      \n        Measurement\n      \n    \n    \n      Month\n      Day\n      Ozone,ppbV\n      Solar R.,cal/m2\n      Wind,mph\n      Temp,°F\n    \n  \n  \n    5\n1\n41\n190\n7.4\n67\n    5\n2\n36\n118\n8.0\n72\n    5\n3\n12\n149\n12.6\n74\n    5\n4\n18\n313\n11.5\n62\n    5\n5\nNA\nNA\n14.3\n56\n    5\n6\n28\nNA\n14.9\n66\n    5\n7\n23\n299\n8.6\n65\n    5\n8\n19\n99\n13.8\n59\n    5\n9\n8\n19\n20.1\n61\n    5\n10\nNA\n194\n8.6\n69"
  },
  {
    "objectID": "blog/posts/GT/index.html#column-값에-따른-스타일-변경-1",
    "href": "blog/posts/GT/index.html#column-값에-따른-스타일-변경-1",
    "title": "GT",
    "section": "Column 값에 따른 스타일 변경 1",
    "text": "Column 값에 따른 스타일 변경 1\n\n조건에 따라 색상 지정\n\n\nair_dt |> \n  gt() |> \n  tab_style(\n    style = list(cell_fill(color=\"red\"),\n                 cell_text(weight=\"bold\",color=\"white\")),\n    locations = cells_body(columns = Wind,\n                           rows = Wind > 10 )\n  ) |> \n  tab_style(\n    style = list(cell_fill(color=\"blue\"),\n                 cell_text(weight=\"bold\",color=\"white\")),\n    locations = cells_body(columns = Wind,\n                           rows = Wind < 10 )\n  )\n\n\n\n\n\n  \n    \n    \n      Ozone\n      Solar.R\n      Wind\n      Temp\n      Month\n      Day\n    \n  \n  \n    41\n190\n7.4\n67\n5\n1\n    36\n118\n8.0\n72\n5\n2\n    12\n149\n12.6\n74\n5\n3\n    18\n313\n11.5\n62\n5\n4\n    NA\nNA\n14.3\n56\n5\n5\n    28\nNA\n14.9\n66\n5\n6\n    23\n299\n8.6\n65\n5\n7\n    19\n99\n13.8\n59\n5\n8\n    8\n19\n20.1\n61\n5\n9\n    NA\n194\n8.6\n69\n5\n10"
  },
  {
    "objectID": "blog/posts/GT/index.html#column-값에-따른-스타일-변경-2",
    "href": "blog/posts/GT/index.html#column-값에-따른-스타일-변경-2",
    "title": "GT",
    "section": "Column 값에 따른 스타일 변경 2",
    "text": "Column 값에 따른 스타일 변경 2\n\n범위 조건으로 색상 지정 (연속형)\n\n\nair_dt |> \n  gt() |> \n  data_color(Wind,\n             direction = \"column\",\n             palette=c(\"red\",\"white\",\"blue\"),\n             domain=c(7,25)\n             )\n\n\n\n\n\n  \n    \n    \n      Ozone\n      Solar.R\n      Wind\n      Temp\n      Month\n      Day\n    \n  \n  \n    41\n190\n7.4\n67\n5\n1\n    36\n118\n8.0\n72\n5\n2\n    12\n149\n12.6\n74\n5\n3\n    18\n313\n11.5\n62\n5\n4\n    NA\nNA\n14.3\n56\n5\n5\n    28\nNA\n14.9\n66\n5\n6\n    23\n299\n8.6\n65\n5\n7\n    19\n99\n13.8\n59\n5\n8\n    8\n19\n20.1\n61\n5\n9\n    NA\n194\n8.6\n69\n5\n10\n  \n  \n  \n\n\n\n\n\ncols_hide() #숨기기"
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html",
    "href": "blog/posts/PSM_matching/index.html",
    "title": "성향점수 매칭 (PSM)",
    "section": "",
    "text": "성향점수 매칭 (Propensity Score Matching,PSM) 은 관찰 연구에서 사용되는 통계기법 중 하나로서, 무작위 대조군 연구 (Randomized control trials, RCT)가 불가능할 때 치료제나 중재(intervention)의 효과를 평가하기 위해 사용됩니다.\nPSM은 치료제나 중재의 효과가 결과 변수(예시. 질병의 발생, 사망 등)에 미치는 영향을 확인하고자 할 때, 그 인과관계에 영향을 미칠 수 있는 변수들의 편향(bias)를 줄이는 것을 목표로 합니다. 이런 변수들을 교란 변수라고 합니다.\nPSM의 핵심은 성별, 나이 등과 같이 관측된 특성을 바탕으로 치료 또는 중재를 받을 확률인 성향점수(Propensity score)를 계산하는 것입니다. 이 성향점수를 계산하기 위해서는 주로 로지스틱 회귀분석이 사용됩니다. 성향점수가 계산되면, 성향점수를 바탕으로 치료를 받은 환자들과 받지 않은 환자들이 매칭되게 됩니다.\n이렇게 PSM을 통해 치료를 받은 집단과 받지 않은 집단의 차이를 최대한 줄인 뒤에 치료나 중재의 인과성(causal effect)을 확인할 수 있습니다.\n이번 포스트에서는 R을 활용해 PSM을 수행하는 방법에 대해 살펴보도록 하겠습니다.\nPSM을 수행하기 위해 필요한 패키지는 MatchIt과 cobalt입니다. MatchIt은 PSM을 수행하기 위한 패키지, cobalt는 PSM 결과를 시각화하기 위한 패키지입니다.\nPSM 설명을 위해 MatchIt 패키지에 있는 lalonde 데이터를 사용하겠습니다.\nlalonde 데이터의 변수는 다음과 같이 구성되어있습니다.\nPSM은 크게 계획 매칭 평가 세 단계로 구분되는데, 이들 각각에 대해 살펴보도록 하겠습니다."
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#계획",
    "href": "blog/posts/PSM_matching/index.html#계획",
    "title": "성향점수 매칭 (PSM)",
    "section": "1. 계획",
    "text": "1. 계획\n계획단계는 treatment에 대해 아래의 내용을 선택하는 단계입니다.\n\n측정될 효과(effect) 어떤 변수의 효과를 평가할 것인지 결정해야 합니다. 예를 들어서 특정 약물의 사용 여부에 따른 환자들의 질병 발생이나 사망에 미치는 영향을 분석하고자 할 때, 특정 약물의 사용이라는 변수를 기준으로 층화하여 매칭을 하는 것이 필요합니다.\n목표 대상 (target population) 목표 대상이란 연구 결과를 적용하고 싶은 대상을 의미합니다. 일반적으로 랜덤 샘플링한 대상에게 연구 결과를 적용하는데, 만약 샘플이 랜덤하지 않다면 목표 대상을 설정하기 어렵습니다.\n\nAverage treatment effect in the population (ATE): 목표 대상에 있는 모든 대상들에 대한 치료제의 평균 효과\nAverage treatment effect in the treated (ATT): 치료를 실제로 받은 사람들과 비슷한 사람들에 대한 치료제의 평균 효과 대부분의 매칭 방법은 ATT를 평가하는 것이 더 낫지만, 일부 매칭은 ATE도 사용 가능합니다.\n\n균형을 맞출 공변량 (covariates)\n\n공변량을 선택하는 것은 치료제의 인과관계를 타당하게 해석하기 위해 교란효과를 없애기 위한 확실한 방법이기 때문에 중요합니다. 총 인과를 평가하기 위해서 모든 공변량들은 치료제 사용 이전에 측정이 되어야 합니다. 이상적으로 공변량은 결측값(missingness)이 없어야 합니다.\n\n\n우선 매칭 이전에 기존 데이터의 불균형을 확인해보겠습니다.\n\ninitial <- matchit(treat ~ age + educ + race + married + nodegree + re74 + re75, data = lalonde, method=NULL, distance = \"glm\")\n\nsummary(initial)\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nPSM 이전의 treat과 control 간의 불균형을 확인할 수 있는 지표는 Standardized mean difference (Std. Mean Diff, SMD)입니다. SMD는 절댓값이 0에 가까우면 가까울수록 treat과 control 간의 균형이 맞는다고 이야기합니다. 통상적으로 연구에서 균형이 잡혔다고 말하는 SMD의 수준은 절댓값 기준 0.1 이하입니다."
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#매칭",
    "href": "blog/posts/PSM_matching/index.html#매칭",
    "title": "성향점수 매칭 (PSM)",
    "section": "2. 매칭",
    "text": "2. 매칭\n이제 매칭을 진행해보겠습니다. 매칭에는 다양한 클래스들과 방법들이 존재합니다. 여기서는 1:1 nearest neighbor (NN)매칭을 통해 성향점수를 계산해보도록 하겠습니다.\nNN 방법을 통해 각 treated 데이터는 가장 가까운 성향점수를 갖는 control 데이터와 짝지어게 됩니다. 짝지어지지 못한 데이터들은 매칭되지 않은 것으로 간주되어 추후 분석에서 제외됩니다.\n\nm1 <- matchit(treat ~ age + educ + race + married + nodegree + re74 + re75, \n              data = lalonde, \n              distance = \"glm\",\n              method=\"nearest\")\nm1\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nmatchit() 에 사용되는 인자들은 크게 다음과 같습니다.\n\ndistance: 치료군과 대조군 간의 거리를 측정하기 위한 방법. 기본값은 glm 입니다. 그 외에 마할라노비스 거리(mahalanobis), 유클리드 거리 등이 올 수 있습니다.\nmethods: 매칭 방법입니다. 대표적으로 nearest, optimal 등이 있습니다.\nreplace: 매칭 시 한 번 매칭된 대조군을 다른 치료군과의 매칭에도 사용할 지 여부를 결정합니다.\ncaliper: 치료군과 대조군 간 매칭을 할 때 허용되는 차이를 입력합니다. 통상적으로 0.2가 많이 사용됩니다.\nratio: 치료군과 대조군 간의 매칭 비율. 2이면 1:2 매칭, 3이면 1:3 매칭이 진행됩니다.\n\nmatchit을 통해 만들어진 m1 객체의 핵심은 weights(매칭 가중치), subclass (매칭 쌍), distance (성향점수 평가), 그리고 match.matrix (treat에 매칭된 control 데이터)입니다."
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#matching-방법-method",
    "href": "blog/posts/PSM_matching/index.html#matching-방법-method",
    "title": "성향점수 매칭 (PSM)",
    "section": "matching 방법 (method)",
    "text": "matching 방법 (method)\n\nnearest: 최근접 이웃 매칭은 매칭 시 가장 많이 활용되는 방법으로, greedy 매칭이라고도 불립니다. 이는 치료군과 대조군의 쌍이 어떻게 매칭됐는지 또는 어떻게 매칭 될지 참고하지 않은 채로 매칭을 진행하기 때문입니다. 따라서 최근접 이웃 매칭은 최적화와 거리가 먼 방법입니다. 이름에서도 알 수 있듯이, 각 치료군과 가장 가까운 거리에 있는 대조군의 데이터를 짝짓게 됩니다. 이 때 거리가 가깝다는 것은 기본적으로 성향점수의 차이를 바탕으로 판단합니다. 즉 치료군과 통제군 간의 성향점수의 차이가 가장 적은 것들을 매칭하게 됩니다.\noptimal: optimal pair 매칭은 optimal 매칭이라고 줄여서 부르는데, 각 치료군과 하나 이상의 대조군을 짝지으려 한다는 점에서 nearest neighbor 매칭과 굉장히 유사합니다. 그러나 최근접 이웃 매칭과 달리, greedy 하지 않고 optimal 하다는 점에서 차이가 있습니다. 이 때 optimal 하다는 것은 각 쌍의 간격의 절댓갑의 합을 최적화하는 것입니다.\n\n\n\n\n\n\n\nNote\n\n\n\n이 외에도 full, genetic, exact 등의 매칭방법이 있습니다. 해당 매칭 방법들은 추후 작성할 예정입니다."
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#평가",
    "href": "blog/posts/PSM_matching/index.html#평가",
    "title": "성향점수 매칭 (PSM)",
    "section": "3. 평가",
    "text": "3. 평가\n매칭 이후에는 매칭이 잘 되었는지 평가가 필요합니다. 만약 매칭을 한 이후에도 공변량의 균형이 맞지 않다면 매칭은 실패한 것으로 간주되고 다른 매칭을 시도해야 합니다.\n매칭 객체 평가 방법은 두 단계로 나누어볼 수 있습니다. 첫 번째는 summary를 통해 공변량들의 SMD 절댓값이 0.1 미만인지 확인하는 것입니다.\n\nsummary(m1)\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\n두 번째는 그래프를 이용해 매칭 전 SMD와 매칭 후의 SMD를 비교하는 것입니다. 여기서 cobalt 패키지를 이용하게 됩니다.\n\nlove.plot(m1, \n          abs=T, \n          thresholds = 0.1,\n          drop.distance = T,\n          binary='std',\n          position='bottom',\n          var.order = 'unadjusted',\n          # var.names = new.name,\n          size = 2,\n          shapes=c('circle filled','circle')\n          )\n\n\n\n\n위의 그래프를 살펴보면, 매칭을 통해 adjusted된 공변량들의 SMD(검은 점)가 점선으로 나타난 0.1 이상인 경우가 꽤 많은 것을 알 수 있습니다. 즉 매칭이 제대로 이루어지지 않은 것이죠. 이럴 경우 다른 방법으로 매칭을 시도해, 치료군과 대조군을 올바르게 매칭해주는 것이 필요합니다.\n\nm2 <- matchit(treat ~ age + educ + race + married + nodegree + re74 + re75, \n              data = lalonde, \n              distance = \"glm\",\n              method=\"nearest\",\n              caliper=.2\n              )\nlove.plot(m2, \n          abs=T, \n          thresholds = 0.1,\n          drop.distance = T,\n          binary='std',\n          position='bottom',\n          var.order = 'unadjusted',\n          # var.names = new.name,\n          size = 2,\n          shapes=c('circle filled','circle')\n          )\n\n\n\n\ncaliper를 0.2로 설정하여 매칭 범위를 넓혔습니다. 다시 말해 성향점수의 차이가 0.2보다 작은 경우는 매칭을 모두 허용했다는 뜻입니다. 그 결과, 비록 일부 변수는 여전히 SMD가 0.1 이상이지만, 첫 번째 매칭보다 SMD가 많이 줄어든 것을 알 수 있습니다."
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#활용",
    "href": "blog/posts/PSM_matching/index.html#활용",
    "title": "성향점수 매칭 (PSM)",
    "section": "활용",
    "text": "활용\nMatchIt을 통해 매칭이 잘 이루어졌다면 match.data() 또는 get_matches()를 이용해 매칭된 데이터를 데이터프레임으로 만들어줄 수 있습니다. 이후 회귀분석, 생존분석 등 추가 모델링 작업을 진행해주면 됩니다.\n\n\n\n\n\n\nNote\n\n\n\nmatch.data()과 get_matches() 모두 매칭 이후 거리(distance), 가중치(weights), 하위그룹(subclass) 변수들이 추가된 데이터프레임을 반환합니다.\n일반적으로 get_matches()는 매칭 시 대조군을 재사용했을 때(replace=TRUE) 활용하고, 나머지 경우에는 match.data()를 사용합니다.\n\n\n\nmatched_df <- match.data(m1)"
  },
  {
    "objectID": "blog/posts/PSM_matching/index.html#참고자료",
    "href": "blog/posts/PSM_matching/index.html#참고자료",
    "title": "성향점수 매칭 (PSM)",
    "section": "참고자료",
    "text": "참고자료\n\nhttps://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html\nhttps://ngreifer.github.io/cobalt/"
  },
  {
    "objectID": "blog/posts/2023-02-01-boxplot/index.html",
    "href": "blog/posts/2023-02-01-boxplot/index.html",
    "title": "ggplot boxplot 그리기",
    "section": "",
    "text": "library(ggplot2)\n\nggplot으로 boxplot을 그릴 때, outlier 제거하기\n\ndiamonds |> \n  ggplot(aes(x=cut, y=price))+\n  geom_boxplot(outlier.shape=NA #outlier 제거하기\n               ) \n\n\n\n\nboxplot에 errorbar 표시\n\ndiamonds |> \n  ggplot(aes(x=cut, y=price)) + \n  stat_boxplot(geom='errorbar') +\n  geom_boxplot(outlier.shape=NA #outlier 제거하기\n               )"
  },
  {
    "objectID": "blog/posts/performace_comparison/index.html#base-r",
    "href": "blog/posts/performace_comparison/index.html#base-r",
    "title": "주요 패키지의 성능비교",
    "section": "1. base R",
    "text": "1. base R\n외부 패키지 없이 그룹 별로 요약값을 계산하려면 aggregate()을 사용해야 합니다.\n\nbaseR = aggregate(diamonds[,c(\"price\", \"depth\",\"carat\")],\n            by = list(diamonds$color, diamonds$cut),\n            FUN = mean)"
  },
  {
    "objectID": "blog/posts/performace_comparison/index.html#dplyr",
    "href": "blog/posts/performace_comparison/index.html#dplyr",
    "title": "주요 패키지의 성능비교",
    "section": "2. dplyr",
    "text": "2. dplyr\n가장 많은 R 사용자들이 활용할 것으로 예상되는 dplyr의 방법입니다. dplyr의 경우, 최근에 업데이트된 1.1 버전과 1.0 버전을 가지고 비교를 해보았습니다.\n\n1) dplyr 1.0\ndplyr에서 group_by() 와 summarise()를 통해 분석을 그룹 별 요약 값을 계산하는 방법입니다.\n\ndiamonds |>\n    group_by(color, cut) |> \n    summarise(across(c(price, depth, carat), mean, na.rm=T)) |> \n    ungroup()\n\n\n\n2) dplyr 1.1\ndplyr 버전 1.1에서부터는 group_by()를 따로 사용하지 않더라도, 각각의 함수에서 .by 인자를 통해 그룹 변수를 지정해줄 수 있게 되었습니다.\n\ndiamonds |> \n    summarise(across(c(price, depth, carat), mean),\n              .by=c(color, cut))"
  },
  {
    "objectID": "blog/posts/performace_comparison/index.html#dtplyr",
    "href": "blog/posts/performace_comparison/index.html#dtplyr",
    "title": "주요 패키지의 성능비교",
    "section": "3. dtplyr",
    "text": "3. dtplyr\ndtplyr는 dplyr의 코드와 data.table의 backend를 합쳐놓은 패키지입니다. dplyr와 같은 코드를 활용해 data.table과 같이 빠른 성능을 낼 수 있는 패키지입니다.\n\n\n\n\n\n\nWarning\n\n\n\n비록 dtplyr가 data.table처럼 빠른 성능을 보여줄 수 있다고는 하나, 순수한 data.table보다는 성능이 떨어지는 것으로 알려져 있습니다. 그 이유는 dplyr의 코드를 data.table 문법으로 변경하는 과정에서 걸리는 시간 때문입니다.1\n\n\ndtplyr에서는 lazy_dt()를 이용해 데이터를 dtplyr 클래스로 만들어줍니다.\n\nlazy_dt(diamonds) |> \n    summarise(across(c(price, depth, carat), mean))"
  },
  {
    "objectID": "blog/posts/performace_comparison/index.html#data.table",
    "href": "blog/posts/performace_comparison/index.html#data.table",
    "title": "주요 패키지의 성능비교",
    "section": "4. data.table",
    "text": "4. data.table\n속도로 잘 알려진 data.table 입니다.\ndata.table 같은 경우, 총 4개의 버전으로 비교해보았습니다.\n\n1) j, by만 이용\n\nas.data.table(diamonds)[,.(mean_price = mean(price),\n                     mean_depth = mean(depth),\n                     mean_carat = mean(carat)), \n                  by=.(color, cut)]\n\n\n\n2) lapply + .SD 이용\nlapply + .SD 문법을 이용하여 계산하고자 하는 변수들을 동시에 입력해줍니다.\n\nas.data.table(diamonds)[,lapply(.SD, mean),.SDcols=c(\"price\",\"depth\",\"carat\"), by=.(color, cut)]\n\n\n\n3) lapply + .SD + keyby\ndata.table의 장점 중 하나는 key를 설정해 데이터를 정렬할 수 있다는 것이죠. 마찬가지로 keyby를 이용해 그룹 변수를 key로 만들어줌으로써, 그룹 변수를 기준으로 데이터를 정렬합니다.\n\nas.data.table(diamonds)[,lapply(.SD, mean),.SDcols=c(\"price\",\"depth\",\"carat\"), keyby=.(color, cut)]\n\n\n\n4) data.table 개발 버전 (1.14.9)\n현재 github에 공개되어있는 1.14.9 버전에서는 data.table 클래스가 아닌 데이터셋도 DT()를 이용해 data.table 문법을 적용할 수 있습니다.\n\ndiamonds |> DT(,.(mean_price = mean(price),\n                     mean_depth = mean(depth),\n                     mean_carat = mean(carat)),\n           by=.(color, cut))\n\n이제, microbenchmark 패키지를 이용해 성능 비교를 진행한 뒤, 이를 boxplot으로 그려 시각화를 해보겠습니다.\n\nset.seed(123)\nresult <- microbenchmark(\n  baseR = aggregate(diamonds[,c(\"price\", \"depth\",\"carat\")],\n            by = list(diamonds$color, diamonds$cut),\n            FUN = mean),\n  dplyr_1_0= diamonds |>\n    group_by(color, cut) |> \n    summarise(across(c(price, depth, carat), mean, na.rm=T)) |> \n    ungroup(),\n  dplyr_1_1 = diamonds |> \n    summarise(across(c(price, depth, carat), mean),\n              .by=c(color, cut)),\n  dtplyr = lazy_dt(diamonds) |> \n    summarise(across(c(price, depth, carat), mean),\n              .by=c(color, cut)),\n  dt = as.data.table(diamonds)[,.(mean_price = mean(price),\n                     mean_depth = mean(depth),\n                     mean_carat = mean(carat)), \n                  by=.(color, cut)],\n  dt_lapply_sd = as.data.table(diamonds)[,lapply(.SD, mean),.SDcols=c(\"price\",\"depth\",\"carat\"), by=.(color, cut)],\n  dt_DT = diamonds |> DT(,.(mean_price = mean(price),\n                     mean_depth = mean(depth),\n                     mean_carat = mean(carat)),\n           by=.(color, cut)),\n  dt_key = as.data.table(diamonds)[,.(mean_price = mean(price),\n                     mean_depth = mean(depth),\n                     mean_carat = mean(carat)),\n              keyby=.(color, cut)],\n  times=50\n  \n)\n\n\ntableboxplot\n\n\n\n\nUnit: milliseconds\n         expr        min         lq       mean     median         uq        max\n        baseR  10.552662  11.173197  12.164903  11.546440  12.962560  20.493071\n    dplyr_1_0 116.217042 118.796475 132.369553 123.273039 133.731381 246.319923\n    dplyr_1_1   2.897798   3.051425   3.747189   3.183015   3.350643  16.925046\n       dtplyr   3.506443   3.692419   4.080181   3.911830   4.210495   6.900628\n           dt   1.388014   1.813020   2.963913   2.015950   2.462214  19.836948\n dt_lapply_sd   1.448530   1.809535   2.476250   2.076978   2.352867   7.611691\n        dt_DT   1.215199   1.509661   2.024631   1.710049   1.893708   9.833071\n       dt_key   1.148820   1.475221   2.090865   1.636802   2.269391   5.287114\n neval\n    50\n    50\n    50\n    50\n    50\n    50\n    50\n    50\n\n\n\n\n\n\n\n\n\n\n\n\n예상된 결과였지만, 역시 data.table을 사용한 방법이 가장 빠른 것으로 나타났습니다.\n재미있는 점은 dplyr 1.0의 group_by()가 생각보다 느리다는 것이었습니다. 심지어 base R의 aggregate()보다 느린 것으로 나타났습니다.\n확실히 데이터가 커지면 커질수록 data.table이 속도 측면에서 큰 우위를 가져가는 것 같습니다."
  },
  {
    "objectID": "blog/posts/ggplot_ggsigfig/index.html",
    "href": "blog/posts/ggplot_ggsigfig/index.html",
    "title": "ggsignif: 통계적 유의성 시각화",
    "section": "",
    "text": "# install.packages('ggsignif')\nlibrary(ggplot2)\nlibrary(ggsignif)\nlibrary(data.table)\n\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_boxplot() + # using `ggsignif` to display comparison of interest\n  geom_signif(\n    comparisons = list(c(\"versicolor\", \"virginica\")),\n    map_signif_level = TRUE\n  )\n\n\n\n\n\ndt_male <- data.table(\n  outcome = rep(c('Death','Heart disease','Stroke',\n                  'Cancer','Hypertension','Diabetes'),2),\n  model = as.factor(rep(2:3, each=6)),\n  value = c(0.877,0.731,0.794,0.776,0.769,0.806,\n            0.887,0.734,0.795,0.783,0.770,0.809)\n)\n\n\ndt_male$outcome <- factor(dt_male$outcome, \n                          levels=c('Death','Heart disease','Stroke','Cancer','Hypertension','Diabetes'))\nsetkey(dt_male, outcome)\nsig_loc <- c()\nfor(i in seq(1,11,2)){\n  sig_loc <- c(sig_loc,mean(dt_male$value[i:i+1]))\n}\ndt_male |> \n  ggplot(aes(x=outcome, y=value, fill=model))+\n  geom_col(position = position_dodge(),\n           width = .7) +\n  theme_classic() +\n  scale_y_continuous(limits=c(0,1),\n                     expand=c(0,0)) +\n  scale_fill_manual(values=c('grey30','grey60'),\n                    labels=c('DEMO + PGHD',\n                             'DEMO + PGHD + HLD'),\n                    ) +\n  theme(legend.position = 'top',\n        axis.title.x = element_blank(),\n        axis.title.y = element_text(size=13),\n        axis.text = element_text(size=12, color='black'),\n        # axis.ticks.x = element_line(color=c(rep(NA,len-1), rep('black',len))),\n        legend.title = element_blank(),\n        legend.direction = 'vertical') +\n  guides(fill=guide_legend(byrow = T))+ # legend 간격 띄우기\n  labs(y='Area under the curve (AUC)') + \n  geom_text(aes(label=value, y=value+0.015),\n            position = position_dodge(width=0.8)) +\n  \n  # 유의미한 변수 표시\n  geom_signif(\n    y_position = sig_loc+0.04,\n    xmin= c(0.75, 1.75, 2.75, 3.75, 4.75, 5.75),\n    xmax = c(1.25,2.25, 3.25, 4.25, 5.25, 6.25),\n    annotations = c('*')\n  )\n\n\n\n\n레퍼런스\n\nhttps://const-ae.github.io/ggsignif/articles/intro.html"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html",
    "href": "blog/posts/2023-01-27-Rbase/index.html",
    "title": "R 기초 이해",
    "section": "",
    "text": "이번 포스트에서는 R을 사용하기 위해 반드시 알아야 하는 R 필수 개념에 대해 설명드리도록 하겠습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#object",
    "href": "blog/posts/2023-01-27-Rbase/index.html#object",
    "title": "R 기초 이해",
    "section": "Object",
    "text": "Object\nobject란 R에서 어떠한 데이터를 갖는 값을 의미합니다. Object는 객체라고도 불리며, 다양한 유형이나 구조의 데이터를 특정 이름으로 저장하는 것을 말합니다.\n\nobject의 생성\nR에서 object(객체)의 생성은 <- 또는 = 를 통해 이루어집니다.\n<-를 기준으로 왼쪽의 값이 object의 이름이고 오른쪽이 object가 갖게 되는 또는 object에 저장되는 데이터입니다.\n\nLHS <- RHS\nA <- B\n\n\n\nobject의 생성과 출력\nobject를 생성한다는 것은 특정 데이터를 특정한 이름으로 저장하는 것입니다.\nA <- B 를 사용해 데이터를 생성하면 A라는 object에는 B라는 데이터가 저장됩니다.\n\na <- 'Welcome to R!'\naa <- 'aa'\naa <- 1\n\n위에서는 ’Welcome to R!’이라고 하는 문자를 a라고 하는 object 로 생성한 것입니다. 반면 object의 출력은 생성한 object 에 저장되어 있는 데이터를 확인하는 것입니다.\nobject를 생성한다고 해서 그 값이 출력되지는 않습니다. object 갖고 있는 데이터를 확인하기 위해서는 object의 이름을 실행하거나 출력하는 함수를 사용해야 합니다.\n\na \n\n[1] \"Welcome to R!\"\n\n# 또는\nprint(a)\n\n[1] \"Welcome to R!\"\n\n\n만약 object를 생성함과 동시에 그 object를 출력하고 싶다면 아래와 같이 실행하면 됩니다.\n\na <- 'Welcome to R!'; print(a)\n\n[1] \"Welcome to R!\""
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#function-함수",
    "href": "blog/posts/2023-01-27-Rbase/index.html#function-함수",
    "title": "R 기초 이해",
    "section": "Function (함수)",
    "text": "Function (함수)\nR에서 함수란 특정한 기능을 수행하는 명령어입니다. 보통 “~을 한다” 처럼 우리말의 동사라고 이해하면 쉽습니다.\nR에서 함수는 일반적으로 ()가 붙습니다.\n\nprint <- '프린트' # 프린트라는 값을 갖는 object\nprint('dd') # 'dd'를 출력하는 함수\n\n[1] \"dd\"\n\n\n위의 예시에서 print는 ’프린트’라는 문자를 갖는 object인 반면, print()는 특정 object를 출력하는 함수입니다.\n물론 함수의 이름으로도 object를 만들 수 있습니다. 그러나 이럴 경우, 분석 과정에서 이름을 혼동하는 경우가 발생할 수 있기 때문에, 나중을 생각한다면 그렇게 권장하는 방법은 아닙니다. 따라서 함수명이 아닌 다른 이름으로 object를 만드시는 것을 추천합니다.\n\n\n\n\n\n\nTip\n\n\n\nR 뿐만 아니라 프로그래밍 언어에는 일반적으로 영어를 활용하여 object 이름을 붙여줍니다.\n다양한 방법들이 있겠으나, 추천드리는 방법은 다음과 같습니다. 우선 띄어쓰기가 필요없는 하나의 단어로 object 이름을 지어주는 것은 말그대로 영어단어를 사용해주면 됩니다.\n\nresult <- 'the result'\n\n하나의 단어를 뛰어 넘어 여러 개의 단어를 붙여 써야 하는 경우에는 아래의 방법 중 원하는 방식을 사용하시면 됩니다.\n\nmyResult\nmy_result\nmy.result"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#작업-경로-working-directory-wd",
    "href": "blog/posts/2023-01-27-Rbase/index.html#작업-경로-working-directory-wd",
    "title": "R 기초 이해",
    "section": "작업 경로 (working directory, wd)",
    "text": "작업 경로 (working directory, wd)\n작업경로란 현재 내가 데이터 분석을 진행하고 있는 위치를 의미합니다.\n작업경로와 관련된 함수는 크게 2가지가 있습니다.\n\ngetwd() 현재 내 작업경로를 확인합니다.\n\ngetwd()\n\n[1] \"/Users/jyh/Documents/blog/blog/posts/2023-01-27-Rbase\"\n\n\nsetwd() 작업경로를 특정한 곳으로 변경합니다.\n\nsetwd(\"작업경로로 설정할 위치\")\n\n\n작업경로를 설정하는 이유는 외부 데이터 파일(e.g., .csv) 을 불러올 때, 그 파일이 있는 경로가 필요하기 때문입니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#패키지-packages",
    "href": "blog/posts/2023-01-27-Rbase/index.html#패키지-packages",
    "title": "R 기초 이해",
    "section": "패키지 (packages)",
    "text": "패키지 (packages)\nR에서 패키지란 특정 함수들을 포함하고 있는 함수 세트 라고 할 수 있습니다.\n데이터 분석에 필요한 패키지들을 설치한 뒤 그 패키지를 불러 들여와야 사용할 수 있습니다.\n\n패키지 설치: install.packages()\n\ninstall.packages('패키지명')\n\n위와 같이 설치하고자 하는 패키지를 install.packages() 함수 안에 따옴표가 붙은 문자 형태로 넣어주어야 합니다.\n패키지 불러오기: library() 또는 require()\n\nlibrary(dplyr)\n# 또는\nrequire(dplyr)\n\n설치된 패키지를 불러오기 위해서는 library() 또는 require() 함수 안에 패키지 이름을 object 형태(따옴표가 붙지 않은 형태)로 넣어주어야 합니다.\n사실 패키지 이름을 입력할 때, 따옴표를 사용해 문자로 넣어주어도 가능합니다. 그러나 문자열로 넣게 된다면 자동완성기능을 활용할 수 없게 됩니다. 그렇기 때문에 패키지의 이름을 전부 입력해야만 하죠. 따라서 가급적이면 따옴표를 사용하지 않는 것을 추천합니다.\n\n\n\n\n\n\nNote\n\n\n\nlibrary()와 require()는 패키지를 불러온다는 점에서 동일한 역할을 하지만, 차이점이 존재합니다.\nlibrary() 는 설치되지 않은 패키지를 불러오면 에러메시지를 출력하지만, require()는 경고와 함께 T/F 값을 반환할 수 있습니다.\n\n# library\nlibrary(ggplot3)\n\nError in library(ggplot3) : there is no package called 'ggplot3'\n\nrequire(ggplot3)\n\n필요한 패키지를 로딩중입니다: ggplot3\nWarning message:\nIn library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :\n  ‘ggplot3’이라고 불리는 패키지가 없습니다\n\n\n\n패키지를 성공적으로 불러왔다면, 패키지에 있는 함수들을 활용할 수 있게 됩니다.\n물론 패키지를 불러오지 않고 (library()나 require()를 이용하지 않고) 패키지 내의 함수를 사용하는 방법이 있긴 합니다.\n\ndplyr::select(column이름) #패키지명::함수\n\n위의 코드처럼 패키지명::함수명 의 방식을 이용한다면 패키지 내에 있는 함수들을 사용할 수 있습니다.\n그러나 매번 패키지명::을 이용해 함수를 이용하는 것보다 library() 함수를 한 번 실행한 뒤, 패키지의 함수들을 이용하는 것이 훨씬 효율적이겠죠?"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#numeric",
    "href": "blog/posts/2023-01-27-Rbase/index.html#numeric",
    "title": "R 기초 이해",
    "section": "numeric",
    "text": "numeric\nnumeric은 이름에서 알 수 있듯이 숫자 데이터입니다. numeric은 정수와 실수를 모두 포함합니다. R에서 정수는 integer, 실수는 double로 표현됩니다.\n\ninteger\n우리가 생각하는 정수는 흔히 소숫점이 없는 숫자입니다. 그러나 R에서는 단순히 숫자에 소수점이 없다고 해서 integer로 인식하지 않습니다.\n\nis.integer(1) # FALSE\n\n[1] FALSE\n\nclass(1) # numeric\n\n[1] \"numeric\"\n\nis.integer(1L)\n\n[1] TRUE\n\nclass(1L)\n\n[1] \"integer\"\n\n\n위의 코드로 봤을 때, 단순한 1은 integer가 아니라 numeric입니다.\nR에서 integer를 사용하기 위해선 숫자 뒤에 L을 붙여주어야 합니다.\n\n\ndouble\n우리가 알고 있는 실수는 소수점이 있는 숫자입니다. 그러나 R에서는 앞서 살펴 보았듯, L이 붙어 integer로 지정된 숫자가 아니라면 모두 실수라고 할 수 있습니다.\n\nis.numeric(1) #TRUE\n\n[1] TRUE\n\nis.double(1) #TRUE\n\n[1] TRUE\n\nclass(1.5) #numeric\n\n[1] \"numeric\"\n\n\n1이라는 숫자는 double이기도 하면서 동시에 numeric입니다. 또한 소수점이 들어가는 숫자의 유형을 확인해보면 double이 아니라 numeric이라는 것을 알 수 있습니다.\n즉, numeric이 숫자데이터에서 가장 상위의 개념이고 그 하위 개념으로 double 과 integer 가 있다고 이해하시면 쉽습니다.\n\n#|echo: false\n#|warning: false\n\nrequire(ggplot2)\nrequire(ggforce)\ntemp <- data.frame(x=c(100,300,200),\n                   y=c(250,250,250),\n                   r=c(80,80,200),\n                   y_label=c(250,250,450),\n                   label = c('integer','double','numeric'))\nggplot(temp) + \n  geom_circle(aes(x0=x,y0=y,r=r, fill=label),alpha=.2) +\n  coord_fixed()+\n  geom_label(data=temp,aes(x=x,y=y_label,label=label))+\n  theme_void()+\n  labs(title='    숫자유형 데이터의 관계')+\n  theme(legend.position='none')\n\n\n\n\n그러나 이 둘을 구분해서 사용하는 경우는 드뭅니다. 모든 숫자 데이터는 numeric으로 사용하셔도 상관없습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#character",
    "href": "blog/posts/2023-01-27-Rbase/index.html#character",
    "title": "R 기초 이해",
    "section": "character",
    "text": "character\ncharacter는 큰 따옴표 또는 작은 따옴표가 붙은 문자열 데이터를 의미합니다.\n\nchr <- 'character'\nchr\n\n[1] \"character\"\n\n\n앞서 object 부분에서 말씀드렸듯이, ‘character’ 라는 문자열 데이터이고, chr는 ‘character’ 값을 갖는 object입니다. chr 와 'chr'을 헷갈리지 마시기 바랍니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#factor",
    "href": "blog/posts/2023-01-27-Rbase/index.html#factor",
    "title": "R 기초 이해",
    "section": "factor",
    "text": "factor\nfactor는 다른 데이터의 유형과 달리 순서 (level)를 갖는 데이터 유형입니다.\n\nvec_num <- 1:10\nvec_fac <- factor(1:10)\nclass(vec_fac) # factor\n\n[1] \"factor\"\n\nprint(vec_fac) \n\n [1] 1  2  3  4  5  6  7  8  9  10\nLevels: 1 2 3 4 5 6 7 8 9 10\n\n\nvec_num과 vec_fac은 모두 1부터 10까지의 데이터를 갖고 있습니다. 그러나 vec_num과 달리 vec_fac을 출력했을 때, Levels도 함께 출력되는 것을 확인할 수 있습니다.\n만약 임의로 factor 변수의 level을 변경하고 싶다면 아래와 같이 factor 함수 안에 levels 부분을 수정해주면 됩니다.\n\nvec_fac2 <- factor(1:10,  levels=c(1,3,5,7,9,10,8,6,4,2))\nprint(vec_fac2)\n\n [1] 1  2  3  4  5  6  7  8  9  10\nLevels: 1 3 5 7 9 10 8 6 4 2\n\n\nvec_fac과 vec_fac2의 levels가 다른 것을 확인할 수 있습니다.\n또한 factor를 ordinal 하게 만들어줄 수 있습니다. ordered=T 를 이용해 nominal 한 factor를 ordinal하게 변경할 수 있습니다.\n\nvec_fac3 <- factor(1:10,\n                   ordered = T,\n                   levels=c(1,3,5,7,9,10,8,6,4,2))\nprint(vec_fac3)\n\n [1] 1  2  3  4  5  6  7  8  9  10\nLevels: 1 < 3 < 5 < 7 < 9 < 10 < 8 < 6 < 4 < 2\n\n\n출력 시 Levels에 < 가 생성되는 것을 알 수 있습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#logical",
    "href": "blog/posts/2023-01-27-Rbase/index.html#logical",
    "title": "R 기초 이해",
    "section": "logical",
    "text": "logical\nlogical은 단어에서 알 수 있듯 논리형 데이터 입니다. 쉽게 말하면 참(TRUE)인지 거짓(FALSE)인지를 나타내는 데이터 유형입니다.\n\nclass(TRUE) #  logical\n\n[1] \"logical\"\n\nTRUE\n\n[1] TRUE\n\nFALSE\n\n[1] FALSE\n\nT\n\n[1] TRUE\n\nF\n\n[1] FALSE\n\n\nR에서 TRUE 와 FALSE 는 각각 T 와 F로 줄여쓸 수 있습니다.\n\n\n\n\n\n\nNote\n\n\n\nR에서 사용되는 변수를 개념적으로 정리해보면 다음과 같습니다.\n연속형 변수(Continuous variable): numeric, integer, double\n범주형 변수(Categorical variable): character, factor"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#vector",
    "href": "blog/posts/2023-01-27-Rbase/index.html#vector",
    "title": "R 기초 이해",
    "section": "Vector",
    "text": "Vector\nvector는 하나 이상의 값을 갖는 1차원 구조의 데이터입니다. 가장 단순하지만, 또 데이터 분석에 많이 활용되는 데이터 구조 중 하나입니다.\n보통 vector를 생성할 때는 c() 를 이용해 생성합니다. c()는 ’concatenate’의 약자입니다.\n\nvec1 <- c(1,2,3); vec1\n\n[1] 1 2 3\n\nvec2 <- c('a','b','c'); vec2\n\n[1] \"a\" \"b\" \"c\"\n\n\nvec1은 numeric 데이터를 갖는 vector이고 vec2는 character 데이터를 갖는 vector입니다.\n여러 유형의 데이터를 갖는 vector인 경우, 하나라도 문자(character) 또는 factor가 있는 경우, 전부 그 유형의 데이터를 갖는 vector로 저장됩니다.\n\nvec3 <- c(1,2,3,'a',5,6)\nvec3\n\n[1] \"1\" \"2\" \"3\" \"a\" \"5\" \"6\"\n\n\nvec3를 선언할 때는 1,2,3 과 같은 숫자가 들어갔지만, vec3를 출력해보면 따옴표가 붙은 character인 것을 알 수 있습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#matrix",
    "href": "blog/posts/2023-01-27-Rbase/index.html#matrix",
    "title": "R 기초 이해",
    "section": "Matrix",
    "text": "Matrix\n2차원 구조의 데이터입니다. 행렬이라고 이해하시면 쉽습니다.\nR에서 matrix를 생성하는 함수는 matrix()입니다.\n\nmat <- matrix(1:12)\nmat2 <- matrix(1:12, nrow=3) # 행의 개수: 3\nmat3 <- matrix(1:12, ncol=3) # 열의 개수: 3\nmat4 <- matrix(1:12, ncol=3, byrow=T)\n\nmatrix() 함수에서는 행 또는 열의 개수를 지정해줄 수 있습니다.\n\nnrow() : 행의 개수 지정\nncol(): 열의 개수 지정\n\n마지막의 byrow인자는 matrix의 입력 순서를 행의 방향으로 설정해주는 것입니다. 초기에는 matrix가 열 방향(위에서 아래로 (↓))이었다면, byrow를 통해 matrix의 값들을 행 방향 (좌에서 우로 (→))으로 입력할 수 있습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#data.frame",
    "href": "blog/posts/2023-01-27-Rbase/index.html#data.frame",
    "title": "R 기초 이해",
    "section": "Data.frame",
    "text": "Data.frame\n2차원 구조의 데이터로서, R에서 데이터분석을 위해 가장 많이 활용되는 데이터 구조입니다. 데이터프레임은 matrix와 동일한 2차원 형태지만 몇 가지 다른 점이 있습니다.\n\nmatrix와 달리, $ 기호를 이용해 column 이름으로 호출 가능합니다.\n각 열(column)별로 다른 데이터 유형을 가질 수 있습니다.\n\ndata.frame은 앞으로 가장 많이 사용할 데이터 구조이기 때문에, 아래에서 더 다뤄보도록 하겠습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#list",
    "href": "blog/posts/2023-01-27-Rbase/index.html#list",
    "title": "R 기초 이해",
    "section": "List",
    "text": "List\nlist는 다른 데이터 구조들과는 다릅니다.\n\n데이터프레임의 column처럼 데이터마다 이름을 가질 수 있습니다.\n각 이름마다 다른 유형, 다른 구조의 데이터를 가질 수 있습니다.\n각 이름마다 길이가 다른 데이터를 가질 수 있습니다.\n\nl <- list()\nl$a <- 1:5\nl$b <- LETTERS # 알파벳 대문자\nl$c <- matrix(1:20,nrow=4, byrow=T)\nl$d <- data.frame(a=1:5, b=letters[1:5])\nl\n\n$a\n[1] 1 2 3 4 5\n\n$b\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n$c\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\n$d\n  a b\n1 1 a\n2 2 b\n3 3 c\n4 4 d\n5 5 e\n\n\n위의 코드를 보면, l이라고 하는 list에는 a라는 이름을 갖는 데이터와 b라는 이름을 갖는 데이터가 존재합니다.\n또한 a는 1부터 5까지의 numeric 유형의 vector를 갖는 반면, b는 알파벳 대문자의 character 유형의 vector를, c는 4행 5열의 matrix를, 마지막으로 d는 data.frame을 갖습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#산술-연산자",
    "href": "blog/posts/2023-01-27-Rbase/index.html#산술-연산자",
    "title": "R 기초 이해",
    "section": "산술 연산자",
    "text": "산술 연산자\n산술연산자는 숫자를 계산하는 데 사용되는 연산자를 말합니다. 우리가 잘 아는 덧셈(+), 뺄셈(-), 곱셈(*), 나눗셈(/) 등의 연산자가 있습니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#논리-연산자",
    "href": "blog/posts/2023-01-27-Rbase/index.html#논리-연산자",
    "title": "R 기초 이해",
    "section": "논리 연산자",
    "text": "논리 연산자\n논리 연산자는 참(TRUE)과 거짓(FALSE)를 반환하는 연산자들을 의미합니다. 보통 LHS와 RHS의 값을 비교할 때 사용하는 연산자입니다.\n연산 결과가 맞는 경우는 TRUE를, 그렇지 않은 경우는 FALSE를 반환합니다.\n논리 연산자는 주로 데이터프레임과 함께 특정 조건을 만족하는 행을 찾아낼 때 사용합니다.\n\nA == B: 같음\n\n10 == 10\n\n[1] TRUE\n\n\nA < B, A <= B : 작음, 작거나 같음\n\n5 < 10\n\n[1] TRUE\n\n5<=10\n\n[1] TRUE\n\n\nA > B, A >= B :큼, 크거나 같음\n\n44>=10\n\n[1] TRUE\n\n10>35\n\n[1] FALSE\n\n\nA != B : 같지 않음\n\n10 != 5\n\n[1] TRUE\n\n\nA & B : A와 B가 모두 참일 때 TRUE를 반환합니다.\n\n10 >5 & 25 > 20\n\n[1] TRUE\n\n\nA | B : A 또는 B 하나만 참이면 TRUE를 반환합니다.\n\n10 <5 | 50 > 49\n\n[1] TRUE"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#파이프-연산자",
    "href": "blog/posts/2023-01-27-Rbase/index.html#파이프-연산자",
    "title": "R 기초 이해",
    "section": "파이프 연산자",
    "text": "파이프 연산자\n파이프 연산자란 R의 특수한 연산자로, % 가 붙은 연산자들을 의미합니다. 대표적인 파이프 연산자는 %>% 가 있습니다. 이 연산자를 사용하기 위해선 magrittr 패키지가 필요합니다. 단축키는 ctrl + shift + m 로 사용할 수 있습니다.\n\n\n\n\n\n\nTip\n\n\n\n기존에 존재하던 %>% 는 magrittr 패키지를 불러들여와야 사용이 가능했습니다. 하지만 R 4.1 이후로는 기본적으로 |> 라는 파이프 연산자를 제공하고 있습니다. 실질적으로 기능은 거의 같기 때문에 원하는 파이프 연산자를 사용하시면 됩니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#요약-함수",
    "href": "blog/posts/2023-01-27-Rbase/index.html#요약-함수",
    "title": "R 기초 이해",
    "section": "요약 함수",
    "text": "요약 함수\nR에서는 다양한 숫자 데이터를 특정한 값으로 요약할 수 있습니다. 이 글에서는 다음과 같은 함수들을 요약함수로 지칭하겠습니다. 대표적인 요약 함수로는 mean()평균, sd()표준편차, min()최소값, max()최대값, median()중앙값, sum()합계, quantile() 4분위수 등이 있습니다.\n\nmean(df$Age) # 평균\n\n[1] 36.05\n\nsd(df$Age) # 표준편차\n\n[1] 20.182\n\nmin(df$Age) # 최소\n\n[1] 4\n\nmax(df$Age) # 최대\n\n[1] 66\n\nmedian(df$Age)  # 중앙값\n\n[1] 39.5\n\nquantile(df$Age) # 4분위수\n\n  0%  25%  50%  75% 100% \n 4.0 14.5 39.5 51.0 66.0 \n\n\n만약 numeric 데이터에 결측치(NA)가 있다면 요약함수를 실행했을 때, NA가 출력됩니다.\n\nmean(df$BMI) \n\n[1] 25.682\n\n\n따라서 데이터 내에 NA가 있는 경우는 요약함수 안에 na.rm=T를 넣어줘야 합니다. NA를 제외한 나머지 데이터로 요약한 값이 출력됩니다.\n\nmean(df$BMI, na.rm=T) \n\n[1] 25.682\n\n\n한편, 숫자형 데이터 이외에, character, factor 등 범주형 데이터의 경우 각 범주의 응답 분포를 확인할 수 있습니다. table() 함수를 통해 이를 파악할 수 있습니다.\n\ntable(df$Gender)\n\n\nfemale   male \n     7     13 \n\n\n응답 분포를 확률로 표현할 수 있는 함수는 prop.table() 입니다. 이 때, prop.table() 에 넣어줄 인자는 table()에 범주를 확인하고자 하는 변수를 넣은 값입니다.\n\nprop.table(table(df$Gender))\n\n\nfemale   male \n  0.35   0.65 \n\nprop.table(table(df$Gender))*100 # percent 위해 100 곱하기\n\n\nfemale   male \n    35     65"
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#data.frame-관련-함수",
    "href": "blog/posts/2023-01-27-Rbase/index.html#data.frame-관련-함수",
    "title": "R 기초 이해",
    "section": "data.frame 관련 함수",
    "text": "data.frame 관련 함수\n다음으로 데이터프레임과 관련된 함수를 알아보겠습니다. 사실 이 함수들은 data.frame뿐만 아니라 vector나 다른 구조의 데이터들과도 사용되는 함수입니다.\n다만 앞으로 우리가 다룰 데이터 구조의 대부분이 data.frame이고, data.frame을 활용해 데이터를 분석할 때 그 활용도가 높기 때문에 편의상data.frame 관련 함수라고 지칭하겠습니다.\n\n데이터 불러오기/저장하기\nread.csv() 를 통해 외부 csv 파일을 작업환경으로 불러올 수 있습니다.\n\ndf2 <- read.csv('데이터명.csv')\n\n반면 작업환경에서 분석하던 data.frame을 외부 csv 파일로 저장할 수 있습니다. write.csv() 함수를 통해 가능합니다.\n\nwrite.csv(df2,'~/temp.csv') #작업저장경로+파일이름\n\n\n\n데이터 훑어보기\n일반적으로 데이터를 분석하기 전, 데이터의 특징을 파악하는 것이 필요합니다. 데이터의 특징을 파악하는 함수들은 head(),tail(), str(), summary()등이 있습니다.\n\nhead(df) # 데이터의 첫 6행을 확인\n\n\n\n  \n\n\n\n\ntail(df) # 데이터의 마지막 6행을 확인\n\n\n\n  \n\n\n\n\nstr(df) # 데이터의 구조 확인\n\n'data.frame':   20 obs. of  76 variables:\n $ ID              : int  51624 51624 51624 51625 51630 51638 51646 51647 51647 51647 ...\n $ SurveyYr        : Factor w/ 2 levels \"2009_10\",\"2011_12\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Gender          : Factor w/ 2 levels \"female\",\"male\": 2 2 2 2 1 2 2 1 1 1 ...\n $ Age             : int  34 34 34 4 49 9 8 45 45 45 ...\n $ AgeDecade       : Factor w/ 8 levels \" 0-9\",\" 10-19\",..: 4 4 4 1 5 1 1 5 5 5 ...\n $ AgeMonths       : int  409 409 409 49 596 115 101 541 541 541 ...\n $ Race1           : Factor w/ 5 levels \"Black\",\"Hispanic\",..: 4 4 4 5 4 4 4 4 4 4 ...\n $ Race3           : Factor w/ 6 levels \"Asian\",\"Black\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ Education       : Factor w/ 5 levels \"8th Grade\",\"9 - 11th Grade\",..: 3 3 3 NA 4 NA NA 5 5 5 ...\n $ MaritalStatus   : Factor w/ 6 levels \"Divorced\",\"LivePartner\",..: 3 3 3 NA 2 NA NA 3 3 3 ...\n $ HHIncome        : Factor w/ 12 levels \" 0-4999\",\" 5000-9999\",..: 6 6 6 5 7 11 9 11 11 11 ...\n $ HHIncomeMid     : int  30000 30000 30000 22500 40000 87500 60000 87500 87500 87500 ...\n $ Poverty         : num  1.36 1.36 1.36 1.07 1.91 1.84 2.33 5 5 5 ...\n $ HomeRooms       : int  6 6 6 9 5 6 7 6 6 6 ...\n $ HomeOwn         : Factor w/ 3 levels \"Own\",\"Rent\",\"Other\": 1 1 1 1 2 2 1 1 1 1 ...\n $ Work            : Factor w/ 3 levels \"Looking\",\"NotWorking\",..: 2 2 2 NA 2 NA NA 3 3 3 ...\n $ Weight          : num  87.4 87.4 87.4 17 86.7 29.8 35.2 75.7 75.7 75.7 ...\n $ Length          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ HeadCirc        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Height          : num  165 165 165 105 168 ...\n $ BMI             : num  32.2 32.2 32.2 15.3 30.6 ...\n $ BMICatUnder20yrs: Factor w/ 4 levels \"UnderWeight\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ BMI_WHO         : Factor w/ 4 levels \"12.0_18.5\",\"18.5_to_24.9\",..: 4 4 4 1 4 1 2 3 3 3 ...\n $ Pulse           : int  70 70 70 NA 86 82 72 62 62 62 ...\n $ BPSysAve        : int  113 113 113 NA 112 86 107 118 118 118 ...\n $ BPDiaAve        : int  85 85 85 NA 75 47 37 64 64 64 ...\n $ BPSys1          : int  114 114 114 NA 118 84 114 106 106 106 ...\n $ BPDia1          : int  88 88 88 NA 82 50 46 62 62 62 ...\n $ BPSys2          : int  114 114 114 NA 108 84 108 118 118 118 ...\n $ BPDia2          : int  88 88 88 NA 74 50 36 68 68 68 ...\n $ BPSys3          : int  112 112 112 NA 116 88 106 118 118 118 ...\n $ BPDia3          : int  82 82 82 NA 76 44 38 60 60 60 ...\n $ Testosterone    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ DirectChol      : num  1.29 1.29 1.29 NA 1.16 1.34 1.55 2.12 2.12 2.12 ...\n $ TotChol         : num  3.49 3.49 3.49 NA 6.7 4.86 4.09 5.82 5.82 5.82 ...\n $ UrineVol1       : int  352 352 352 NA 77 123 238 106 106 106 ...\n $ UrineFlow1      : num  NA NA NA NA 0.094 ...\n $ UrineVol2       : int  NA NA NA NA NA NA NA NA NA NA ...\n $ UrineFlow2      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Diabetes        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ DiabetesAge     : int  NA NA NA NA NA NA NA NA NA NA ...\n $ HealthGen       : Factor w/ 5 levels \"Excellent\",\"Vgood\",..: 3 3 3 NA 3 NA NA 2 2 2 ...\n $ DaysPhysHlthBad : int  0 0 0 NA 0 NA NA 0 0 0 ...\n $ DaysMentHlthBad : int  15 15 15 NA 10 NA NA 3 3 3 ...\n $ LittleInterest  : Factor w/ 3 levels \"None\",\"Several\",..: 3 3 3 NA 2 NA NA 1 1 1 ...\n $ Depressed       : Factor w/ 3 levels \"None\",\"Several\",..: 2 2 2 NA 2 NA NA 1 1 1 ...\n $ nPregnancies    : int  NA NA NA NA 2 NA NA 1 1 1 ...\n $ nBabies         : int  NA NA NA NA 2 NA NA NA NA NA ...\n $ Age1stBaby      : int  NA NA NA NA 27 NA NA NA NA NA ...\n $ SleepHrsNight   : int  4 4 4 NA 8 NA NA 8 8 8 ...\n $ SleepTrouble    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 1 1 1 ...\n $ PhysActive      : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 NA 1 NA NA 2 2 2 ...\n $ PhysActiveDays  : int  NA NA NA NA NA NA NA 5 5 5 ...\n $ TVHrsDay        : Factor w/ 7 levels \"0_hrs\",\"0_to_1_hr\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ CompHrsDay      : Factor w/ 7 levels \"0_hrs\",\"0_to_1_hr\",..: NA NA NA NA NA NA NA NA NA NA ...\n $ TVHrsDayChild   : int  NA NA NA 4 NA 5 1 NA NA NA ...\n $ CompHrsDayChild : int  NA NA NA 1 NA 0 6 NA NA NA ...\n $ Alcohol12PlusYr : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 2 2 2 ...\n $ AlcoholDay      : int  NA NA NA NA 2 NA NA 3 3 3 ...\n $ AlcoholYear     : int  0 0 0 NA 20 NA NA 52 52 52 ...\n $ SmokeNow        : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 NA 2 NA NA NA NA NA ...\n $ Smoke100        : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 1 1 1 ...\n $ Smoke100n       : Factor w/ 2 levels \"Non-Smoker\",\"Smoker\": 2 2 2 NA 2 NA NA 1 1 1 ...\n $ SmokeAge        : int  18 18 18 NA 38 NA NA NA NA NA ...\n $ Marijuana       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 2 2 2 ...\n $ AgeFirstMarij   : int  17 17 17 NA 18 NA NA 13 13 13 ...\n $ RegularMarij    : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 NA 1 NA NA 1 1 1 ...\n $ AgeRegMarij     : int  NA NA NA NA NA NA NA NA NA NA ...\n $ HardDrugs       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 1 1 1 ...\n $ SexEver         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 NA 2 NA NA 2 2 2 ...\n $ SexAge          : int  16 16 16 NA 12 NA NA 13 13 13 ...\n $ SexNumPartnLife : int  8 8 8 NA 10 NA NA 20 20 20 ...\n $ SexNumPartYear  : int  1 1 1 NA 1 NA NA 0 0 0 ...\n $ SameSex         : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 NA 2 NA NA 2 2 2 ...\n $ SexOrientation  : Factor w/ 3 levels \"Bisexual\",\"Heterosexual\",..: 2 2 2 NA 2 NA NA 1 1 1 ...\n $ PregnantNow     : Factor w/ 3 levels \"Yes\",\"No\",\"Unknown\": NA NA NA NA NA NA NA NA NA NA ...\n\n\n\nsummary(df) # 데이터의 column 요약통계량\n\n       ID           SurveyYr     Gender        Age          AgeDecade\n Min.   :51624   2009_10:20   female: 7   Min.   : 4.00    0-9   :4  \n 1st Qu.:51636   2011_12: 0   male  :13   1st Qu.:14.50    30-39 :4  \n Median :51650                            Median :39.50    40-49 :4  \n Mean   :51651                            Mean   :36.05    50-59 :4  \n 3rd Qu.:51666                            3rd Qu.:51.00    10-19 :2  \n Max.   :51679                            Max.   :66.00    60-69 :2  \n                                                          (Other):0  \n   AgeMonths          Race1         Race3             Education\n Min.   : 49.0   Black   : 1   Asian   : 0   8th Grade     :0  \n 1st Qu.:176.2   Hispanic: 0   Black   : 0   9 - 11th Grade:1  \n Median :475.0   Mexican : 1   Hispanic: 0   High School   :6  \n Mean   :436.2   White   :16   Mexican : 0   Some College  :3  \n 3rd Qu.:615.8   Other   : 2   White   : 0   College Grad  :4  \n Max.   :795.0                 Other   : 0   NA's          :6  \n                               NA's    :20                     \n      MaritalStatus        HHIncome  HHIncomeMid        Poverty     \n Divorced    : 1    25000-34999:5   Min.   : 17500   Min.   :1.030  \n LivePartner : 1    75000-99999:5   1st Qu.: 30000   1st Qu.:1.360  \n Married     :11    15000-19999:2   Median : 40000   Median :1.910  \n NeverMarried: 1    20000-24999:1   Mean   : 53824   Mean   :2.424  \n Separated   : 0    35000-44999:1   3rd Qu.: 87500   3rd Qu.:2.330  \n Widowed     : 0    (Other)    :3   Max.   :100000   Max.   :5.000  \n NA's        : 6    NA's       :3   NA's   :3        NA's   :3      \n   HomeRooms      HomeOwn           Work       Weight          Length   \n Min.   : 3.00   Own  :13   Looking   :2   Min.   :17.00   Min.   : NA  \n 1st Qu.: 5.75   Rent : 7   NotWorking:6   1st Qu.:56.40   1st Qu.: NA  \n Median : 6.00   Other: 0   Working   :7   Median :75.20   Median : NA  \n Mean   : 6.70              NA's      :5   Mean   :67.70   Mean   :NaN  \n 3rd Qu.: 7.50                             3rd Qu.:84.75   3rd Qu.: NA  \n Max.   :11.00                             Max.   :93.80   Max.   : NA  \n                                                           NA's   :20   \n    HeadCirc       Height           BMI           BMICatUnder20yrs\n Min.   : NA   Min.   :105.4   Min.   :15.30   UnderWeight: 0     \n 1st Qu.: NA   1st Qu.:146.5   1st Qu.:23.68   NormWeight : 0     \n Median : NA   Median :166.7   Median :26.41   OverWeight : 0     \n Mean   :NaN   Mean   :159.1   Mean   :25.68   Obese      : 0     \n 3rd Qu.: NA   3rd Qu.:169.6   3rd Qu.:27.68   NA's       :20     \n Max.   : NA   Max.   :181.9   Max.   :32.22                      \n NA's   :20                                                       \n         BMI_WHO      Pulse          BPSysAve        BPDiaAve     \n 12.0_18.5   :2   Min.   :60.00   Min.   : 86.0   Min.   : 37.00  \n 18.5_to_24.9:5   1st Qu.:66.00   1st Qu.:109.0   1st Qu.: 64.00  \n 25.0_to_29.9:9   Median :74.00   Median :113.0   Median : 72.00  \n 30.0_plus   :4   Mean   :75.26   Mean   :116.8   Mean   : 71.37  \n                  3rd Qu.:83.00   3rd Qu.:126.5   3rd Qu.: 84.00  \n                  Max.   :96.00   Max.   :152.0   Max.   :100.00  \n                  NA's   :1       NA's   :1       NA's   :1       \n     BPSys1          BPDia1          BPSys2          BPDia2     \n Min.   : 84.0   Min.   :46.00   Min.   : 84.0   Min.   :36.00  \n 1st Qu.:106.0   1st Qu.:62.00   1st Qu.:108.0   1st Qu.:67.00  \n Median :114.0   Median :70.00   Median :114.0   Median :72.00  \n Mean   :115.7   Mean   :71.89   Mean   :116.4   Mean   :72.42  \n 3rd Qu.:125.5   3rd Qu.:85.00   3rd Qu.:127.0   3rd Qu.:85.00  \n Max.   :154.0   Max.   :98.00   Max.   :150.0   Max.   :98.00  \n NA's   :2       NA's   :2       NA's   :1       NA's   :1      \n     BPSys3          BPDia3        Testosterone   DirectChol       TotChol     \n Min.   : 88.0   Min.   : 38.00   Min.   : NA   Min.   :0.670   Min.   :3.000  \n 1st Qu.:107.5   1st Qu.: 60.50   1st Qu.: NA   1st Qu.:1.140   1st Qu.:4.128  \n Median :115.0   Median : 73.00   Median : NA   Median :1.255   Median :4.925  \n Mean   :116.6   Mean   : 70.11   Mean   :NaN   Mean   :1.327   Mean   :4.948  \n 3rd Qu.:119.5   3rd Qu.: 82.00   3rd Qu.: NA   3rd Qu.:1.340   3rd Qu.:5.820  \n Max.   :154.0   Max.   :102.00   Max.   : NA   Max.   :2.120   Max.   :6.700  \n NA's   :2       NA's   :2        NA's   :20    NA's   :2       NA's   :2      \n   UrineVol1       UrineFlow1       UrineVol2     UrineFlow2  Diabetes\n Min.   :  7.0   Min.   :0.0460   Min.   : NA   Min.   : NA   No :20  \n 1st Qu.:106.0   1st Qu.:0.3465   1st Qu.: NA   1st Qu.: NA   Yes: 0  \n Median :155.0   Median :0.9390   Median : NA   Median : NA           \n Mean   :173.5   Mean   :0.8229   Mean   :NaN   Mean   :NaN           \n 3rd Qu.:238.0   3rd Qu.:1.1160   3rd Qu.: NA   3rd Qu.: NA           \n Max.   :352.0   Max.   :1.7420   Max.   : NA   Max.   : NA           \n NA's   :1       NA's   :6        NA's   :20    NA's   :20            \n  DiabetesAge      HealthGen DaysPhysHlthBad  DaysMentHlthBad LittleInterest\n Min.   : NA   Excellent:0   Min.   : 0.000   Min.   : 0.00   None   :8     \n 1st Qu.: NA   Vgood    :6   1st Qu.: 0.000   1st Qu.: 0.00   Several:2     \n Median : NA   Good     :5   Median : 0.000   Median : 3.00   Most   :3     \n Mean   :NaN   Fair     :3   Mean   : 1.714   Mean   : 6.50   NA's   :7     \n 3rd Qu.: NA   Poor     :0   3rd Qu.: 2.250   3rd Qu.:13.75                 \n Max.   : NA   NA's     :6   Max.   :10.000   Max.   :20.00                 \n NA's   :20                  NA's   :6        NA's   :6                     \n   Depressed  nPregnancies     nBabies     Age1stBaby SleepHrsNight\n None   :9   Min.   :1.00   Min.   :2    Min.   :27   Min.   :4.0  \n Several:4   1st Qu.:1.00   1st Qu.:2    1st Qu.:27   1st Qu.:4.5  \n Most   :0   Median :1.00   Median :2    Median :27   Median :6.0  \n NA's   :7   Mean   :1.25   Mean   :2    Mean   :27   Mean   :6.0  \n             3rd Qu.:1.25   3rd Qu.:2    3rd Qu.:27   3rd Qu.:7.5  \n             Max.   :2.00   Max.   :2    Max.   :27   Max.   :8.0  \n             NA's   :16     NA's   :19   NA's   :19   NA's   :5    \n SleepTrouble PhysActive PhysActiveDays       TVHrsDay      CompHrsDay\n No  :9       No  :6     Min.   :1.000   0_hrs    : 0   0_hrs    : 0  \n Yes :6       Yes :9     1st Qu.:3.000   0_to_1_hr: 0   0_to_1_hr: 0  \n NA's:5       NA's:5     Median :5.000   1_hr     : 0   1_hr     : 0  \n                         Mean   :4.444   2_hr     : 0   2_hr     : 0  \n                         3rd Qu.:5.000   3_hr     : 0   3_hr     : 0  \n                         Max.   :7.000   (Other)  : 0   (Other)  : 0  \n                         NA's   :11      NA's     :20   NA's     :20  \n TVHrsDayChild CompHrsDayChild Alcohol12PlusYr   AlcoholDay     AlcoholYear \n Min.   :0.0   Min.   :0.0     No  : 1         Min.   :1.000   Min.   :  0  \n 1st Qu.:1.0   1st Qu.:1.0     Yes :12         1st Qu.:2.000   1st Qu.:  0  \n Median :4.0   Median :1.0     NA's: 7         Median :3.000   Median : 52  \n Mean   :2.8   Mean   :2.2                     Mean   :3.222   Mean   : 68  \n 3rd Qu.:4.0   3rd Qu.:3.0                     3rd Qu.:3.000   3rd Qu.:100  \n Max.   :5.0   Max.   :6.0                     Max.   :6.000   Max.   :364  \n NA's   :15    NA's   :15                      NA's   :11      NA's   :7    \n SmokeNow  Smoke100      Smoke100n    SmokeAge     Marijuana AgeFirstMarij  \n No  : 6   No  :6   Non-Smoker:6   Min.   :13.00   No  :2    Min.   :13.00  \n Yes : 2   Yes :8   Smoker    :8   1st Qu.:16.50   Yes :9    1st Qu.:13.00  \n NA's:12   NA's:6   NA's      :6   Median :18.00   NA's:9    Median :17.00  \n                                   Mean   :19.71             Mean   :15.78  \n                                   3rd Qu.:18.00             3rd Qu.:17.00  \n                                   Max.   :38.00             Max.   :19.00  \n                                   NA's   :13                NA's   :11     \n RegularMarij  AgeRegMarij    HardDrugs SexEver       SexAge     \n No  :9       Min.   :15.00   No  :7    No  : 0   Min.   :12.00  \n Yes :2       1st Qu.:16.25   Yes :6    Yes :13   1st Qu.:13.00  \n NA's:9       Median :17.50   NA's:7    NA's: 7   Median :16.00  \n              Mean   :17.50                       Mean   :16.42  \n              3rd Qu.:18.75                       3rd Qu.:17.75  \n              Max.   :20.00                       Max.   :27.00  \n              NA's   :18                          NA's   :8      \n SexNumPartnLife  SexNumPartYear   SameSex       SexOrientation  PregnantNow\n Min.   :  1.00   Min.   :0.0000   No  :9   Bisexual    :3      Yes    : 0  \n 1st Qu.:  8.00   1st Qu.:0.5000   Yes :4   Heterosexual:8      No     : 0  \n Median :  9.00   Median :1.0000   NA's:7   Homosexual  :0      Unknown: 0  \n Mean   : 17.46   Mean   :0.7273            NA's        :9      NA's   :20  \n 3rd Qu.: 20.00   3rd Qu.:1.0000                                            \n Max.   :100.00   Max.   :1.0000                                            \n NA's   :7        NA's   :9                                                 \n\n\n\n\n행과 열의 수 확인\n다음으로 데이터의 행과 열의 개수를 확인하는 함수들입니다.\n\nnrow(df) # 행의 개수\n\n[1] 20\n\nncol(df) # 열의 개수\n\n[1] 76\n\ndim(df) # 데이터의 행과 열의 수 확인\n\n[1] 20 76\n\n\n\n\n\n\n\n\nTip\n\n\n\n기존에 선언했던 object 와 동일한 이름으로 새로운 데이터를 저장하게 되면, 기존에 저장되어 있던 값은 사라지게 됩니다.\n\ntemp <- data.frame(id=c(1,2,3),name=c('a','b','c'));\ntemp <- data.frame(id=c(10,20,30),name=c('A','B','C'))\n\n따라서 object의 이름을 설정할 때는 헷갈리지 않게끔 다른 이름으로 지어주는 것이 필요합니다."
  },
  {
    "objectID": "blog/posts/2023-01-27-Rbase/index.html#논리-함수",
    "href": "blog/posts/2023-01-27-Rbase/index.html#논리-함수",
    "title": "R 기초 이해",
    "section": "논리 함수",
    "text": "논리 함수\n논리 함수는 논리 연산자처럼 TRUE 나 FALSE를 반환하는 함수입니다.\n\nis.na() : NA가 있는지 확인하는 함수입니다. NA가 있는 경우 TRUE를, 그렇지 않은 경우를 FALSE로 반환합니다.\nis.na()는 모든 행에 대해 결과를 반환하므로, 보통 table() 함수처럼 응답을 요약하여 해당 변수에 몇 개의 결측치가 있는지 확인하기 위해 사용하는 것이 일반적입니다.\n\ntable(is.na(df$BMI))\n\n\nFALSE \n   20 \n\n\nis.numeric(), is.factor(), is.character(): 특정 column의 데이터 유형을 확인하는 함수입니다. 해당 유형이 맞다면 TRUE, 아니면 FALSE를 반환합니다.\n\nis.numeric(15)\n\n[1] TRUE\n\nis.factor('dd')\n\n[1] FALSE\n\nis.character('dexmedetomidine')\n\n[1] TRUE\n\n\nifelse() 함수는 data.frame에서 가장 많이 쓰이는 함수 중 하나입니다. 그렇기 때문에 매우 중요하고, 반드시 알고 넘어가야 하는 함수입니다.\nifelse()의 구조는 다음과 같습니다.\n\n# ifelse(조건, \n#        '조건이 참일 때 사용할 값', \n#        '조건이 거짓일 때 사용할 값')\n\nifelse()는 주로 새로운 column을 생성할 때, 또는 기존의 column을 변경할 때 많이 사용됩니다.\n\ndf$Obesity <- ifelse(df$BMI>=25,1,0)\ntable(df$Obesity)\n\n\n 0  1 \n 7 13 \n\n\n위의 코드에서 df$BMI>=25가 조건절에 해당됩니다. 1은 BMI가 25보다 클 때 비만이라는 의미로 넣는 값이고, 0은 BMI가 25미만인 경우에 넣어줄 값입니다.\n또한 ifelse()와 is.na()를 함께 사용하여 해당 열에 결측치가 있는 경우, 채워넣기 위한 용도로 자주 활용됩니다.\n\ndf$BMI2 <- ifelse(is.na(df$BMI), \n                 mean(df$BMI, na.rm=T),\n                 df$BMI)\nsummary(df$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.30   23.68   26.41   25.68   27.68   32.22 \n\nsummary(df$BMI2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.30   23.68   26.41   25.68   27.68   32.22 \n\n\nBMI2는 BMI와 다르게 NA를 BMI의 평균으로 채워넣었습니다."
  },
  {
    "objectID": "blog/posts/subgroup_analysis/index.html",
    "href": "blog/posts/subgroup_analysis/index.html",
    "title": "Subgroup analysis (feat. forester)",
    "section": "",
    "text": "Subgroup analysis이란 치료군-대조군 연구에서 특정 약물의 사용에 따른 임상적 결과 (질병의 발생, 사망 등)를 비교하고자 할 때, 그 효과가 과연 하위집단(e.g. 성별, 수술 여부) 간에는 어떻게 나타나는지, 그리고 그 하위그룹 변수와 약물 사용 변수 간의 상호작용이 있는지 분석하는 방법입니다.\n이번 포스트에서는 R에서 하위집단 분석을 수행하는 방법에 대해 살펴보겠습니다."
  },
  {
    "objectID": "blog/posts/subgroup_analysis/index.html#준비",
    "href": "blog/posts/subgroup_analysis/index.html#준비",
    "title": "Subgroup analysis (feat. forester)",
    "section": "준비",
    "text": "준비\nSubgroup analysis를 수행하기 위해 필요한 패키지는 Publish 입니다. Subgroup anlysis 예제는 Publish 패키지에 내장된 traceR 데이터를 사용하도록 하겠습니다.\n\nrequire(Publish)\nrequire(survival)\nrequire(data.table)\ndata(traceR)\n\ntraceR 의 변수는 다음과 같습니다.\n\nweight: 체중(kg)\nheight: 키(m)\nabdominalCircumference: 복부둘레(cm)\nseCreatinine: 세럼 크레아티닌 (mmol/L)\nwallMotionIndex: 좌심실 기능 (0: 최악, 2: 정상)\nobservationTime: 관찰기간\nage: 나이(연)\nsex: 성별(0: 여성, 1: 남성)\nsmoking: 흡연 (0: 핀 적 없음, 1: 과거, 2: 현재)\ndead: 사망(0: 생존, 1: 사망)\ntreatment: 위약 또는 치료제(trandolapril: 고혈압 치료제)"
  },
  {
    "objectID": "blog/posts/subgroup_analysis/index.html#회귀식-만들기",
    "href": "blog/posts/subgroup_analysis/index.html#회귀식-만들기",
    "title": "Subgroup analysis (feat. forester)",
    "section": "회귀식 만들기",
    "text": "회귀식 만들기\n우선 콕스 회귀식을 하나 만들어주겠습니다.\n과연 치료제가 사망을 줄일 수 있는지 확인해보도록 하겠습니다. 관찰시간 변수는 observationTime, 결과변수는 dead, 그리고 중재(intervention) 변수는 treatment 입니다.\n\nfit <- coxph(Surv(observationTime, dead==1) ~ treatment, data=traceR)\n\npublish(fit)\n\n  Variable        Units HazardRatio       CI.95  p-value \n treatment      placebo         Ref                      \n           trandolapril        0.89 [0.79;0.99]   0.0313 \n\n\n결과를 봤을 때, 치료제를 사용한 집단의 Hazard ratio (HR)가 0.887 (95% CI 0.795-0.989) 로 나타났습니다. 즉, 치료제를 투여받은 집단이 그렇지 않은 집단에 비해 사망위험이 감소하는 경향이 있다고 해석할 수 있습니다."
  },
  {
    "objectID": "blog/posts/subgroup_analysis/index.html#subgroup-analysis",
    "href": "blog/posts/subgroup_analysis/index.html#subgroup-analysis",
    "title": "Subgroup analysis (feat. forester)",
    "section": "Subgroup analysis",
    "text": "Subgroup analysis\n위의 결과에서 치료제가 사망률을 낮추는 효과가 있는 것으로 나타났죠. 이제 하위 집단을 대상으로, 과연 이 치료제가 하위 집단별로 상호작용을 하는지, 즉 하위집단별로 치료제의 효과가 다르게 나타나는지 확인해보겠습니다.\nSubgroup analysis를 위한 함수는 sugroupAnalysis()입니다. 이 함수는 로지스틱 회귀분석 또는 콕스 회귀분석처럼 결과가 0과 1로 나뉘는 변수를 대상으로 수행할 수 있습니다.\ntraceR의 데이터를 바탕으로 하위 집단들을 생성해주겠습니다. 일반적으로 하위집단은 두 개의 범주를 갖습니다.\n\nsetDT(traceR) # data.table로 변경\n\ntraceR[,`:=`(\n  age_2 = fifelse(age < 68, \"under68\",\"over68\") |> as.factor(),\n  wmi_2 = fifelse(wallMotionIndex < 0.9, \"bad\", \"good\") |> as.factor(),\n  abd_2 = fifelse(abdominalCircumference >= 95, \"fat\", \"slim\") |> as.factor(),\n  sex = factor(sex, labels=c(\"female\",\"male\"))\n)]\n\ntraceR에 있는 데이터를 바탕으로 하위집단 세 가지를 추가했습니다.\n\nage_2: 나이의 평균인 68세보다 낮은 경우와 그렇지 않은 경우\nwmi_2: wallMotionIndex가 0.9 미만인 경우 bad, 그렇지 않으면 good\nabd_2: 복부 둘레가 95cm 이상이면 fat, 그렇지 않으면 slim\n\n여기에 sex 까지 더해 총 4개의 하위집단을 사용하겠습니다. 이 때 주의해야 할 것은 모든 하위집단은 반드시 factor로 되어있어야 한다는 것입니다.\n\nsubgroups <- c(\"sex\", \"age_2\", \"wmi_2\", \"abd_2\")\n\nsub_fit <- subgroupAnalysis(fit, \n                      data = traceR,\n                      treatment = \"treatment\",\n                      subgroups = subgroups)\nsub_fit_tbl <- copy(sub_fit) |> as.data.table()\n\nsubgroup analysis 결과가 나왔습니다. 이제 이 표를 gt()를 이용해 보기 좋게끔 변경해주도록 하겠습니다.\n\nsub_fit_tbl[,`:=`(\n  hr_ci = paste0(format(round(HazardRatio,2),nsmall=2), \n                                ' (', format(round(Lower,2),nsmall=2), '-', \n                                format(round(Upper,2),nsmall=2),\n                                ')'),\n         control = paste0(event_placebo,'/',sample_placebo),\n         case = paste0(event_trandolapril,'/',sample_trandolapril))]\n\nsub_fit_tbl[,.(subgroups, level, case, control, hr_ci, pinteraction)] |> \n  group_by(subgroups) |> \n  gt()  |> \n  cols_label(\n    level = md(\"**Subgroup**\"),\n    case = md(\"**Case**\"),\n    control = md(\"**Control**\"),\n    hr_ci = md(\"**HR (95% CI)**\"),\n    pinteraction = md(\"***P* for interaction**\")\n  ) |> \n  fmt_number(\n    column = pinteraction,\n    decimal = 4\n  ) |> \n  cols_align(\n    align=\"center\",\n    columns = -level\n  ) |> \n  cols_align(\n    align = \"left\",\n    columns = level\n  ) |> \n  tab_style(\n    style = list(cell_text(weight=\"bold\")),\n    locations = cells_row_groups()\n  ) \n\n\n\n\n\n  \n    \n    \n      Subgroup\n      Case\n      Control\n      HR (95% CI)\n      P for interaction\n    \n  \n  \n    \n      sex\n    \n    female\n183/249\n199/252\n0.83 (0.67-1.01)\n0.4077\n    male\n448/627\n453/621\n0.91 (0.80-1.04)\n0.4077\n    \n      age_2\n    \n    over68\n422/493\n427/476\n0.87 (0.76-0.99)\n0.9324\n    under68\n209/383\n225/397\n0.88 (0.73-1.06)\n0.9324\n    \n      wmi_2\n    \n    bad\n133/146\n148/161\n0.87 (0.69-1.10)\n0.8484\n    good\n498/730\n504/712\n0.90 (0.79-1.02)\n0.8484\n    \n      abd_2\n    \n    fat\n319/435\n296/403\n0.93 (0.79-1.09)\n0.3287\n    slim\n274/389\n324/427\n0.83 (0.71-0.98)\n0.3287\n  \n  \n  \n\n\n\n\n위의 표에서 Case와 Control은 각각 치료군과 대조군의 인원 중 사망한 사람들의 수를 나타냅니다. 또한 위험비가 95% 신뢰구간과 함께 나타나있고, 상호작용의 유의성을 알려주는 p for interaction이 있습니다.\n\n결과를 정리해보면 다음과 같습니다.\n하위집단들의 상호작용이 유의하지 않은 것으로 나타났습니다. 왜냐하면 상호작용 p-value를 의미하는 Pinteraction이 하위집단 모두에서 0.5 이상으로 나타났기 때문입니다. 즉, 남녀 할 것 없이, 68세 미만이든 이상이든, 좌심실 기능이 좋든 나쁘든, 뚱뚱하든 날씬하든 모두 치료제가 사망률을 낮춰주었다고 해석할 수 있습니다."
  },
  {
    "objectID": "blog/posts/subgroup_analysis/index.html#시각화",
    "href": "blog/posts/subgroup_analysis/index.html#시각화",
    "title": "Subgroup analysis (feat. forester)",
    "section": "시각화",
    "text": "시각화\nSubgroup analysis는 표 뿐만 아니라 그래프를 통해 결과를 보여줄 수 있습니다.\n\n\n\n위와 같은 그림을 forest plot이라고 부릅니다. forest plot은 메타 분석 (meta analysis)에서 많이 활용되는 그래프이지만, subgroup analysis에서도 자주 활용되는 그래프입니다. Publish에서 제공되는 함수를 통해 forest plot을 그릴 수 있습니다.\n\nplotConfidence(x=sub_fit[,c(\"HazardRatio\",\"Lower\",\"Upper\")],\n              label = sub_fit[,c(\"subgroups\",\"level\")],\n              xlab = \"Hazard ratio\",\n              points.cex = 2,\n              points.col=\"darkblue\",\n              # arrows.col=\"darkblue\",\n              points.pch=15,\n              cex=1)\n\n\n\n\n자 그런데 어디가 좀 부족해보입니다. p-value는 나오지도 않고 하니… 뭔가 아쉽죠.\nR에는 subgroup analysis 의 forest plot을 그리기 위한 패키지들이 몇 가지 존재합니다. 저는 그 중에서 forester 패키지를 사용하여 forest plot을 그려보도록 하겠습니다. 참고로 이 패키지는 CRAN에 없기 때문에, github를 통해 다운로드받아야 합니다.\n\ndevtools::install_github(\"rdboyes/forester\")\nrequire(forester)\n\n이제 forester() 를 이용해 forest plot을 그려보겠습니다.\n\nsub_fit_tbl\n\n\n\n  \n\n\nforester(left_side_data = sub_fit_tbl[,c(\"subgroups\",\"level\", \"case\",\"control\")],\n         estimate = as.numeric(sub_fit_tbl[['HazardRatio']]),\n         ci_low = as.numeric(sub_fit_tbl$Lower),\n         ci_high= as.numeric(sub_fit_tbl$Upper),\n         estimate_col_name = \"HR (95% CI)\",\n         estimate_precision =2,\n         xlim=c(0.6,1.1),\n         null_line_at = 1,\n         arrows = T,\n         arrow_labels=c('Mortality ↓ ', 'Mortality ↑'),\n         )\n\n\n\n\nforester()는 Hazard ratio 그래프를 기준으로 왼쪽과 오른쪽에 숫자들이 들어갈 수 있습니다.\n\nleft_side_data: 그래프 왼쪽에 들어갈 수치들\nright_side_data: 그래프 오른쪽에 들어갈 수치들\nestimate: HR 또는 OR\nci_lower: 하위 신뢰구간\nci_upper: 상위 신뢰구간\nxlim: 그래프의 x축 범위\nnull_line_at: 신뢰구간 기준(1) 점선으로 표시\narrows: 그래프의 x축 아래에 텍스트 표시 여부\narrow_labels: 그래프의 x축 아래에 표시할 텍스트\n\n원래는 estimate, ci_lower, ci_upper를 입력하면 자동으로 그래프 오른쪽에 HR (ci_lower to ci_upper) 형태의 열이 생성됩니다, 만약, right_side_data 값을 입력하는 경우, 그 열이 오른쪽에 들어가게 됩니다. 물론 right_side_data에 값을 준다고 해도 반드시 estimate, ci_lower, ci_upper 값은 입력해야합니다.\n또한 신뢰구간이 xlim에서 설정한 값을 넘어갈 경우 그래프는 화살표로 표시됩니다.\n더 자세한 사항은 ?forester 를 참고하시기 바랍니다.\n위의 그래프를 좀더 예쁘게 그리기 위해 하위집단 분석결과 데이터를 바탕으로 직접 데이터프레임을 만들어 그래프를 그리면 더 아름다운 forest plot이 그려보겠습니다.\n\ndf <- data.table(\n  Subgroups = c(\"Sex\", \"  Female\", \"  Male\", \"Age\", \"  <68\", \"  ≥68\", \"WMI\", \"  Bad\",\"  Good\", \"Abdominal\",\"  Fat\", \"  Slim\"),\n  Case = c(NA, \"183/249\", \"448/627\", NA, \"422/493\", \"209/383\", NA, \"133/146\", \"498/730\", NA, \"319/435\", \"274/389\"),\n  Control = c(NA, \"199/252\",\"453/621\", NA, \"427/476\", \"225/397\", NA, \"148/161   \", \"504/712\", NA, \"296/403\", \"324/427\"),\n  HR = c(NA, 0.825, 0.913, NA, 0.868, 0.876, NA, 0.874, 0.897, NA, 0.930, 0.831),\n  ci_low = c(NA, 0.675, 0.801, NA, 0.758, 0.726, NA, 0.691, 0.792, NA, 0.794, 0.708),\n  ci_high = c(NA, 1.009, 1.040, NA, 0.993, 1.058, NA, 1.105, 1.015, NA, 1.090, 0.976),\n  `P for interaction` = c(0.4077, NA, NA, 0.9324, NA, NA, 0.8484, NA, NA, 0.3287, NA, NA)\n)\ndf[!is.na(HR),`   HR (95% CI)` := paste0(format(HR,2), ' (',format(ci_low,2),'—',format(ci_high,2),\")\")]   \nforester(\n  left_side_data = df[,c(\"Subgroups\",\"Case\",\"Control\")],\n  right_side_data = df[,c(\"   HR (95% CI)\",\"P for interaction\")],\n  estimate = df$HR,\n  ci_low = as.numeric(df$ci_low),\n  ci_high= as.numeric(df$ci_high),\n  estimate_precision =2,\n  xlim=c(0.6,1.1),\n  null_line_at = 1,\n  nudge_x = 1,\n  arrows = T,\n  arrow_labels=c('Mortality ↓ ', 'Mortality ↑'),\n  justify=c(0, 0.5,0.5, 0.5, 0.5)\n)\n\n\n\n\nforest plot을 그리기 위한 데이터 프레임을 직접 만드는 건 굉장히 번거로운 일이지만, 그만큼 훌륭한 그래프가 그려지기 때문에 나쁘지 않은 것 같습니다. ☺️"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html",
    "href": "blog/posts/mice_imputation/index.html",
    "title": "MICE를 이용한 multiple imputation",
    "section": "",
    "text": "Multiple imputation은 평균(mean)이나 중앙값(median) 등 하나의 값으로 결측값을 채워넣는 single imputation과 달리, 결측치(NA)를 여러가지 변수를 고려하여 채워넣는 방법입니다. 즉, NA가 있는 변수 이외의 다른 변수들까지 함께 고려하여, 채워넣을 변수를 지정하는 방법입니다.\nR에는 multiple imputation을 도와주는 대표적인 패키지인 mice 가 있습니다. mice는 Multivariate Imputation by Chained Equations 의 약자입니다.\nmice를 통해 연속형, 이분형, 범주형 등 다양한 변수들의 결측값들을 채워넣을 수 있습니다."
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#설치",
    "href": "blog/posts/mice_imputation/index.html#설치",
    "title": "MICE를 이용한 multiple imputation",
    "section": "설치",
    "text": "설치\nMICE 를 사용하기 위한 패키지는 아래와 같습니다. mice 패키지를 통해 MICE를 진행하도록 하겠습니다.\n\n\n\nmice를 활용한 imputation 매커니즘은 아래의 그림과 같습니다.\n\n\n\n\n\n\nmice(): 결측치가 있는 불완전한 데이터에 대해 결측치 대체를 진행합니다. 이 때 결측치가 대체된 여러 개의 데이터를 생성합니다.\nwith(): 결측치가 대체된 각각의 데이터를 활용해 분석을 진행합니다.결측치가 대체된 데이터마다 분석결과가 만들어집니다 (mira, Multiply imputed repeated analysis).\npool(): 여러 개의 분석 결과를 합칩니다 (mipo, multiply imputed polled object)."
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#데이터-살펴보기",
    "href": "blog/posts/mice_imputation/index.html#데이터-살펴보기",
    "title": "MICE를 이용한 multiple imputation",
    "section": "데이터 살펴보기",
    "text": "데이터 살펴보기\nmice에 내장되어 있는 nhanes 데이터를 이용하도록 하겠습니다.\n\nmice::nhanes\n\n\n\n  \n\n\n\n우선 데이터의 어떤 변수에 NA가 있는지, 결측값의 규칙을 확인할 수 있습니다.\n\nmd.pattern(nhanes)\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\n위 코드를 실행하게 되면, 테이블과 그래프가 함께 출력됩니다.\n테이블의 경우, 1이 NA가 아닌 경우, 0이 NA인 경우를 의미합니다.\n좀 더 직관적으로 NA의 패턴을 이해하려면 그래프를 보는 것이 좋습니다.\n위의 그림에서 파란 색은 값이 존재하는 것이고, 빨간색이 NA인 부분입니다. 그래프를 해석하면 다음과 같습니다.\n\n맨 위는 column별 이름입니다.\n오른쪽의 번호는 NA가 있는 개수를 규칙으로 번호를 매긴 것입니다. 예를 들어 0은 age ~ chl까지 하나도 결측치가 없는 것을 의미합니다. 1의 경우에는 4가지 column중 NA가 한 군데 존재하는 것을 의미합니다. 3의 경우 4가지 column 중 3개에서 NA가 있는 경우를 의미합니다.\n왼쪽의 번호는 데이터에서 오른쪽의 규칙에 해당하는 row(행)들이 몇 개가 있는지를 나타냅니다. 예를 들어 결측치가 하나도 없는 행은 전체 데이터에서 총 13개이고, 결측치가 hyp, bmi, chl에 있는 경우는 총 7개의 행이 있다는 것입니다.\n맨 아래의 숫자 (0, 8, 9, 10)은 각 column별로 존재하는 NA의 합입니다."
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#imputation",
    "href": "blog/posts/mice_imputation/index.html#imputation",
    "title": "MICE를 이용한 multiple imputation",
    "section": "Imputation",
    "text": "Imputation\n이제 mice()를 이용하여 NA를 채워넣어보도록 하겠습니다. mice()에서 인자를 설정해줄 수 있는데, 다음과 같습니다.\n\nmethod: 결측치를 채우는 imputation 방법. 지정해주는 값에 따라 single imputation이 될 수도, multiple imputation이 될 수도 있음.\n\nsingle imputation: mean, median, norm.predict 등\nmultiple imputation: pmm, rf, cart 등\n\nm: NA를 채운 multiple imputation 데이터셋의 수\nmaxit: multiple imputation 값을 지정할 때 iteration 횟수\nseed: 동일한 결과 출력을 위한 난수 지정\n\nm의 경우 imputation 된 데이터의 개수인데, m이 높을수록 정확성이 올라가지만, imputation 되는 데 시간이 많이 소요됩니다.\nmaxit의 경우, MCMC(Markov Chain Monte Carlo) 알고리즘을 이용해 반복을 하게 되는데, 이 때 반복할 횟수를 지정해주게 됩니다. maxit이 높을수록 알고리즘의 수렴도가 개선되지만, m과 마찬가지로 계산시간이 오래 걸릴 수 있습니다.\n\n\n\n\n\n\nNote\n\n\n\nmice 패키지의 창시자 Van Buuren (2018)은 “iteration의 수는 변수 간 상관관계와 변수의 결측 비율에 따라 달라진다”고 이야기했습니다. 그는 또한 5-20번의 반복이면 결과가 제대로 수렴할 것이라고 이야기했습니다.\n\n\n\n\n\n\n\n\nTip\n\n\n\nMonte Carlo\n통계적인 수치를 얻기 위해 수행하는 시뮬레이션. 통계학의 특성 상, 무수히 많은 시도를 거쳐야 정답을 파악할 수 있지만, 현실적으로는 불가능하기 때문에 유한한 시도를 통해 정답을 추정하는 것.\nMarkov Chain\n어떤 상태에서 다른 상태로 넘어갈 때, 바로 전 단계의 상태에만 영향을 받는 확률 과정.\n\n예시: 어제 짜장면을 먹은 사람의 경우, 오늘 면 종류의 음식을 먹지 않을 것\n\nMCMC를 수행한다는 것은 첫 샘플을 랜덤하게 선정한 뒤, 그 다음의 샘플링이 추천되는 방식의 시도를 무수하게 해보는 것.\n출처: https://angeloyeo.github.io/2020/09/17/MCMC.html\n\n\n\nimp <- mice(nhanes, \n            method = \"pmm\",\n            maxit = 5, \n            m = 5,\n            seed = 2023,\n            printFlag = F)\nimp\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nimp는 mice 객체입니다. multiple imputation 횟수와 함께 imputation methods가 등장합니다.\nimp 객체는 NA가 채워진 데이터는 물론, imputation 이전 데이터, 채워진 값, NA의 개수, 반복횟수 등의 값을 갖고 있습니다. 예를 들어 imp$method를 통해 변수별 imputation 방법을 확인할 수 있습니다.\n\nattributes(imp)\n\n$names\n [1] \"data\"            \"imp\"             \"m\"               \"where\"          \n [5] \"blocks\"          \"call\"            \"nmis\"            \"method\"         \n [9] \"predictorMatrix\" \"visitSequence\"   \"formulas\"        \"post\"           \n[13] \"blots\"           \"ignore\"          \"seed\"            \"iteration\"      \n[17] \"lastSeedValue\"   \"chainMean\"       \"chainVar\"        \"loggedEvents\"   \n[21] \"version\"         \"date\"           \n\n$class\n[1] \"mids\"\n\nimp$method\n\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \n\n\nmice의 imputation methods는 다양합니다 (methods(mice) 참고).\nmice() 함수에서 method 인자를 통해 imputation 방법을 지정해줄 수도 있습니다.\n기본적으로 mice()는 연속형 변수에 대해서는 pmm을, 범주형 변수에 대해서는 logreg를 적용해 NA를 채워넣습니다.\n범주형 변수에 대해 자동으로 logreg를 적용하기 위해서는 해당 변수를 character 또는 factor로 변경해주어야 합니다.\n\nnhanes$hyp <- as.factor(nhanes$hyp)\nimp2 <- mice(nhanes, method = c(\"\",\"pmm\",\"logreg\",\"pmm\"), printFlag = F)\nimp2\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n     age      bmi      hyp      chl \n      \"\"    \"pmm\" \"logreg\"    \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\npmm은 hot deck 방법 중 하나입니다. 이 때, hot deck 방법이란, NA가 아닌 데이터를 다른 변수와 매칭시켜 NA를 채워넣는 방법입니다. 쉽게 말해서, NA가 있는 행의 데이터의 다른 열을 바탕으로 NA의 행을 채워넣는 것을 말합니다.\npmm 알고리즘은 쉽게 말해, 특정 변수의 NA가 아닌 값과 다른 변수를 활용하여 NA의 값을 채워넣는 것입니다. 예를 들어 nhanes 데이터의 bmi 변수에 결측치가 있어 이를 채워넣는다고 했을 때, age와 bmi의 관계를 고려하여 bmi의 NA를 채워넣는 것입니다.\n\n\n\n\n\n\nNote\n\n\n\n좀더 디테일하게 pmm 알고리즘의 과정을 살펴보겠습니다.\n\nbmi 변수와 age 변수를 활용하여 regression을 통해 age에 대한 회귀계수 \\(\\hat{\\beta}_{bmi}\\) 을 계산합니다.\n베이지안 회귀분석을 통해 age에 대한 회귀계수 \\(\\bar{\\beta}_{bmi}\\) 를 계산합니다.\n\\(\\hat{\\beta}_{bmi}\\)를 바탕으로 결측치가 아닌 bmi 값들에 대한 관측값 \\(bmi_{obs}\\)을 예측합니다. 또한 결측치 bmi 값들에 대한 \\(bmi_{pred}\\) 를 계산합니다. 여기서 \\(bmi_{pred}\\) 값이 각각 21.5, 23.8, 28.3이 나왔다고 가정하겠습니다.\n\\(bmi_{obs}\\) 와 \\(bmi_{pred}\\) 의 차이를 구하여 가장 낮은 차이를 갖는 값들을 구합니다. 예를 들어 첫 번째 \\(bmi_{pred}\\) 값인 21.5와의 차이를 계산했을 때, 1.4, 1.5, 1.7 순으로 낮은 값을 갖습니다.\n\n\n\n\\(age\\)\n\\(bmi\\)\n\\(bmi_{obs}\\)\n\\(difference\\)\n\n\n\n\n1\nNA\nNA\nNA\n\n\n2\n22.7\n23.0\n1.5\n\n\n1\nNA\nNA\nNA\n\n\n3\nNA\nNA\nNA\n\n\n1\n20.4\n20.7\n1.7\n\n\n1\n22.5\n22.9\n1.4\n\n\n1\n30.1\n30.7\n9.2\n\n\n\n이후, 낮은 값을 갖는 \\(bmi_{obs}\\) 의 값인 23.0, 20.7, 22.9 등이 랜덤하게 선택되어, bmi의 NA를 채우게 됩니다.\n\n\n\n이처럼 pmm 알고리즘의 장점은 NA를 비현실적인 데이터로 채우는 것이 아니라, NA가 아닌 관측되는 값 중에서 채워넣는다는 것입니다.\nlogreg 방식도 pmm과 마찬가지로 진행이 됩니다.\nmice로 NA를 채워준 이후, 원래 데이터들처럼 잘 채워졌는지 확인하기 위해서 stripplot() 을 통해 분포를 확인해볼 수 있습니다. 파란 점이 관측값, 붉은 점이 NA에서 채워진 값들입니다.\n그래프를 봤을 때, imputation된 값들이 일반 관측치들과 거의 유사한 분포를 띄고 있는 것을 확인할 수 있습니다.\n\nstripplot(imp2,\n          data = chl~.imp,\n          pch=20, \n          cex = 2)"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#modeling",
    "href": "blog/posts/mice_imputation/index.html#modeling",
    "title": "MICE를 이용한 multiple imputation",
    "section": "Modeling",
    "text": "Modeling\nwith 함수를 통해 imputation 객체 내에 있는 결측치 대체 데이터별 모델링을 수행합니다. mice()에서 지정된 imputation 수 (default=5) 별로 각각 모델링이 됩니다.\n\nfit <- with(imp2, lm(chl ~ bmi + age))\nfit\n\ncall :\nwith.mids(data = imp2, expr = lm(chl ~ bmi + age))\n\ncall1 :\nmice(data = nhanes, method = c(\"\", \"pmm\", \"logreg\", \"pmm\"), printFlag = F)\n\nnmis :\nage bmi hyp chl \n  0   9   8  10 \n\nanalyses :\n[[1]]\n\nCall:\nlm(formula = chl ~ bmi + age)\n\nCoefficients:\n(Intercept)          bmi          age  \n     -11.89         5.44        35.63  \n\n\n[[2]]\n\nCall:\nlm(formula = chl ~ bmi + age)\n\nCoefficients:\n(Intercept)          bmi          age  \n     42.963        3.644       32.914  \n\n\n[[3]]\n\nCall:\nlm(formula = chl ~ bmi + age)\n\nCoefficients:\n(Intercept)          bmi          age  \n    -53.261        6.587       36.017  \n\n\n[[4]]\n\nCall:\nlm(formula = chl ~ bmi + age)\n\nCoefficients:\n(Intercept)          bmi          age  \n     26.877        5.038       19.503  \n\n\n[[5]]\n\nCall:\nlm(formula = chl ~ bmi + age)\n\nCoefficients:\n(Intercept)          bmi          age  \n     15.417        4.626       31.404"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#pooling",
    "href": "blog/posts/mice_imputation/index.html#pooling",
    "title": "MICE를 이용한 multiple imputation",
    "section": "Pooling",
    "text": "Pooling\n마지막으로 풀링을 통해, 앞서 만든 5개의 imputation 된 데이터들의 회귀계수, 신뢰 구간, p-value 등을 계산할 수 있습니다.\n\nfit_pooled <- pool(fit)\nsummary(fit_pooled, conf.int = T)"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#데이터-추출",
    "href": "blog/posts/mice_imputation/index.html#데이터-추출",
    "title": "MICE를 이용한 multiple imputation",
    "section": "데이터 추출",
    "text": "데이터 추출\ncomplete()를 통해 imputation 된 데이터를 내보낼 수 있습니다. mice() 시 m(imputation 횟수)을 5로 주었기 때문에 1~5까지의 imputation 데이터가 존재합니다.\n일반적으로 결측치가 대체된 데이터셋은 “long” 을 이용해 데이터를 추출하여 요약통계량을 계산합니다. “long”을 이용하면 5개의 데이터셋이 행 단위로 붙게 됩니다.\n\n\n\n\n\n\nDanger\n\n\n\ncomplete() 사용 시, 1~5번 중 하나를 선택하여 데이터셋을 사용하는 것은 잘못된 방법입니다.\n\n\n\ndt_imp <- complete(imp2, \"long\")"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#기술통계량-계산",
    "href": "blog/posts/mice_imputation/index.html#기술통계량-계산",
    "title": "MICE를 이용한 multiple imputation",
    "section": "기술통계량 계산",
    "text": "기술통계량 계산\n\n\n\n\n\n\nWarning\n\n\n\n해당 부분은 아직까지 공부 중입니다. 확인해본 결과, 아래의 방법들로 기술통계량을 계산할 수는 있으나, 생각보다 번거롭습니다. 더 나은 함수나 패키지가 있는지 확인이 되면 수정하도록 하겠습니다!\n\n\n지금까지는 mice를 통해 회귀분석 등의 모델링을 하는 방법에 대해 살펴보았습니다.\n이제 문제는 multiple imputation된 데이터들을 어떻게 종합하여 baseline table 등을 만들 것인지 입니다. multiple imputation된 데이터에서 각 변수별 기술통계량 또는 요약통계량(descriptive statistics)을 계산하는 방법은 Rubin’s rule을 따라야 합니다.\nRubin’s Rules (RR) 이란 평균, 회귀계수, 표준오차, 신뢰 구간, p-value와 같은 파라미터 측정치들을 풀링하기 위해 고안된 규칙입니다.\n한마디로 여러 번 진행된 imputation의 값들을 반영하기 위해, 풀링하여 계산하여야 한다는 뜻입니다.\n\nt-test\nmultiple imputation 을 위한 t-tests는 MKmisc 패키지의 mi.t.test() 를 이용해 가능합니다. 다만 MKmisc 패키지를 설치하기 위해서는 BiocMananger 를 우선 설치해야 합니다. 그 다음 limma를 설치해주어야 MKmisc를 설치할 수 있습니다.\n\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"limma\")\ninstall.packages(\"MKmisc\")\nlibrary(MKmisc)\n\nmi.t.test()에 들어가는 값은 complete(\"long\")을 이용해 추출한 전체 데이터를 리스트로 변환한 값, 연속형 변수 x, 범주형 종속형 변수 y입니다.\n\nlibrary(MKmisc)\n\ndt_imp_list <- split(dt_imp, dt_imp$.imp)\nMKmisc::mi.t.test(dt_imp_list, x='bmi', y=\"hyp\")\n\n\n    Multiple Imputation Welch Two Sample t-test\n\ndata:  Variable bmi: group 1 vs group 2\nt = -0.020371, df = 8.7977, p-value = 0.9842\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.954949  3.884601\nsample estimates:\n mean (1)    SD (1)  mean (2)    SD (2) \n26.758382  4.664684 26.793556  3.600881 \n\n\n\n\nchi-square\nmultiple imputation 데이터에 카이제곱 검정을 수행하기 위해서는 miceadds패키지의 micombine.chisquare()를 이용합니다.\n\ninstall.packages(\"miceadds\")\nlibrary(miceadds)\n\n\nimps <- unique(dt_imp$.imp)\ndt_imp$hyp <- as.character(dt_imp$hyp) |> as.numeric()\nmi_chisq <- lapply(imps, \\(x)\n       xtabs(hyp ~ age, dt_imp, subset = imps==x)) |> \n  lapply(chisq.test)\nmi_chisq[[1]]$statistic\n\nX-squared \n 7.258065 \n\nsapply(mi_chisq, \\(x) x$statistic) |> miceadds::micombine.chisquare(df=2, display = T)\n\nCombination of Chi Square Statistics for Multiply Imputed Data\nUsing 5 Imputed Data Sets\nF(2, 7.94)=-0.111     p=1 \n\n\n\n\nANOVA\nANOVA 또한 miceadds패키지를 이용합니다. mi.anova()를 이용합니다.\n\nmi.anova(imp2, formula = \"bmi ~ age\")\n\nUnivariate ANOVA for Multiply Imputed Data (Type 2)  \n\nlm Formula:  bmi ~ age\nR^2=0.2115 \n..........................................................................\nANOVA Table \n                  SSQ df1      df2 F value  Pr(>F)    eta2 partial.eta2\nage          84.80682   1 558.8433  5.5094 0.01926 0.21146      0.21146\nResidual    316.24166  NA       NA      NA      NA      NA           NA"
  },
  {
    "objectID": "blog/posts/mice_imputation/index.html#참고자료",
    "href": "blog/posts/mice_imputation/index.html#참고자료",
    "title": "MICE를 이용한 multiple imputation",
    "section": "참고자료",
    "text": "참고자료\nhttps://data.library.virginia.edu/getting-started-with-multiple-imputation-in-r/\nhttps://angeloyeo.github.io/2020/09/17/MCMC.html\nhttps://bookdown.org/mwheymans/bookmi/data-analysis-after-multiple-imputation.html\nhttps://bookdown.org/mwheymans/bookmi/rubins-rules.html\nhttps://nerler.github.io/EP16_Multiple_Imputation/slide/03_analysis_and_pooling.pdf"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html",
    "href": "blog/posts/mlr3_resampling/index.html",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "",
    "text": "Important\n\n\n\n이 글은 mlr3book1을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 mlr32 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다."
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#resampling-종류",
    "href": "blog/posts/mlr3_resampling/index.html#resampling-종류",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "Resampling 종류",
    "text": "Resampling 종류\nmlr3 에서 실행가능한 모든 리샘플링 전략은 mlr_resamplings 딕셔너리를 통해 확인가능합니다. 홀드아웃, 교차 검증(CV), 부트스트랩 등이 포함되어있습니다.\n\nas.data.table(mlr_resamplings)\n\n\n\n  \n\n\n\nparams 열을 보면 resampling을 위한 파라미터들이 나와있습니다. 예를 들어 holdout은 ratio를 통해 어떤 비율로 train, test를 나눌 것인지, cv는 folds를 통해 몇 개로 데이터를 나눌 것인지를 설정해줄 수 있죠."
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#resampling-객체-생성",
    "href": "blog/posts/mlr3_resampling/index.html#resampling-객체-생성",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "Resampling 객체 생성",
    "text": "Resampling 객체 생성\n리샘플링 객체를 만들어봅시다. 우선 holdout을 이용해 리샘플링을 진행하겠습니다. 리샘플링 객체는 rsmp()를 통해 만들 수 있습니다.\n\nresampling <- rsmp(\"holdout\")\nresampling\n\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n\n\n생성한 리샘플링 객체를 확인했을 때, Instantiated: FALSE라고 되어있습니다. 아직 리샘플링을 수행하지 않았기 때문입니다.\n또한 holdout 의 ratio를 정해주지 않았기 문에 2/3가 초기값으로 설정되어있습니다. 즉 데이터의 3분의 2는 훈련에, 3분의 1은 검증에 쓰이게 됩니다. 새로운 리샘플링 객체를 만들 때 holdout 비율을 설정하거나 기존의 객체의 파라미터 값을 수정할 수 있습니다.\n\nresampling <- rsmp(\"holdout\", ratio=0.8)\nresampling$param_set$values <- list(ratio=0.5)\n\nholdout은 성능을 일반화하는 과정을 한 번밖에 수행하지 않습니다. 따라서 좀더 신뢰성 있는 성능 측정을 위해, 가장 많이 사용되는 리샘플링 방법 중 하나인 교차 검증(cv)를 사용하도록 하겠습니다.\n\nresampling <- rsmp(\"cv\", folds=10)"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#instantiation",
    "href": "blog/posts/mlr3_resampling/index.html#instantiation",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "Instantiation",
    "text": "Instantiation\n데이터가 들어가 있는 태스크에 대해 리샘플링을 수행하기 위해 리샘플링 객체의 $instantiate() 메소드를 이용해야합니다. 메소드 안에 태스크를 넣어주면 리샘플링이 적용됐다는 의미에서 Instantiated: TRUE가 출력됩니다.\n\ntask <- tsk(\"sonar\")\nresampling$instantiate(task)\nresampling\n\n<ResamplingCV>: Cross-Validation\n* Iterations: 10\n* Instantiated: TRUE\n* Parameters: folds=10"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#실행",
    "href": "blog/posts/mlr3_resampling/index.html#실행",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "실행",
    "text": "실행\n리샘플리의 실행은 resample() 을 사용합니다.\n\nlearner <- lrn(\"classif.rpart\", predict_type=\"prob\")\nrr <- resample(task, learner, resampling)\nrr\n\n<ResampleResult> with 10 resampling iterations\n task_id    learner_id resampling_id iteration warnings errors\n   sonar classif.rpart            cv         1        0      0\n   sonar classif.rpart            cv         2        0      0\n   sonar classif.rpart            cv         3        0      0\n   sonar classif.rpart            cv         4        0      0\n   sonar classif.rpart            cv         5        0      0\n   sonar classif.rpart            cv         6        0      0\n   sonar classif.rpart            cv         7        0      0\n   sonar classif.rpart            cv         8        0      0\n   sonar classif.rpart            cv         9        0      0\n   sonar classif.rpart            cv        10        0      0\n\n\n리샘플링 실행이 완료되었고, rr이라는 객체 안에 리샘플링 결과가 저장되어있습니다. 리샘플링을 통한 모델의 성능 평가는 $score()와 $aggregate() 메소드를 이용합니다.\n$score()와 $aggregate() 모두 Measure 객체를 이용해 성능을 측정합니다. 기본적으로 성능을 측정할 때는 검증을 위한 데이터셋을 활용하게 됩니다.\n우선 정확도(classif.acc)를 통해 모델의 정확도를 평가해보겠습니다.\n\nacc <- rr$score(msr(\"classif.acc\"))\nacc[,.(iteration, classif.acc)]\n\n\n\n  \n\n\n\n$score()를 통해 모델의 성능을 살펴보니 cv에서 설정한 10번의 반복 별로 성능이 출력되는 것을 알 수 있습니다.\n다음으로 $aggregate()를 이용해 모델의 성능을 살펴보겠습니다. $aggregate()는 반복 때마다 계산된 성능의 평균 값을 계산하여 출력합니다.\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.7219048 \n\n\n여러 개의 평가 지표로 모델의 성능을 평가할 수도 있습니다.\n\nmeasures <- msrs(c(\"classif.acc\",\n                   \"classif.sensitivity\",\n                   \"classif.specificity\",\n                   \"classif.auc\"))\n\nrr$aggregate(measures)\n\n        classif.acc classif.sensitivity classif.specificity         classif.auc \n          0.7219048           0.7557955           0.7094017           0.7738624"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#결과-시각화",
    "href": "blog/posts/mlr3_resampling/index.html#결과-시각화",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "결과 시각화",
    "text": "결과 시각화\nmlr3viz의 autoplot()을 통해 리샘플링 결과를 시각화할 수 있습니다.\n\nrequire(mlr3viz)\nautoplot(rr, measure = msr(\"classif.acc\"), type=\"boxplot\")\nautoplot(rr, measure = msr(\"classif.acc\"), type=\"histogram\")\n\n\n\n\n\n\nBoxplot\n\n\n\n\n \n\n\n\n\n\nHistogram"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#벤치마킹-설계하기",
    "href": "blog/posts/mlr3_resampling/index.html#벤치마킹-설계하기",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "벤치마킹 설계하기",
    "text": "벤치마킹 설계하기\nmlr3에서 벤치마크 실험을 진행하기 위해서는 benchmark_grid() 를 이용해 설계를 해줘야 합니다. 이 때 설계라는 것은 어떤 태스크를 사용할 것인지, 어떤 러너들을 어떻게 리샘플링 할 것인지 등을 조합해주는 것입니다.\nsonar 태스크를 예시로, 분류 모형 중 로지스틱 회귀분석 (classif.log_reg), 의사결정나무(classif.rpart), 랜덤 포레스트(classif.ranger) 러너를 5 Fold 교차검증(CV)으로 비교하도록 설계를 진행하겠습니다.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\ntsk <- tsk(\"sonar\")\nlrns <- lrns(c(\"classif.log_reg\",\"classif.rpart\",\"classif.ranger\"),\n                  predict_type=\"prob\")\nrsmp <- rsmps(\"cv\", folds=5)\ndesign <- benchmark_grid(\n  tasks = tsk,\n  learners = lrns,\n  resamplings = rsmp\n)\nhead(design)"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#벤치마킹-실험하기",
    "href": "blog/posts/mlr3_resampling/index.html#벤치마킹-실험하기",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "벤치마킹 실험하기",
    "text": "벤치마킹 실험하기\n벤치마크 설계를 실행하기 위해선 benchmark()함수를 이용합니다.\n\nbmr <- benchmark(design)\nbmr\n\n<BenchmarkResult> of 15 rows with 3 resampling runs\n nr task_id      learner_id resampling_id iters warnings errors\n  1   sonar classif.log_reg            cv     5        0      0\n  2   sonar   classif.rpart            cv     5        0      0\n  3   sonar  classif.ranger            cv     5        0      0\n\n\n벤치마킹이 끝났다면, $aggregate() 를 이용해 성능을 검증해보면 됩니다. 분류의 정확도를 예시로 살펴보겠습니다.\n\nacc <- bmr$aggregate(msr(\"classif.acc\"))\nacc[,.(task_id, learner_id, classif.acc)]"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#벤치마크결과-객체-살펴보기",
    "href": "blog/posts/mlr3_resampling/index.html#벤치마크결과-객체-살펴보기",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "벤치마크결과 객체 살펴보기",
    "text": "벤치마크결과 객체 살펴보기\n벤치마크결과(BenchmarkResult) 객체는 리샘플링결과(ResampleResult) 객체의 집합입니다. 이 객체는 $resample_result(i) 메소드를 이용해 리샘플링 결과를 추출할 수 있습니다. 이 때, i는 벤치마크 실험번호를 의미합니다. 위의 예시는 세 가지 러너로 벤치마킹을 했기 때문에, i는 러너를 의미합니다.\n\nrr_lr <- bmr$resample_result(1)\nrr_rpart <- bmr$resample_result(2)\nrr_ranger <- bmr$resample_result(3)\nrr_lr\n\n<ResampleResult> with 5 resampling iterations\n task_id      learner_id resampling_id iteration warnings errors\n   sonar classif.log_reg            cv         1        0      0\n   sonar classif.log_reg            cv         2        0      0\n   sonar classif.log_reg            cv         3        0      0\n   sonar classif.log_reg            cv         4        0      0\n   sonar classif.log_reg            cv         5        0      0\n\n\n\nrr_ranger\n\n<ResampleResult> with 5 resampling iterations\n task_id     learner_id resampling_id iteration warnings errors\n   sonar classif.ranger            cv         1        0      0\n   sonar classif.ranger            cv         2        0      0\n   sonar classif.ranger            cv         3        0      0\n   sonar classif.ranger            cv         4        0      0\n   sonar classif.ranger            cv         5        0      0\n\n\nResampleResult 객체들은 as_benchmark_result() 함수를 이용해 다시 BenchmarkResult로 변환될 수 있습니다. 또한 c()를 이용해 묶을 수 있습니다.\n\nbmr1 = as_benchmark_result(rr_rpart)\nbmr2 = as_benchmark_result(rr_ranger)\n\nbmr_combined = c(bmr1, bmr2)\nbmr_combined$aggregate(msr(\"classif.acc\"))\n\n\n\n  \n\n\n\n\nlibrary(mlr3viz)\nautoplot(bmr, measure=msr(\"classif.acc\"))"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#클래스-함수-정리",
    "href": "blog/posts/mlr3_resampling/index.html#클래스-함수-정리",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "클래스, 함수 정리",
    "text": "클래스, 함수 정리\n\n\n\n\n\n\n\n\nR6 Class\nSugar function\n요약\n\n\n\n\nResampling\nrsmp()\n리샘플링을 위해 훈련, 검증 세트 배정\n\n\nResampleResult\nresample()\n리샘플링으로 주어진 태스크에 대한 러너 평가\n\n\n-\nbenchmark_grid()\n러너, 태스크, 리샘플링 그리드 구성\n\n\nBenchmarkResult\nbenchmark()\n벤치마크 그리드로 러너 평가"
  },
  {
    "objectID": "blog/posts/mlr3_resampling/index.html#레퍼런스",
    "href": "blog/posts/mlr3_resampling/index.html#레퍼런스",
    "title": "mlr3 리샘플링 벤치마킹",
    "section": "레퍼런스",
    "text": "레퍼런스\n\nhttps://mlr3book.mlr-org.com/performance.html#sec-roc"
  },
  {
    "objectID": "blog/posts/ggsurvplot_with_list/index.html",
    "href": "blog/posts/ggsurvplot_with_list/index.html",
    "title": "Kaplan-Meier 곡선",
    "section": "",
    "text": "특정 범주 변수에 따른 생존률 시각화하는 사용자지정 함수를 만들어보았습니다. survival의 lung 데이터 이용하여 예시를 보여드리겠습니다.\n\nlibrary(survival)\nlibrary(survminer)\nlibrary(dplyr)\ndf <- lung |> \n  transmute(time,\n            status,  # censoring status 1=censored, 2=dead\n            Age = age,\n            Sex = factor(sex, labels = c(\"Male\", \"Female\")),\n            ECOG = factor(ph.ecog),\n            `Meal Cal` = as.numeric(meal.cal))\n\nvars <- c(\"ECOG\", \"Sex\")\nsurv_plot_func <- function(df, vars, time, status) {\n\n  results_list <- lapply(vars, \\(x,...){\n    # Creating a formula as a string\n    form <<- paste0(\"Surv(\", time, \", \", status,\") ~ \",x)\n    fit <- survfit(as.formula(form), data=df)\n    \n    # Plot the Kaplan-Meier curve using ggsurvplot\n    ggsurv <- ggsurvplot(fit, pval = TRUE, conf.int = TRUE,\n                         risk.table = TRUE, legend.title = \"\",\n                         surv.median.line = \"hv\", xlab = \"Time\", ylab = \"Survival Probability\")\n\n    # Return the fit and ggsurv as a list\n    list(fit = fit, ggsurv = ggsurv)\n  }) |> setNames(vars)\n\n    # Return the list of results\n  return(results_list)\n}\nres_list <- surv_plot_func(df, vars, \"time\", \"status\")\n\nggsurvplot 의 코드 문제인지 form을 <- 로 선언하면 에러메시지가 발생합니다. 그래서 <<-을 통해 권역 객체로 선언하였습니다."
  },
  {
    "objectID": "blog/posts/2023-02-09-EDA/index.html",
    "href": "blog/posts/2023-02-09-EDA/index.html",
    "title": "DataExplorer을 활용한 EDA",
    "section": "",
    "text": "1. DataExplorer\n\nrequire(DataExplorer)\nrequire(dataxray)\ncreate_report(titanic_train, \n              config = configure_report(add_plot_qq=F,\n                                        add_plot_prcomp = F,\n                                        global_ggtheme = theme_classic(),\n                                        ))\n\nplot_histogram(titanic_train)\ntitanic_train$Survived <- as.factor(titanic_train$Survived)\nplot_boxplot(titanic_train, by = 'Survived',\n             ggtheme = theme_classic())\nstr(titanic_train)\ndummify(titanic_train)\n\n\nExpReport(titanic_train,\n          Target = 'Survived',\n          op_file = 'temp.html')\n\n\n\n2. dataxary\n\ntitanic_train |> \n  dataxray::make_xray() |> \n  view_xray()"
  },
  {
    "objectID": "blog/posts/2023-01-25-dplyr_basic/index.html",
    "href": "blog/posts/2023-01-25-dplyr_basic/index.html",
    "title": "dplyr 기초 문법 이해하기",
    "section": "",
    "text": "R에서 가장 인기가 많은 패키지 중 하나를 꼽으라고 한다면 바로 dplyr 일 것입니다.\ndplyr 는 데이터 핸들링 패키지 중 하나로, 아래의 작업을 빠르고 쉽게 도와줄 수 있습니다.\n\n데이터를 다루는 흐름을 그대로 코드에 나타냄으로써, 분석 흐름을 이해하기 쉽습니다.\n단순한 영어 동사로 이루어진 함수들을 제공하여, 작성하는 코드를 이해하기 쉽습니다.\n\ndplyr 패키지에 대해 설명하기 위해 starwars 데이터를 사용하도록 하겠습니다. 자세한 사항은 ?starwars 를 통해 확인할 수 있습니다.\n\nlibrary(dplyr)\n\nstarwars\n\n\n\n  \n\n\n\nstarwars는 tibble 데이터 구조를 갖습니다. 이는 dplyr 로 데이터를 불러올 때의 데이터 구조입니다. tibble은 data.frame과 동일한 구조이지만 다른 점이 존재합니다.\n\ntibble은 달리 행의 일부분만 보여줍니다.\ntibble은 column의 데이터 유형(type)도 확인할 수 있습니다.\n\n\n1. dplyr의 동사들\ndplyr는 영어 단어 동사를 함수로 사용하여 데이터를 다루게 됩니다. 이 함수들은 크게 세 가지로 나누어 볼 수 있습니다.\n\n행 (Rows)\n\nfilter() : 열을 기준으로 조건에 맞는 행들을 선택합니다.\nslice() : 위치를 기준으로 행들을 선택합니다.\narrange() : 행의 순서를 변경합니다.\n\n열 (Columns)\n\nselect() : 특정 열을 선택합니다.\nrename() : 열의 이름을 변경합니다.\nmutate() : 새로운 열을 추가합니다.\nrelocate() : 열의 순서를 변경합니다.\n\n행 요약\n\nsummarise() : 여러 행을 계산합니다.\n\n\n\n\n2. Pipe 구조\n모든 dplyr의 함수는 분석하고자 하는 데이터(tibble)을 첫 번째 인자로 받습니다. dplyr에서는 f(x,y) 구조 보다는 x %>% f(y) 형태로 입력합니다. 이 때 %>% 또는|>를 파이프(pipe) 연산자라고 합니다. 파이프 연산자를 여러 번 사용하여 여러 함수를 이어서 실행할 수 있습니다.\n\n\n\n3. filter() 행 선택하기\nfilter()는 데이터에서 일부 행들을 선택하는 함수입니다. filter() 안에는 찾고자 하는 조건이 TRUE인 행들만 선택이 되게 됩니다.\n예를 들어, starwars 데이터에서 skin_color가 light이고 eye_color가 brown인 경우를 찾을 때는\n\nstarwars |> filter(skin_color=='light',\n                   eye_color=='brown')\n\n\n\n  \n\n\n\n이는 기본 R 코드에서 다음과 같이 구현할 수 있습니다.\n\nstarwars[starwars$skin_color=='light' & starwars$eye_color=='brown',]\n\n\n\n  \n\n\n\ndplyr 를 이용하면, 불필요하게 데이터$변수 를 반복해서 사용할 필요가 없겠죠.\n\n\n4. arrange(): 행 순서 정렬하기\narrange()는 filter()와 동일하게 행을 다루는 함수입니다. 다만 arrange()는 다루는 데이터의 행을 정렬합니다.\n만약 두 가지 이상의 열 이름을 제공하는 경우, 첫 번째 열에서 같은 값이 나왔을 때, 추가적으로 행을 정렬하는 기준이 정해지게 됩니다.\n\nstarwars |> arrange(height)\n\n\n\n  \n\n\n\n위의 정렬된 결과를 보면 R2-D2와 R4-P17의 키가 동일한 것을 알 수 있습니다. 이처럼 동일한 값이 오는 경우, arrange()에 하나의 값을 더 추가함으로써 추가적인 정렬 기준을 세우는 것입니다.\n\nstarwars |> arrange(height, mass)\n\n\n\n  \n\n\n\nheight를 기준으로 오름차순이 되었고, height가 같은 경우, mass가 작은 값부터 오게끔 정렬되었습니다.\n만약 내림차순으로 데이터를 정렬하고 싶은 경우는 desc() 를 함께 사용하면 됩니다.\n\nstarwars |> arrange(desc(height))\n\n\n\n  \n\n\n\n\n\n5. slice(): 행의 위치를 통한 행 선택\nslice()는 행의 번호(index)를 통해 행들을 선택합니다.\n\nstarwars |> slice(5:10)\n\n\n\n  \n\n\n\nslice() 계열의 함수들은 다음과 같습니다.\n\nslice_head(), slice_tail(): 각각 데이터의 첫 ~행, 마지막 ~행을 선택할 수 있습니다.\n\nstarwars |> slice_head(n=5)\n\n\n\n  \n\n\nstarwars |> slice_tail(n=3)\n\n\n\n  \n\n\n\nslice_sample(): 무작위로 행을 선택합니다. n을 통해 행의 개수를 선택하거나, prop 옵션을 통해 특정 비율만큼 행을 선택할 수 있습니다.\n\nstarwars |> slice_sample(n=10)\n\n\n\n  \n\n\nstarwars |> slice_sample(prop = 0.1)\n\n\n\n  \n\n\n\nreplace=T를 통해 복원추출을 수행할 수 있습니다.\nslice_min(), slice_max()를 통해 특정 열의 가장 높거나 가장 낮은 값을 갖는 행을 선택할 수 있습니다.\n\nstarwars |> slice_max(height,n=3)\n\n\n\n  \n\n\n\n\n\n\n6. select(): 열 선택하기\n큰 데이터를 다루게 될 경우, 많은 열이 존재하기 마련입니다. 이 중 분석하고자 하는 일부의 열만 선택할 수 있도록 도와주는 함수가 바로 select()입니다.\n\nstarwars |> select(hair_color, skin_color, eye_color)\n\n\n\n  \n\n\n\n만약 열들이 붙어 있다면 :을 이용해 한번에 출력할 수도 있습니다.\n\nstarwars |> select(hair_color:eye_color)\n\n\n\n  \n\n\n\n또한 select() 안에서 열 이름이 갖는 규칙을 활용하여, 특정 규칙만을 갖는 열만 선택할 수 있습니다.\n\nstarts_with() : 특정한 값으로 시작하는 열 이름 찾기\n\nstarwars |> select(starts_with('hair'))\n\n\n\n  \n\n\n\nends_with() : 특정한 값으로 끝나는 열 이름 찾기\n\nstarwars |> select(ends_with('hair'))\n\n\n\n  \n\n\n\nmatches() : 정규 표현식을 통한 열 이름 찾기\n\nstarwars |> select(matches('^[a-z]{4}_'))\n\n\n\n  \n\n\n\ncontains() :특정한 문자가 포함된 열 이름 찾기\n\nstarwars |> select(contains('_'))\n\n\n\n  \n\n\n\nnum_range() : 특정 숫자범위를 갖는 열 이름 찾기\nnum_range() 함수를 설명하기 위해 tidyr 패키지에 있는 billboard 데이터를 사용하겠습니다.\n\nlibrary(tidyr)\nbillboard |> select(num_range('wk',1:10))\n\n\n\n  \n\n\n\n\n\n\n7. rename(): 열 이름 변경\nselect()문을 이용해 원하는 이름으로 열을 출력할 수도 있습니다.\n\nstarwars |> select(home_world=homeworld)\n\n\n\n  \n\n\n\n그러나 select()는 선택된 열 이외에 다른 열들은 모두 버리기 때문에, rename()을 이용해 열 이름을 변경해줄 수 있습니다.\n\n# rename(new = old)\nstarwars |> rename(home_world = homeworld)\n\n\n\n  \n\n\n\n\n\n8. mutate(): 새로운 열 추가하기\n\nstarwars |> mutate(height_m = height / 100)\n\n\n\n  \n\n\n\ntibble 특성 상 새롭게 만든 열이 바로 보이지 않지만, select()를 이용해 출력할 수 있습니다.\n\nstarwars |>\n  mutate(height_m = height / 100) %>%\n  select(height_m, height, everything())\n\n\n\n  \n\n\n\nmutate() 안에서 새롭게 만든 열도 접근이 가능합니다.\n\nstarwars |> \n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %>%\n  select(BMI, everything())\n\n\n\n  \n\n\n\n만약 기존의 열은 없애고 새로운 열만 남기고 싶다면, transmute() 를 이용할 수 있습니다.\n\nstarwars |>\n  transmute(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  )\n\n\n\n  \n\n\n\n\n\n9. relocate(): 순서 변경\nrelocate()는 옮길 열들과, 옮길 위치를 지정해주면 됩니다.\n\n# .before or .after\nstarwars |> relocate(sex:homeworld, # 옮길 열들\n                    .before = height # 옮길 위치\n                    )\n\n\n\n  \n\n\n\n\n\n10. summarise(), summarize(): 요약\nsummarise() 는 하나의 열에 대해 평균, 표준편차, 중앙값 등으로 요약하기 위해 함께 사용되는 함수입니다.\n\nstarwars |> summarise(height = mean(height, na.rm = TRUE))\n\n\n\n  \n\n\n\ndplyr와 %>%(또는 |>)를 사용하여 출력했을 때, 출력된 값이 저장되지 않습니다. 즉 분석하는 데이터에는 변경사항이 없습니다. 만약 새로운 열을 추가하거나 이름을 변경할 때, 또는 계산한 값을 저장하기 위해서는 <- 를 이용해 데이터에 저장하거나 새로운 값으로 선언해야 합니다.\n\nstarwars2 <- starwars %>%\n  group_by(species, sex) %>%\n  select(height, mass) %>%\n  summarise(\n    height = mean(height, na.rm = TRUE),\n    mass = mean(mass, na.rm = TRUE)\n  )\n\n# starwars2에는 group_by 후 summarise한 값만 존재\nstarwars2\n\n\n\n  \n\n\n\nReference\n\nhttps://dplyr.tidyverse.org/articles/dplyr.html"
  },
  {
    "objectID": "blog/posts/mimic-iv-setting/index.html",
    "href": "blog/posts/mimic-iv-setting/index.html",
    "title": "MIMIC-IV 분석 준비",
    "section": "",
    "text": "MIMIC-IV (Medical Information Mart for Intensive Care IV)는 매사추세츠주 보스턴에 위치한 베스 이스라엘 디콘네스 메디컬 센터의 중환자실(ICU)에 입원한 환자들의 개인 식별 정보가 제거된 전자 건강 기록(EHR) 공개 데이터셋입니다. 2008년부터 2019년까지의 기간 동안 수집된 자료를 포함하며, MIMIC 데이터셋의 네 번째 버전입니다.\nMIMIC-IV 데이터셋은 생체 신호, 검사 결과, 약물, 진단 및 수술 등 포괄적인 임상 자료뿐만 아니라 인구통계학적 정보, 입원 및 퇴원 세부 정보, 기타 임상 문서도 포함합니다. 이 데이터셋은 380,000명 이상의 고유 환자에 대한 2,800만 건 이상의 중환자실 입원 정보를 포함하고 있어, 세계에서 가장 큰 공개적으로 이용 가능한 중환자실 데이터셋 중 하나입니다.\nMIMIC-IV 데이터셋은 47개 테이블과 7,000개가 넘는 변수로 구성된 관계형 데이터베이스 형식으로 구성되어 있습니다. 또한, 사망률, 패혈증, 재입원 등과 같은 유도 데이터셋도 포함되어 있으며, 이러한 데이터셋은 특정 연구 질문에 대한 사전 처리 데이터를 제공합니다. 또한 MIMIC-IV에는 개인 식별 정보가 제거된 자유 형식의 임상 노트도 포함되어 있습니다.\nMIMIC-IV는 패혈증, 급성 신손상, 사망률 예측 및 자연어 처리 등 다양한 의료 연구 분야에서 사용되고 있습니다. 이러한 데이터셋의 이용 가능성은 임상 의사 결정 지원, 환자 분류 및 맞춤형 의학에 대한 새로운 알고리즘, 모델 및 방법을 개발하고 시험하는 데 필수적인 기반을 제공합니다.\nMIMIC-IV 데이터셋에 대한 접근은 데이터 사용 계약서 작성 및 기관 검토 위원회(IRB) 승인을 받아야 합니다. 그러나 이 데이터셋은 전 세계 연구원들이 PhysioNet 저장소를 통해 무료로 접근할 수 있으며, 이 저장소는 데이터셋을 활용한 연구를 위한 튜토리얼, 코드 및 기타 자원을 제"
  },
  {
    "objectID": "blog/posts/mimic-iv-setting/index.html#데이터-다운로드",
    "href": "blog/posts/mimic-iv-setting/index.html#데이터-다운로드",
    "title": "MIMIC-IV 분석 준비",
    "section": "1. 데이터 다운로드",
    "text": "1. 데이터 다운로드\nphysionet에서 MIMIC-IV 데이터 다운로드하기"
  },
  {
    "objectID": "blog/posts/mimic-iv-setting/index.html#db에-데이터-저장하기",
    "href": "blog/posts/mimic-iv-setting/index.html#db에-데이터-저장하기",
    "title": "MIMIC-IV 분석 준비",
    "section": "2. DB에 데이터 저장하기",
    "text": "2. DB에 데이터 저장하기\n원하는 DB를 선택하여 MIMIC-IV 데이터를 저장할 수 있습니다.\nDB가 PC에 설치되어 있어야 합니다.\nPostgreSQL, mySQL, 구글 bigquery, duckdb, sqlite 중에서 선택 가능합니다.\n저는 PostgreSQL 으로 진행하겠습니다.\ngithub에서 mimic-code/mimic-iv/buildmimic/postgres/\n에 들어가면 진행 가능.\n\n# clone repo\ngit clone https://github.com/MIT-LCP/mimic-code.git\ncd mimic-code\n# download data\nwget -r -N -c -np --user <USERNAME> --ask-password https://physionet.org/files/mimiciv/2.0/\nmv physionet.org/files/mimiciv mimiciv && rmdir physionet.org/files && rm physionet.org/robots.txt && rmdir physionet.org\ncreatedb mimiciv\npsql -d mimiciv -f mimic-iv/buildmimic/postgres/create.sql\npsql -d mimiciv -v ON_ERROR_STOP=1 -v mimic_data_dir=mimiciv/2.0 -f mimic-iv/buildmimic/postgres/load_gz.sql\npsql -d mimiciv -v ON_ERROR_STOP=1 -v mimic_data_dir=mimiciv/2.0 -f mimic-iv/buildmimic/postgres/constraint.sql\npsql -d mimiciv -v ON_ERROR_STOP=1 -v mimic_data_dir=mimiciv/2.0 -f mimic-iv/buildmimic/postgres/index.sql"
  },
  {
    "objectID": "blog/posts/mimic-iv-setting/index.html#참고자료",
    "href": "blog/posts/mimic-iv-setting/index.html#참고자료",
    "title": "MIMIC-IV 분석 준비",
    "section": "참고자료",
    "text": "참고자료\nhttps://physionet.org/content/mimic-iv-demo/2.2/\nhttps://github.com/MIT-LCP/mimic-code"
  },
  {
    "objectID": "blog/posts/regular expression/index.html",
    "href": "blog/posts/regular expression/index.html",
    "title": "stringr을 이용한 문자 추출하기",
    "section": "",
    "text": "아래와 같은 문자열 데이터가 있다고 가정해봅시다.\n\n\n [1] \"                         location         age year  sales\"\n [2] \"1          South-East Asia Region   <20 years 2002 0.01\"  \n [3] \"2      Western Sub-Saharan Africa 20-24 years 2010 0.04\"  \n [4] \"3      Commonwealth Middle Income 40-44 years 2003 0.18\"  \n [5] \"4                  Eastern Europe 45-49 years 2008 0.37\"  \n [6] \"5  World Bank Lower Middle Income   <20 years 2005 0.01\"  \n [7] \"6                         Oceania 45-49 years 2006 0.26\"  \n [8] \"7      Commonwealth Middle Income 55-59 years 2004 0.30\"  \n [9] \"8      Western Sub-Saharan Africa 30-34 years 1997 0.04\"  \n[10] \"9        High-income Asia Pacific 65-74 years 2000 0.24\"  \n\n\n비록 문자 형태의 벡터이지만, 열 단위로 묶인 데이터프레임의 형태를 띄고 있습니다.\n저 문자열 속에서 location, age, year, sales 를 나눠 데이터프레임으로 만들어주려면 (일일이 복붙하면서 데이터프레임으로 선언하는 노가다를 해야 합니다) 정규표현식(Regular expression)을 사용해야 합니다.\n정규표현식 규칙은 크게 다음과 같습니다.\n\n숫자: \\\\d 또는 [:digit:]\n문자: [:alpha:]\n숫자 또는 문자: [:alnum:]\n0개 이상: *\n한 개 이상: +\n시작하는 단어: ^\n끝나는 단어: $\n구두점 등의 특수문자: [:punct:] 또는 [:symbol:] 또는 \\\\특수기호\nR에서 특수문자 정규표현식을 찾을 때, 사용될 특수문자에 따라 [:punct:] 또는 [:symbol:]을 사용해야 합니다.\n\nlibrary(stringi)\nascii <- stri_enc_fromutf32(1:127)\nmessage(\"Punct: \", stri_extract_all_regex(ascii, \"[[:punct:]]\")[[1]])\nmessage(\"Symbol: \", stri_extract_all_regex(ascii, \"[[:symbol:]]\")[[1]])\n\n\n여러 가지 규칙을 한 번에 찾을 때, 규칙이 긴 것부터 찾는다. 다시 말해 찾고자 하는 단어가 긴 단어의 규칙을 먼저 입력해야 한다.\n여러 개의 패턴을 적용할 때 | 을 단위로 정규표현식을 찾아줄 수가 있습니다. paste() 또는 paste0() 함수의 collapse를 이용하여 여러 패턴을 붙여줄 수 있습니다.\n자 이제 처음에 보여드렸던 문자 데이터에서 각각의 데이터를 추출하여 데이터프레임으로 만들어보겠습니다. 가장 먼저 location 입니다.\n지역이 굉장히 다양하고, 또 단어마다 규칙들이 많아 찾기가 어려운 규칙입니다.\n\nlibrary(stringr)\nf <- \"[:alpha:]+ [:alpha:]+ [:alpha:]+ [:alpha:]+ [:alpha:]+\"\ng <- \"[:alpha:]+-[:alpha:]+ [:alpha:]+ - [:alpha:]+\"\nd <- \"[:alpha:]+-[:alpha:]+ [:alpha:]+ [:alpha:]+\"\nh <- \"[:alpha:]+ [:alpha:]+-[:alpha:]+ [:alpha:]+\"\na <-  \"[:alpha:]+ [:alpha:]+ [:alpha:]+\"\nc <- \"[:alpha:]+ [:alpha:]+-[:alpha:]+\"\ne <- \"[:alpha:]+ [:alpha:]+\"\nb <-  \"[:alpha:]+\"\nptrn_loc <- paste0(c(f,g,d,h,a,c,e,b),collapse = \"|\")\n\nloc <- str_extract(\n  string = temp[2:length(temp)],\n  pattern = ptrn_loc)\n\nhead(loc)\n\n[1] \"South-East Asia Region\"         \"Western Sub-Saharan Africa\"    \n[3] \"Commonwealth Middle Income\"     \"Eastern Europe\"                \n[5] \"World Bank Lower Middle Income\" \"Oceania\"                       \n\n\n다음은 age를 찾아보겠습니다. age를 살펴보면, 네 가지 정도로 분류할 수 있습니다. 50-54 형태, 80+ 형태 <20 형태, 그리고 All ages가 있죠.\n\naa <- \"[:digit:]{2}-[:digit:]{2} years\"\nbb <- \"\\\\<[:digit:]{2}+ years\"\ncc <- \"[:digit:]{2}\\\\+ years\"\ndd <- \"All ages\" # all ages\nptrn_age <- paste0(c(aa,bb,cc,dd),collapse = \"|\")\nages <- str_extract(\n  string = temp[2:length(temp)],\n  pattern = ptrn_age\n)\n\nhead(ages)\n\n[1] \"<20 years\"   \"20-24 years\" \"40-44 years\" \"45-49 years\" \"<20 years\"  \n[6] \"45-49 years\"\n\n\n나머지 year와 sales는 앞의 두 가지보다 간단합니다.\n\nyear <- str_extract(\n  string = temp[2:length(temp)],\n  pattern = \"[:digit:]{4}\"\n) |> as.numeric()\n\nhead(year)\n\n[1] 2002 2010 2003 2008 2005 2006\n\nsales <- str_extract(\n  string = temp[2:length(temp)],\n  pattern = \"[:digit:]{1}\\\\.[:digit:]{2}\"\n) |> as.numeric()\n\nhead(sales)\n\n[1] 0.01 0.04 0.18 0.37 0.01 0.26\n\n\n이제 위에서 추출한 loc, ages, year, sales를 하나의 데이터프레임으로 만들어주면 끝입니다.\n\ndf <- data.frame(\n  location = loc,\n  age = ages,\n  year = year,\n  sales = sales\n)\n\nhead(df)\n\n\n\n  \n\n\n\n\n레퍼런스\n\nhttps://continuous-development.tistory.com/33\nhttps://stackoverflow.com/questions/26348643/r-regex-with-stringi-icu-why-is-a-considered-a-non-punct-character"
  },
  {
    "objectID": "blog/posts/ggplot_axis조정/index.html",
    "href": "blog/posts/ggplot_axis조정/index.html",
    "title": "ggplot 세부 조정: 축 조정",
    "section": "",
    "text": "1) x축 tick 간격 조정\nggplot()을 이용해 그래프를 그리면 일반적으로 다음과 같이 출력됩니다.\n\nlibrary(ggplot2)\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_col()\n\n\n\n\nx축의 tick이 막대그래프의 중앙에 위치해있습니다. tick이 막대의 가운데가 아니라, 막대 양 옆에 위치하여 막대 아래에는 Species의 이름만 나타나게 하려면 다음과 같이 실행할 수 있습니다.\n\nlibrary(data.table)\niris_dt <- as.data.table(iris)\niris_dt$Species2 <- as.integer(as.factor(iris_dt$Species))\niris_dt\n\n\n\n  \n\n\nx_ticks <- c(0, unique(iris_dt$Species2))+0.5\nx_ticks\n\n[1] 0.5 1.5 2.5 3.5\n\nlen <- length(x_ticks)\n\nggplot(iris_dt, aes(x = Species2, y = Sepal.Length)) +\n  geom_col() +\n  scale_x_continuous(\n    breaks= c(unique(iris_dt$Species2),x_ticks),\n    labels = c(unique(as.character(iris_dt$Species)), rep(\"\",len))\n  ) +\n  theme(\n    axis.ticks.x = element_line(color=c(rep(NA,len-1), rep('black',len)))\n  )\n\n\n\n\n\n\n2) y축 간격 없애기\n원래 그래프를 보면 y축의 0과 x축이 떨어져 있는 것을 알 수 있습니다.\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_col()\n\n\n\n\n이제 y축의 범위를 아래와 같이 지정해주겠습니다.\n\nggplot(iris, aes(x = Species, y = Sepal.Length)) +\n  geom_col() +\n  scale_y_continuous(expand=c(0,0))\n\n\n\n\n이처럼 expand를 통해 막대그래프가 x축에 붙어있을 수 있도록 출력할 수 있습니다."
  },
  {
    "objectID": "blog/posts/db in shiny/index.html",
    "href": "blog/posts/db in shiny/index.html",
    "title": "R Shiny에서 DB 다루기",
    "section": "",
    "text": "# 데이터 읽기\ndbData <- reactive({\n    dbTrigger$depend()\n      as.data.table(dbGetQuery(con, 'SELECT id, dates, cat_big, type, cat_small,\n                                      weight, no_rep, no_set,\n                                      weight * no_rep * no_set as volume, memo\n                               FROM diary'))\n  })\n\n\n# 데이터 db에 입력\nobserveEvent(input$write,{\n    record <- list(\n      dates = input$dates,\n      cat_big = input$cat_big,\n      type = input$type,\n      cat_small = input$cat_small,\n      weight = input$weight,\n      no_rep = input$no_rep,\n      no_set = input$no_set,\n      memo = input$memo\n    )\n    if(input$cat_small==\"\" |\n       input$no_rep==0 |\n       input$no_set==0){\n      shinyWidgets::show_alert(\n        type = 'error',\n        title='Oops!!',\n        text = \"운동 일지 빈칸이 존재함.\")\n    }\n    else {\n      query <- sqlInterpolate(con,\n                     'INSERT INTO diary ([dates],[cat_big],[type],[cat_small],[weight],[no_set],[no_rep],[memo])\n                     VALUES (?dates, ?cat_big, ?type, ?cat_small, ?weight, ?no_set, ?no_rep, ?memo);',\n                     dates = input$dates,\n                     cat_big = input$cat_big,\n                     type = input$type,\n                     cat_small = input$cat_small,\n                     weight = input$weight,\n                     no_rep = input$no_rep,\n                     no_set = input$no_set,\n                     memo = input$memo\n                     )\n      dbExecute(con, query)\n      dbTrigger$trigger()\n      shinyWidgets::show_alert(\n        type='success',\n        title='Success !!',\n        text = \"운동일지 저장 완료.\")\n      Sys.sleep(2)\n      # session$reload()\n  }\n})\n\n\n  observeEvent(input$records_cell_edit, {\n    row  <- input$records_cell_edit$row; print(row)\n    column <- input$records_cell_edit$col; print(column)\n    value <- input$records_cell_edit$value; print(value)\n    target_id <-  dt()[row, id] # 변경id\n\n    edited_col_name <- names(dt())[column+1]\n    dt()[row, col := input$records_cell_edit$value,\n                       env= list(col = edited_col_name)]\n\n    query <- dbSendQuery(con,\n                         paste0(\"UPDATE diary SET '\",edited_col_name,\"' = ? where id = ?\"),\n                            params=c(value, target_id)\n    )\n    DBI::dbClearResult(query)\n    dbTrigger$trigger()\n  })\n\n    query <- dbSendQuery(con,\n                         paste0(\"UPDATE diary SET '\",edited_col_name,\"' = ? where id = ?\"),\n                            params=c(value, target_id)\n    )\n    DBI::dbClearResult(query)\n    dbTrigger$trigger()\n  })"
  },
  {
    "objectID": "blog/posts/2023-01-19-quarto-intro/index.html",
    "href": "blog/posts/2023-01-19-quarto-intro/index.html",
    "title": "Quarto 사용법",
    "section": "",
    "text": "Quarto는 Rmarkdown을 계승하는 차세대 문서양식입니다.\n이미 아실지 모르겠지만, RStudio는 R 뿐만 아니라 Python까지 지원하고 있습니다 (reticulate란 패키지를 통해).\n다만 Rstudio 라는 이름 때문인지, Python 유저들은 jupyter를 주로 사용했던 것 같습니다.\n\n아무튼 2022년 12월 이후, RStudio사는 POSIT으로 사명을 변경하였습니다. 이는 RStudio가 R 사용자만을 위한 통합개발환경 (Integrated Development Environment, IDE)에서 벗어나 R, Python, Julia 등을 사용하는 모든 데이터 사이언티스들을 위한 IDE를 추구한다는 의미라고 볼 수 있습니다.\n\nQuarto 역시 마찬가지입니다. 전신이었던 Rmarkdown이 갖는 이름에서 벗어나 Quarto라는 새로운 이름을 통해 R과 Python 유저 모두 편리하게 사용할 수 있는 document 양식을 지향합니다.\n실제로 Quarto는 기존의 Rmarkdown 사용자를 위한 knitr 뿐만 아니라 python의 jupyter notebook 사용자들을 위한 jupyter 모드도 사용이 가능합니다.\n더 자세한 내용은 Quarto 홈페이지를 참고하세요!"
  },
  {
    "objectID": "blog/posts/2023-01-19-quarto-intro/index.html#quarto-준비하기",
    "href": "blog/posts/2023-01-19-quarto-intro/index.html#quarto-준비하기",
    "title": "Quarto 사용법",
    "section": "Quarto 준비하기",
    "text": "Quarto 준비하기\nQuarto를 사용하기 위해서는 Quarto 패키지를 설치해야 합니다.\n\n\n# not run\ninstall.packages('quarto')\n\nQuarto 설치가 완료되었다면 RStudio의 좌측 상단에서 Quarto document를 생성할 수 있습니다.\n\n\nQuarto document는 .qmd 확장자로 끝나는 파일입니다.\n.qmd 파일은 Source 에디터, Visual 에디터로 작업을 수행할 수 있습니다. 특히 Visual 에디터는 다양한 markdown 문법들을 단축키들을 활용해 원하는 문서를 쉽게 만들 수 있다는 장점이 있습니다."
  },
  {
    "objectID": "blog/posts/2023-01-19-quarto-intro/index.html#quarto의-구조",
    "href": "blog/posts/2023-01-19-quarto-intro/index.html#quarto의-구조",
    "title": "Quarto 사용법",
    "section": "Quarto의 구조",
    "text": "Quarto의 구조\nQuarto는 크게 3가지의 부분으로 구성됩니다.\n\n1) YAML\nYAML은 YAML Ain’t Markup Language의 이름을 갖고 있는 언어로, 가독성에 초점을 두고 개발되었습니다.\nQuarto의 시작 부분에 문서 형식을 설정하는 용도로 yaml문법을 이용합니다.\n\n대표적으로 설정해줄 수 있는 옵션은 다음과 같습니다.\n\ntoc: markdown의 목차를 한눈에 볼 수 있게 색인을 생성합니다.\ntoc-location: toc의 위치를 설정합니다. left, right\ncode-fold: r 코드 부분을 접거나 펼 수 있습니다.\ntheme: Quarto의 문서 테마를 지정할 수 있습니다. 참고\n\n그 외 설명은 quarto 공식 홈페이지를 참고하세요.\n\n\n2) R code\nr chunk를 만드는 방법은 (백틱``)을 3번 입력하거나, 단축키(ctrl+shift+i)를 이용하면 됩니다.\n\n\n\n3) markdown\n간단한 markdown 문법을 소개합니다. quarto의 visual 에디터를 이용할 때의 단축키도 첨부합니다.\n\nHeader\n제목과 같이 큰 글자는 #의 개수에 따라 달라집니다.\n\n\n글자 옵션\nquarto에서 사용할 수 있는 텍스트 옵션들은 다음과 같습니다.\n\n굵게: Asterisk (ctrl + b)\n기울이기: Asterisk (ctrl + i)\n밑줄:  Asterisk  (ctrl + u)\nsuperscript: R2 (^텍스트^)\nsubscript: HRmax - HRmin (~텍스트~)\nstrike through : 밑줄찍찍 (~~텍스트~~)\nverbatim code: Quarto (ctrl + d)\n글자 색상 변경: special css 문법 활용하기.\n링크 삽입: 네이버\n이미지 삽입: \nblockquote : 문장 앞에 >를 붙여줍니다.\n\n\n인용구 등을 표시할 때 blockquote를 많이 사용합니다.\n\n\n\nList 출력\n\n1) Unordered list (순서없는 리스트)\n\nItem 1\n\nitem 1-1\nitem 1-2\n\nitem 1-2-1\nitem 1-2-2\n\n\n\n\n\n2) Ordered list (순서가 지정된 리스트)\n\nitem 1\n\nitem 1-1\nitem 1-2\n\nsub-item 1\n\nitem 1\n\n\n\n\n자세한 markdown syntax는 Quarto 홈페이지를 참고하세요."
  },
  {
    "objectID": "blog/posts/2023-01-19-quarto-intro/index.html#quarto-실행하기",
    "href": "blog/posts/2023-01-19-quarto-intro/index.html#quarto-실행하기",
    "title": "Quarto 사용법",
    "section": "Quarto 실행하기",
    "text": "Quarto 실행하기\nRender 버튼(shortcut: ctrl + shift + k)을 누르시면 html 문서가 생성됩니다.\n\n1 + 1\n\n[1] 2\n\n\nterminal 창의 명령어를 통해서도 실행 가능합니다. 해당 파일의 경로에서 아래의 명령어를 실행합니다.\n\nquarto render test.qmd --to html\n\n\n# | echo: true\n#| eval: false\n2 * 2\n\n[1] 4\n\n\ncode 출력과 관련된 옵션은 다음과 같습니다.\n\necho: false 는 입력한 코드가 출력되지 않게 합니다.\neval: false는 입력한 코드의 결과물이 출력되지 않게 합니다.\nwarning: false는 코드 실행 시 나타나는 경고 메시지를 출력하지 않게 합니다.\n\n\n\n```{r}\n#| warning: false\n#| message: false\n#| output-location: column\n#| label: fig-airquality\n#| fig-cap: Teup and ozone level\nlibrary(ggplot2)\nggplot(airquality, aes(Temp, Ozone))+\n  geom_point()+\n  geom_smooth()\n```\n\n\n\n\nFigure 1: Teup and ozone level\n\n\n\n\n\n\nCallout\n\n\n\n\n\n\nNote\n\n\n\ncallout에는 5 종류가 있습니다.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning callout 입니다.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant callout 입니다.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip callout입니다.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nCaution callout 입니다.\n\n\n\n\nTabsets\nmarkdown에서는 tab을 나누어 각각 화면을 출력할 수 있습니다.\n\nSplit up and flip between sections\n\nCodeOutput\n\n\n\n```{r}\n#| eval: false\nhead(mtcars)\n```\n\n\n\n\nhead(mtcars)\n\n\n\n  \n\n\n\n\n\n\n\n\n그래프 출력하기\n\n# #|을 통해 r chunk의 옵션을 변경할 수 있습니다.\n# layout을 통해 figure의 위치를 조절할 수 있습니다.\n\nrequire(dplyr)\nrequire(ggplot2)\ncars |>\n  ggplot(aes(speed, dist)) +\n  geom_point()\nmtcars |>\n  ggplot(aes(disp, mpg))+\n  geom_point()\n\n\n\n\n\n\nSpeed and Stopping Distances of Cars\n\n\n\n\n \n\n\n\n\n\nEngine displacement and fuel efficiency in cars\n\n\n\n\n\n\n\n\n\nReference\n\nhttps://quarto.org/docs/visual-editor/\nhttps://quarto.org/"
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html",
    "href": "blog/posts/data.table_basic/index.html",
    "title": "data.table 기초 문법",
    "section": "",
    "text": "data.table은 R의 data.frame을 상속하여 만들어진 패키지입니다. 그렇기 때문에 대괄호([])를 쓰는 등의 문법은 대체로 data.frame과 비슷합니다. 그러나 data.table은 기존의 data.frame과 다른 장점을 갖고 있습니다.\ndata.table의 장점은 다음과 같습니다.\n\n매우 빠른 속도\ndata.table은 기본 data.frame 구조보다 훨씬 빠르게 데이터를 연산합니다.\n\nrequire(data.table)\nrequire(dplyr)\nrequire(microbenchmark)\ndata('storms')\ndf_test <- function(){\n  aggregate(x=storms[,c('wind','pressure')],\n            by = list(storms$name,storms$year, storms$month,storms$day),\n            FUN = mean)\n}\n\ndplyr_test <- function(){\n  storms %>% \n    group_by(name, year, month, day) %>% \n    summarise(wind=mean(wind), pressure=mean(pressure),\n              )\n}\nstorm_dt <- as.data.table(storms)\ndt_test <- function(){\n  storm_dt[,.(wind=mean(wind), pressure=mean(pressure)), by=.(name, year,month,day)]\n}\n\nmicrobenchmark(df_test(), dplyr_test(), dt_test(), times=10)\n\n\n\n  \n\n\n\n위는 각각 data.frame, dplyr, data.table로 동일한 결과를 불러오도록 실행했을 때 걸린 시간입니다. data.table의 결과가 최소 10배는 더욱 빠른 것을 알 수 있습니다.\n효율적인 메모리 처리\ndata.table은 다른 데이터 패키지보다 효율적으로 데이터연산을 처리합니다. 그렇기 때문에 메모리 사용에 있어서도 더 적은 양으로 더 빠르게 계산을 진행합니다.\n낮은 패키지 의존성\n패키지 의존성이라는 것은 특정 패키지를 불러오기 위해 또다른 패키지를 불러오는 것입니다. 우리가 배울 data.table은 R 사용자들에게 자주 활용되는 tidyverse 계열의 dplyr패키지보다 의존성이 훨씬 낮습니다. 그렇기 때문에 번거롭게 하나의 패키지를 사용하기 위해 다른 패키지들을 설치해줄 필요가 없습니다.\n\n\n\ndplyr vs data.table 패키지 의존성 비교"
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html#data.table-함수",
    "href": "blog/posts/data.table_basic/index.html#data.table-함수",
    "title": "data.table 기초 문법",
    "section": "2. data.table 함수",
    "text": "2. data.table 함수\ndata.table 패키지에는 data.table에서 사용할 수 있는 고유의 함수들이 존재합니다. 여기서는 우선 앞으로 사용할 데이터를 불러오고 설정하기 위한 함수들을 우선 배워봅시다.\n\nfread(): f(ast) + read의 의미입니다. 말그대로 빠르게 데이터를 불러오는 것(read)을 의미합니다. .csv, .txt 등의 확장자 이름을 가진 파일들을 불러올 수 있습니다.\n\n\nrequire(NHANES)\ndt <- as.data.table(NHANES)\n\nfread()를 통해 불러온 파일의 class는 data.table입니다.\n\nclass(dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\ndata.table을 출력했을 때, data.frame과 다른 점은 크게 두 가지가 있습니다.\ndata.table은 우선 모든 column에 대해 첫 5개, 마지막 5개의 행을 출력합니다.\n또한 행의 번호에 :가 붙어 출력됩니다.\n\nfwrite(): f(ast) + write 입니다. 말그대로 빠르게 데이터를 저장(write)합니다. .csv, .txt 등의 확장자 파일로 저장할 수 있습니다.\n\n\n\nsetnames(): column 이름을 사용자가 알아보기 쉽게끔 변경해야 할 때가 있습니다. setnames()는 column의 이름을 변경하는 함수입니다. column의 이름을 하나만 바꾸고 싶은 경우에는 문자열 하나만 넣어주면 되고, 여러 개의 column 이름을 동시에 변경할 때는 문자 벡터를 넣어주면 됩니다.\n\n\n# 하나의 column 이름을 변경할 때\nsetnames(dt, old='Gender', new='gender')\n\n# 여러 개의 column 이름을 동시에 변경할 때\nsetnames(dt,\n         old = c('Age','Race1','Education'),# 바꿔줄 기존의 column 이름\n         new = c('age','race','education') # 새로운 column 이름\n         )\n\n\ndata.frame 등을 data.table로 변경하기\n새롭게 파일을 불러오는 것 뿐만 아니라 기존의 data.frame을 data.table 형태로 변경해줄 수 있습니다.\nsetDT() : 영구적으로 data.table 형태로 저장합니다. 저장된 값을 따로 출력하지는 않습니다.\n\nsetDT()\n\nas.data.table() : 일시적으로 data.table 형태로 출력합니다. 출력된 데이터가 저장되지는 않습니다.\n\nrequire(data.table)\nhead(as.data.table(iris))"
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html#i-data.table-행",
    "href": "blog/posts/data.table_basic/index.html#i-data.table-행",
    "title": "data.table 기초 문법",
    "section": "3. i: data.table 행",
    "text": "3. i: data.table 행\ndata.table에서 행(row)을 다루는 부분을 i 라고 부릅니다 (왜 i가 된건지는 모르겠습니다).\ndata.table에서 행을 다룬다는 것은 특정한 조건을 만족하는 행들을 추출 또는 선택(filtering)한다는 것과 같습니다.\n자, 이제 data.table에서 행을 다루는 방법을 살펴보겠습니다.\n\n1) 논리 연산자를 이용한 row filtering\ndata.frame 의 행을 다루는 부분에서도 배웠지만, 기본적으로 i에서는 논리 연산자를 이용해 행을 선택합니다. 논리 연산자의 조건을 만족하는 행들, 즉 논리 연산자의 실행 결과가 TRUE 인 행들만 추출하는 것입니다.\n\ndt[age>=30] |> head()\n\n\n\n  \n\n\n\n\ndt[gender=='male' & age>=45] |> head() # 남성이고 45세 이상\n\n\n\n  \n\n\n\n\ndt[gender=='female' | age>=50] |> head() #여성 또는 50세 이상\n\n\n\n  \n\n\n\n이처럼 &나 |를 붙여주면, 여러 조건을 사용하여 원하는 행들을 추출할 수 있습니다.\n\n\n\n\n\n\n행을 다룰 때 data.table과 data.frame의 차이점\n\n\n\ndata.frame과 달리 data.table 문법에서는, 대괄호 안에서 column을 df$var 양식으로 사용하지 않아도 됩니다. 그냥 column의 이름만 사용하면 됩니다.\n또한 data.frame에서는 행 부분을 입력할 때, 꼭 , 를 붙여줘야 했습니다. 그러나 data.table의 경우 행만 filtering 할 때, 굳이 ,을 붙여줄 필요가 없습니다.\n어떤가요? data.table이 훨씬 더 간단하죠? 😃\n\n\n\n\n2) Infix 연산자를 이용한 row filtering\ndata.table에서는 논리 연산자 뿐만 아니라 infix 연산자를 이용하여, 조건을 충족시키는 행을 선택할 수 있습니다. 파이프 연산자 역시 조건을 만족하는 경우인 TRUE 에 해당하는 값들만 선택합니다.\n\n\n\n\n\n\nNote\n\n\n\ninfix 연산자는 함수 피연산자의 양쪽에 있는 인수에 계산을 적용하는 기능을 제공합니다.\n\n\n\nA %in% B: A가 B 안에 있는지 확인합니다. 이 때 B는 vector가 옵니다. 문자를 확인하는 경우에는 %chin% 을 이용해 더욱 빠르게 계산할 수 있습니다.\n\ndt[race %in% c('Black','White')] |> head()\n\n\n\n  \n\n\n\nA %like% B: A가 B와 비슷한지 확인합니다. 이 때 B에는 보통 문자열(character)이 옵니다.\n\ndt[MaritalStatus %like% 'Married'] |> head()\n\n\n\n  \n\n\n\nA %between% B: A가 B 사이에 있는지 확인합니다. 이 때 B는 c(0,10)과 같은 범위로 지정합니다.\n\ndt[BMI %between% c(20,25)] |> head()\n\n\n\n  \n\n\n\n\n\n\n\n\n\nTip\n\n\n\n위에서 소개해드린 %%을 이용한 infix 연산자 말고도 data.table 패키지 내의 함수를 이용할 수도 있습니다.\n%between%은 between()\n%like% 는 like()\n\n# 예시\ndt[like(race,'ite')]\n\n\n\n  \n\n\n\n\n\n\n\n\n3) 함수를 이용한 row filtering\n논리 연산자나 infix 연산자 뿐만 아니라 TRUE/FALSE를 반환하는 다른 함수들도 행을 선택하는 데 활용할 수 있습니다. 가장 대표적인 함수가 바로 is.na() 입니다.\n\ndt[is.na(Testosterone)] |> head()\n\n\n\n  \n\n\n\nNA가 아닌 데이터를 출력하는 방법은 !is.na() 입니다. Chapter 2의 논리 연산자 부분에서 NOT을 의미하는 기호는 !라고 배웠습니다.\n\ndt[!is.na(SleepHrsNight)] |> head()\n\n\n\n  \n\n\n\n\n\n4) 행의 정렬\n행의 정렬 역시 i 부분에서 담당합니다. 특정한 column을 기준으로 데이터를 오름차순 또는 내림차순 정렬을 할 때 사용할 수 있습니다.\ndata.table에서 특정 column을 기준으로 데이터를 정렬하는 방법은 두 가지가 있습니다.\n\norder()\norder()는 정렬한 값의 출력만 합니다.\n\ndt[order(BMI)] |> head()\n\n\n\n  \n\n\n\n만약 내림차순으로 정렬하고 싶은 경우는 변수 앞에 -를 붙여주면 됩니다.\n\ndt[order(-BMI)] |> head()\n\n\n\n  \n\n\n\nsetorder()\nsetorder()는 데이터를 정렬하여 data.table에 저장합니다. order와는 다르게 정렬된 결과를 출력하지는 않습니다.\n\nsetorder(dt, # 정렬할 데이터\n         age # 기준이 되는 변수\n         )\n\n\n\n\n\n\n\n\nset이 들어가는 함수\n\n\n\ndata.table에서 set이 붙는 함수는 어떤 값을 출력없이 저장하는 함수입니다.\n\nsetnames() # column의 이름 변겅\nsetorder() # row 정렬\nsetDT() #  data.table로 저장"
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html#j-data.table-열",
    "href": "blog/posts/data.table_basic/index.html#j-data.table-열",
    "title": "data.table 기초 문법",
    "section": "4. j: data.table 열",
    "text": "4. j: data.table 열\nj 부분은 데이터의 열 (column)을 다루는 부분을 의미합니다. j 부분을 통해 원하는 열들을 선택하거나, 특정 열을 계산할 수 있습니다. 또한 새로운 변수를 생성하거나 기존의 변수를 수정 또는 삭제할 수 있습니다.\nj 를 활용하기 위해선 dt[,j] 처럼 앞에 ,를 항상 붙여줘야 합니다.\n\n1) column 선택\ndata.table에서 열을 선택하는 방법은 다양합니다.\n\ndt[,c('X','Y')]\ndt[,list(X,Y)]\ndt[,.(X,Y)]\n\n여기서 list()와 .()은 동일한 기능입니다. 동일한 기능이라면 더 적은 코드를 입력하는 후자가 더 낫겠죠?\n\ndt[,.(gender,age)] |> head()\n\n\n\n  \n\n\n\n문자 vector를 이용해서도 원하는 column을 선택할 수 있습니다. 이 때는 ..기호를 이용해야 합니다.\n\ntarget <- c('gender','age','race')\ndt[,..target] |> head()\n\n\n\n  \n\n\n\n\n\n2) column 계산\ncolumn을 계산하는 것 역시 data.table의 j 부분에서 담당합니다. 예를 들어 특정 column의 평균을 계산하거나 표준편차를 계산하는 경우가 있겠죠. 만약 하나의 column만 계산하는 경우 data.table의 [] 안에서 함수를 이용하면 됩니다.\n\ndt[,mean(BMI,na.rm=T)] |> head()\n\n[1] 26.66014\n\n\n만약 여러 개의 column에 대해 계산하는 경우 또는 계산하는 값을 data.table 형태로 출력하고 싶은 경우, column 선택에서 배웠던 .()를 활용하면 됩니다.\n\ndt[,.(mean_BMI = mean(BMI, na.rm=T),\n      sd_BMI = sd(BMI,na.rm=T))] |> head()\n\n\n\n  \n\n\n\n\n\n3) column 생성 및 변경 :=\ndata.table에는 특수한 기호가 있습니다. 바로 := 입니다. walrus (바다코끼리) 연산자라고도 불리기도 합니다. 바다코끼리의 어금니를 닮아서 붙여진 이름 같습니다.\n\n\n\n바다코끼리(Walrus)의 어금니가 := 와 닮았습니다.\n\n\ndata.table을 사용한다면 := 연산자를 활용할 줄 아는 것이 굉장히 중요합니다. 새로운 column을 추가하거나, 기존의 column을 변경할 때, 또는 column을 삭제할 때 활용되는 연산자 입니다.\n\na. column의 생성\n새로운 column을 생성하기 위해서는 dt[,column_name := value] 형식으로 코드를 작성합니다. :=의 왼쪽에는 새로운 column의 이름이, 오른쪽에는 데이터를 입력합니다.\n\ndt[,age_group := paste0((age %/% 10) * 10,'대')]\ndt[,table(age_group)] |> head()\n\nage_group\n 0대 10대 20대 30대 40대 50대 \n1391 1374 1356 1338 1398 1304 \n\n\nage 를 이용하여 age_group(연령대) column을 추가하였습니다.\n만약 여러 개의 column을 동시에 추가하고 싶다면, 백틱(``) 또는 따옴표('')과 함께 :=를 사용해야 합니다.\n이 때 ()안의 등호들은 :=가 아니라 그냥 등호(=)를 사용해야 합니다.\n\ndt[,`:=`(\n  age2 = age/5,\n  age3 = age/10\n)]\n\n\n\nb. column의 변경\ncolumn을 변경한다는 것은 기존의 column 값들을 다르게 바꿔준다는 것입니다. 예를 들면 1과 2로 이루어진 column을 ’no’와 ’yes’로 변경해주는 것처럼요.\ncolumn을 변경하는 것 역시 생성과 마찬가지로 dt[,column_name := value] 형식으로 코드를 작성하면 됩니다. column_name에는 바꿔줄 column의 이름이, value에는 새롭게 넣어줄 데이터를 입력합니다.\n\ndt[,gender := ifelse(gender=='male','m','f')]\n\n위의 코드처럼 :=를 이용해 male, female로 되어있는 Gender 를 각각 m과 f로 변경할 수 있습니다.\n한편 여러 개의 column을 생성하거나 변경하는 경우, 새로운 column이 생성되는 []안에서 그 column을 바로 사용할 수 없습니다.\n\n#|eval: false\ndt[,`:=`(\n  A2=age,\n  A3=A2\n)]\n\n# 에러메시지 발생\n# Error in eval(jsub, SDenv, parent.frame()) : object 'A2' not found\n\n위 코드는 A2와 A3 column을 동시에 생성하는 코드입니다. 그러나 A2가 아직 만들어지지 않았기 때문에, A3 column은 생성될 수 없습니다. 따라서,\n\ndt[,A2:=age][,A3:=A2][] |> head()\n\n\n\n  \n\n\n\n위의 코드처럼 []을 여러 번 붙여 순차적으로 column을 생성 또는 변경해주어야 합니다. 이를 chaining이라고 합니다.\n\n\nc. column의 삭제\n특정 데이터의 column을 삭제할 때는 dt[,column_name := NULL] 형식을 사용합니다. 삭제하려는 하는 column_name만 선택하면 되겠죠.\n\ndt[,column_name := NULL] # 삭제대상 열 이름 := NULL\n\n:= 연산자를 활용한 코드를 실행하게 되면 해당 데이터가 저장이 될 뿐, 출력이 되진 않습니다 (set함수와 같은 역할). 변경한 column을 확인하기 위해선 대괄호를 한 번 더 붙여줍니다.\n\ndt[,column_name := NULL][] |> head()"
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html#by-그룹-별-분석",
    "href": "blog/posts/data.table_basic/index.html#by-그룹-별-분석",
    "title": "data.table 기초 문법",
    "section": "5. by: 그룹 별 분석",
    "text": "5. by: 그룹 별 분석\nby: 특정 column에 따라 j 를 계산합니다. 예를 들어 성별에 따른 나이의 평균을 구하는 경우, 따라서 by를 사용하기 위해서는 j 부분이 존재해야 합니다.\n\ndt[,mean(age),by=MaritalStatus]\n\n\n\n  \n\n\n\nkeyby: by와 마찬가지로 특정 그룹에 따라 계산합니다. 하지만 by와 다르게 그룹으로 선택된 column을 기준으로 정렬하여 결과값을 출력합니다.\n\ndt[,mean(age),keyby=MaritalStatus]\n\n\n\n  \n\n\n\n혼인상태(MaritalStatus)에 따른 나이(age)의 평균 값이 혼인상태의 이름으로 오름차순 정렬이 되어 출력된 것을 확인할 수 있습니다."
  },
  {
    "objectID": "blog/posts/data.table_basic/index.html#이외-유용한-data.table-전용-함수",
    "href": "blog/posts/data.table_basic/index.html#이외-유용한-data.table-전용-함수",
    "title": "data.table 기초 문법",
    "section": "6. 이외 유용한 data.table 전용 함수",
    "text": "6. 이외 유용한 data.table 전용 함수\n\nfifelse(): fast ifelse입니다. ifelse()보다 더 빠르게 작업을 수행합니다.기본 ifelse()와 다르게 yes와 no 자리에 오는 인자들의 유형(type)이 반드시 동일해야 합니다.\n\ndt[,SEX := fifelse(gender=='m','남성','여성')]\ndt[,Temp := fifelse(age<10,NA,age)] # error 발생.\n\nfcase() : fast case when입니다. ifelse()처럼 조건을 사용해 데이터를 조작하지만, 반복적으로 ifelse() 함수를 계속해서 사용할 필요가 없기 때문에 더 적은 코드를 사용합니다.\n\ndt[,age_group2 := fcase(\n  age<20,'10대',\n  age<30,'20대',\n  age<40,'30대',\n  age<50,'40대',\n  age<60,'50대'\n)]\ndt[,table(age_group2, useNA = 'always')]\n\nage_group2\n10대 20대 30대 40대 50대 <NA> \n2765 1356 1338 1398 1304 1839 \n\n\n조건에 해당되지 않는 나머지 값들은 자동으로 NA 처리 됩니다. 나머지 값들에 값을 주고 싶다면 default 인자를 활용하면 됩니다.\n\ndt[,age_group2 := fcase(\n  age<20,'10대',\n  age<30,'20대',\n  age<40,'30대',\n  age<50,'40대',\n  age<60,'50대',\n  default='60대 이상'\n)]\ndt[,table(age_group2, useNA = 'always')]\n\nage_group2\n     10대      20대      30대      40대      50대 60대 이상      <NA> \n     2765      1356      1338      1398      1304      1839         0 \n\n\nuniqueN() : 고유한 데이터의 수를 확인하는 데 사용하는 함수입니다. 주로 사람의 ID 수를 카운트 하는 데 사용합니다.\n\ndt[,BMI2:=fifelse(is.na(BMI),'donno', fifelse(BMI>=30,'obese','normal'))][,uniqueN(BMI2)] |> head()\n\n[1] 3"
  },
  {
    "objectID": "blog/posts/2023-01-19-ojs/index.html",
    "href": "blog/posts/2023-01-19-ojs/index.html",
    "title": "Observable.js",
    "section": "",
    "text": "PlotData\n\n\n\n\n\n\nviewof bill_length_min = Inputs.range([32,50],\n{value: 35, step:1, label: \"Bill length (min)\"})\n\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\",\"Biscoe\",\"Dream\"],\n  {value: [\"Torgersen\",\"Biscoe\"],\n    label: \"Islands:\"\n  }\n)\n\nfiltered = transpose(data).filter(function(penguin){\n  return bill_length_min < penguin.bill_length_mm && islands.includes(penguin.island)\n});\n\n\nPlot.rectY(filtered,\n  Plot.binX(\n    {y: \"count\"},\n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight:80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  })\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInputs.table(filtered)"
  },
  {
    "objectID": "blog/posts/2023-02-02-imputation/index.html",
    "href": "blog/posts/2023-02-02-imputation/index.html",
    "title": "Imputation의 종류",
    "section": "",
    "text": "데이터에 결측치(NA)가 있는 경우, 결측치를 다른 값으로 채워넣어주는데, 이것을 imputation이라고 부릅니다. imputation을 할 때 꼭 확인해야 하는 것이 있는데, 바로 imputation 전후에 변수의 분포(distribution)을 확인하는 것입니다.\n해당 변수의 분포를 확인하는 것은 imputation 전후에 변수의 분포를 비교하기 위함입니다. 좋은 imputation은 imputation 이전의 분포와 imputation 이후의 분포가 비슷하게 해주어야 하는데, 이는 imputation을 실시한다 해도 원래 데이터의 분포를 반영하게끔 유지해야 하기 때문입니다.\ntitanic 패키지의 데이터를 활용해 imputation 연습을 진행해보도록 하겠습니다.\n\nsummary(titanic_train$Age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.42   20.12   28.00   29.70   38.00   80.00     177 \n\n\n\ntitanic_train |> ggplot(aes(x=Age))+ \n  geom_histogram(color='black', fill='dodgerblue') + theme_classic()\n\n\n\n\ntitanic_train 데이터의 나이(Age)를 살펴보니, NA가 177건이 있고, 결측치를 제외한 나머지 데이터는 위와 같은 분포를 띄고 있습니다. 이제 imputation을 통해 결측치를 채워넣어주도록 하겠습니다.\nImputation에는 크게 두 가지가 존재합니다. 하나는 Single imputation이고 다른 하나는 Multiple imputation입니다.\n\n1. Single imputation\nSingle imputation은 특정 패키지를 사용하지 않고, 평균, 중앙값 등 해당 변수의 단순 요약값을 이용하여 결측치를 채워넣는 방법을 의미합니다.\n평균, 중앙값 등 여러 가지 단일한 값을 채워 넣어 imputation 전과 후의 Age 변수의 분포를 비교해보도록 하겠습니다.\n\ntitanic_age_impute <- data.frame(\n  original = titanic_train$Age,  impute_const = ifelse(is.na(titanic_train$Age),0, titanic_train$Age),\n  impute_mean = ifelse(is.na(titanic_train$Age), mean(titanic_train$Age,na.rm=T), titanic_train$Age),\n  impute_median = ifelse(is.na(titanic_train$Age), median(titanic_train$Age,na.rm=T), titanic_train$Age) ) \n\n\np1 <- titanic_age_impute |> ggplot(aes(x=original)) + geom_histogram(color='black', fill='dodgerblue') + labs(title='Original distribution') + theme_classic()\n\np2 <- titanic_age_impute |> ggplot(aes(x=impute_const))+ geom_histogram(color='black', fill='darksalmon') + labs(title='Constant-imputed distribution') + theme_classic()\n\np3 <- titanic_age_impute |> ggplot(aes(x=impute_mean))+ geom_histogram(color='black', fill='darkseagreen') + labs(title='Mean-imputed distribution') + theme_classic()\n\np4 <- titanic_age_impute |> ggplot(aes(x=impute_median))+ geom_histogram(color='black', fill='goldenrod') + labs(title='Median-imputed distribution') + theme_classic()\n\n\nlibrary(patchwork)\n(p1+p2)/(p3+p4)\n\n\n\n\nSingle imputation 이후, 데이터의 분포를 Original과 비교했을 때, 분포가 굉장히 달라져있는 것을 볼 수 있습니다. Single imputation 특성 상, 하나의 값으로 모든 NA를 채워넣기 때문에 기존 데이터의 분포가 왜곡되게 됩니다.\n\n\n\n2. Multiple imputation\nmultiple imputation은 single imputation과 달리, 결측치를 여러가지 변수로 채워넣는 방법입니다. NA가 있는 변수 이외의 다른 변수들까지 함께 고려하여, 채워넣을 변수를 지정하는 방법입니다.\nmultiple imputation을 도와주는 대표적인 패키지는 mice 입니다.\nmice 패키지는 데이터의 어떤 변수에 NA가 있는지 확인할 수 있습니다.\n\nrequire(mice) \nmd.pattern(titanic_train |> select(Survived, Pclass, SibSp, Parch, Age, Sex))\n\n\n\n\n    Survived Pclass SibSp Parch Sex Age    \n714        1      1     1     1   1   1   0\n177        1      1     1     1   1   0   1\n           0      0     0     0   0 177 177\n\n\n위의 그림으로 봤을 때, Age에 NA가 있는 것을 알 수 있습니다.\n하지만 mice 패키지의 가장 큰 활용분야는 NA를 채워넣는 것입니다. mice() 를 이용해 결측치를 채워넣습니다. 이 때 method 인자에 원하는 imputation 방법을 설정할 수 있습니다.\n대표적인 method는 pmm, cart, lasso.norm 등이 있습니다.\n\npmm: Predictive mean matching\ncart: classification and regression trees 이용\nlasso.norm: Lasso linear regression 이용\n\n그 외에도 다양한 imputation 방법이 있으니, 자세한 사항은 ?mice()를 참고하시기 바랍니다.\nmice()로 결측치를 채워넣은 뒤, complete()를 이용해 결측치가 채워진 데이터를 생성합니다.\n\nm_pmm <- mice(titanic_train, method='pmm', printFlag = F)\nm_cart <- mice(titanic_train, method='cart', printFlag = F) \nm_lasso <- mice(titanic_train, method = 'lasso.norm', printFlag = F)\n\ntitanic_age_impute <- data.frame(\n  original = titanic_train$Age,  \n  impute_pmm = complete(m_pmm)$Age,\n  impute_cart = complete(m_cart)$Age,  \n  impute_lasso = complete(m_lasso)$Age)\n\np1 <- titanic_age_impute |> ggplot(aes(x=original)) + geom_histogram(color='black', fill='dodgerblue') + labs(title='Original distribution') + theme_classic()\n\np2 <- titanic_age_impute |> ggplot(aes(x=impute_pmm))+ geom_histogram(color='black', fill='darksalmon') + labs(title='PMM imputed distribution') + theme_classic()\n\np3 <- titanic_age_impute |> ggplot(aes(x=impute_cart))+ geom_histogram(color='black', fill='darkseagreen') + labs(title='CART imputed distribution') + theme_classic()\n\n\np4 <- titanic_age_impute |> ggplot(aes(x=impute_lasso))+ geom_histogram(color='black', fill='goldenrod') + labs(title='Lasso imputed distribution') + theme_classic()\n\n\n(p1+p2)/(p3+p4)\n\n\n\n\nimputation을 살펴봤을 때, CART 를 이용한 imputation이 원래의 Age 분포와 가장 비슷한 것으로 나타났습니다.\nmice의 다양한 imputation method는 채워넣고자 하는 변수의 유형 (continuous or categorical)에따라 다양한 방법이 존재합니다. 적절한 imputation 방법들을 통해 결측치를 채워넣어보시기 바랍니다.\n\n\n레퍼런스\n\nhttps://amices.org/mice/"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html",
    "href": "blog/posts/seoul_r_meetup/index.html",
    "title": "서울 R 밋업 자료",
    "section": "",
    "text": "install.packages(\"mlr3verse\")\ninstall.packages(\"apcluster\")\ninstall.packages(\"ranger\")\n\nlibrary(mlr3)\nlibrary(mlr3verse)\nlibrary(mlr3learners)\nlibrary(ranger)\nlibrary(xgboost)\n\n# R6 Object oriented programming\n\n# methods\ntask$print()\n\n# fields\n\ntask$negative\n\n# Dictionaries\n# key-value structure\n\ntasks, learners\n\nmlr_learners$get(\"classif.log_reg\")\nmlr_tasks\n\n\nas.data.table(mlr_tasks) |> View()\nas.data.table(mlr_learners) |> View()\nas.data.table(mlr_measures) |> View()\n\n# Helper function (sugar functions) : shortcut\n\nLearnerClassifXgboost$new()\nlrn(\"classif.xgboost\", predict_type=\"prob\")\n\nlrn(\"classif.logreg\")\nmlr_tasks$get(\"iris\")\nmlr_learners$get(\"classif.log_reg\")\n\nLearnerClassifRpart\nLearnerClassifRanger\nas.data.table(mlr_learners) |> View()"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#task",
    "href": "blog/posts/seoul_r_meetup/index.html#task",
    "title": "서울 R 밋업 자료",
    "section": "Task",
    "text": "Task\n\ntask <- tsk(\"ilpd\")"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#learner",
    "href": "blog/posts/seoul_r_meetup/index.html#learner",
    "title": "서울 R 밋업 자료",
    "section": "Learner",
    "text": "Learner\n\nlearner_lr <- lrn(\"classif.log_reg\", predict_type=\"prob\")\nlearner_rf <- lrn(\"classif.ranger\", predict_type=\"prob\")"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#train-predict",
    "href": "blog/posts/seoul_r_meetup/index.html#train-predict",
    "title": "서울 R 밋업 자료",
    "section": "train & predict",
    "text": "train & predict\n\nsplit <- partition(task, ratio=.7, seed=2023)\n\nlr_pred <- learner_lr$\n  train(task, split$train)$\n  predict(task, split$test)"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#evaluation",
    "href": "blog/posts/seoul_r_meetup/index.html#evaluation",
    "title": "서울 R 밋업 자료",
    "section": "Evaluation",
    "text": "Evaluation\n\n# msr(\"\")\n# msrs(\"\")\nmeasures_mac <- msrs(c(\"classif.acc\",\"classif.sensitivity\",\"classif.specificity\", \"classif.precision\",\"classif.fbeta\", \"classif.auc\"), average =\"macro\")\n\nmeasures_mic <- msrs(c(\"classif.acc\",\"classif.sensitivity\",\"classif.specificity\", \"classif.precision\",\"classif.fbeta\", \"classif.auc\"), average =\"micro\")\n\nlr_pred$score(measures_mac)"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#resampling",
    "href": "blog/posts/seoul_r_meetup/index.html#resampling",
    "title": "서울 R 밋업 자료",
    "section": "Resampling",
    "text": "Resampling\n\nmlr_resamplings\nresample <- rsmp(\"cv\", folds=10)\n\nrr$instantiate(task)\n\nrr  <- resample(task, learner_lr, resample)\nas.data.table(rr)\n\nrr$score(measures)\nrr$aggregate(measures_mac)\nrr$aggregate(measures_mic)"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#visualization",
    "href": "blog/posts/seoul_r_meetup/index.html#visualization",
    "title": "서울 R 밋업 자료",
    "section": "Visualization",
    "text": "Visualization\n\nrequire(mlr3viz)\nautoplot(rr, type=\"roc\")"
  },
  {
    "objectID": "blog/posts/seoul_r_meetup/index.html#benchmarking",
    "href": "blog/posts/seoul_r_meetup/index.html#benchmarking",
    "title": "서울 R 밋업 자료",
    "section": "benchmarking",
    "text": "benchmarking\n\nrequire(mlr3verse)\n\n\nilpd <- mlr3data::ilpd \nilpd$gender <- ifelse(ilpd$gender==\"Male\",1,0) |> as.numeric()\ntask <- as_task_classif(ilpd, target=\"diseased\")\n\nlearners <- list(\n  lrn(\"classif.log_reg\", predict_type=\"prob\", id=\"LR\"),\n  lrn(\"classif.rpart\", predict_type=\"prob\", id=\"DT\"),\n  lrn(\"classif.ranger\", predict_type=\"prob\", id=\"RF\"),\n  lrn(\"classif.xgboost\", predict_type=\"prob\", id=\"XGB\")\n)\nrsmp <- rsmp(\"cv\", folds=5)\n\ndesign <- benchmark_grid(task, learners, rsmp)\n\n\nbmr <- benchmark(design)\nprint(bmr)\n\nbmr$aggregate(measures)\n\n\nautoplot(bmr, \"roc\")"
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html",
    "href": "blog/posts/mlr3_feature_selection/index.html",
    "title": "mlr3 피처 선택",
    "section": "",
    "text": "Important\n\n\n\n이 글은 mlr3book1을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 mlr32 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다."
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html#인트로",
    "href": "blog/posts/mlr3_feature_selection/index.html#인트로",
    "title": "mlr3 피처 선택",
    "section": "인트로",
    "text": "인트로\n피처 선택 (feature selection)은 크게 두 종류가 있습니다. 먼저 Filter 방식은 데이터 전처리 단계에서 사용됩니다. 반면 Wrapper 방식은 모델을 만들 때 피처의 일부분(subset)을 사용해 성능을 평가할 때 사용되는 방법입니다.\n피처 선택은 변수 선택으로도 불리는데, 이 과정은 모델을 학습할 떄 사용될 피처의 일부분을 찾는 과정입니다. 모델 학습 시 최적의 피처들을 사용할 경우 아래와 같은 장점이 있습니다.\n\n과적합(overfitting) 감소로 인한 성능 향상\n불필요한 feature에 의존하지 않는 안정된(robust) 모델\n모델이 간단해지면서 해석이 용이해짐 4 빠른 모델 학습과 예측\n잠재적으로 값비싼 feature 수집 불필요"
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html#필터-방식",
    "href": "blog/posts/mlr3_feature_selection/index.html#필터-방식",
    "title": "mlr3 피처 선택",
    "section": "필터 방식",
    "text": "필터 방식\n필터 방법을 통한 피처 선택은 머신러닝 모델을 학습하기 이전에 수행할 수 있는 전처리(preprocessing) 단계입니다. 예를 들어서 예측하고자 하는 타겟 변수와의 상관계수가 0.2보다 큰 변수들만 선택하는 것입니다.\n이러한 방법은 각각의 피처와 타겟 변수에 대한 관계만을 고려하기 때문에 단변량(univariate) 필터라고 합니다. 문제는 이러한 방법이 연속형 변수들로 회귀분석에만 적용할 수 있고, 상관계수가 0.2라는 기준도 임의적이죠.\n그래서 피처의 중요성 (feature importance)을 기반으로 하는 다변량 필터 등이 등장을 하게 됩니다. 비록 단변량 필터들은 다변량 필터나 래퍼방식보다 간단하고 빠르게 계산할 수 있다는 장점이 있지만, 성능 측면에서는 같은 고급 필터 방법들이 더 뛰어나곤 합니다.\n필터 알고리즘은 각 피처별로 상관계수나 순위와 같은 수치를 부여해 피처를 선택하게 됩니다. 수치가 낮은 리처들은 모델링 단계에서 제외되는 방식입니다.\nmlr3에서는 mlr3filters 패키지를 통해 필터 방법을 수행할 수 있습니다. mlr3filters를 통해 수행할 수 있는 필터들은 다음과 같습니다.\n\nFilterCorrelation: 숫자 피처 - 숫자 타겟 간 피어슨 또는 스피어만 상관계수 활용\nFilterInformationGain: 피처의 정보로 인해 감소되는 타겟의 불확실성 정도 활용\nFilterJMIM: Minimal joint mutual information maximization, 선택된 피처들 간에 겹치는 정보를 최소화\nFilterPermutation: 주어진 러너에서 각 피처별 순열 피처 중요도를 계산\nFilterAUC: 각 피쳐별로 ROC 곡선 아래의 공간 계산\n\n그 외에 수행 가능한 모든 필터 방법들은 다음의 사이트를 참고하시기 바랍니다.\n\nFilter value 계산\n우선 원하는 필터 방법을 수행하기 위해 R 객체 형태로 만들어주겠습니다. 다른 mlr3 인스턴스와 마찬가지로 딕셔너리(mlr_filters)나 sugar function인 flt()를 통해 선언할 수 있습니다. 각 필터 객체들은 필터 값과 순위를 계산해 내림차순(점수가 높은 순)으로 정렬하는 $calculate() 메소드를 가집니다. 예를 들어 information gain 필터를 사용해보겠습니다.\n\nlibrary(mlr3filters)\nlibrary(mlr3)\nfilter = flt(\"information_gain\")\n\ntask = tsk(\"penguins\")\nfilter$calculate(task)\n\nas.data.table(filter)\n\n\n\n  \n\n\n\npenguins 데이터의 경우 species 가 target인데, 펭귄의 종을 설명하는 각 피처들의 점수를 보여줍니다.\n일부 filter들은 hyperparameters가 존재하는데, learner에서 param_set을 변경해주는 것처럼 간단히 변경 가능합니다. 예를 들면 correlation 방법의 경우 피어슨 또는 스피어만 방법으로 변경해줄 수 있습니다.\n\nfilter_cor <- flt('correlation')\nfilter_cor$param_set$values <- list(method='spearman')\nfilter_cor$param_set\n\n<ParamSet>\n       id    class lower upper nlevels    default    value\n   <char>   <char> <num> <num>   <int>     <list>   <list>\n1:    use ParamFct    NA    NA       5 everything         \n2: method ParamFct    NA    NA       3    pearson spearman\n\ntask2 <- tsk(\"mtcars\")\n\nfilter_cor$calculate(task2)\nas.data.table(filter_cor)\n\n\n\n  \n\n\n\n\n\nFeature importance filters\n피처 중요도 필터는 중요도(importance) 가 있는 모델에서 사용 가능한 필터입니다. 중요도 계산을 할 수 있는 러너들은 아래와 같습니다.\n\nas.data.table(mlr_learners)[sapply(properties, \\(x) \"importance\" %in% x)]\n\n\n\n  \n\n\n\nranger와 같은 일부 러너는 중요도 속성을 갖고 있지만, learner를 만들 때 지정을 해줘야 합니다.\n\nlibrary(mlr3learners)\nlrn_rf <- lrn('classif.ranger', importance='impurity')\n\n이제 FilterImportance 필터를 이용해보겠습니다.\n\n# 결측변수 제거하기\ntask$filter(which(complete.cases(task$data())))\n\nfilter_imp <- flt('importance', learner=lrn_rf)\nfilter_imp$calculate(task)\nfilter_imp\n\n<FilterImportance:importance>: Importance Score\nTask Types: classif\nProperties: -\nTask Properties: -\nPackages: mlr3, mlr3learners, ranger\nFeature types: logical, integer, numeric, character, factor, ordered\n          feature     score\n1:    bill_length 75.228997\n2: flipper_length 50.796965\n3:     bill_depth 33.371397\n4:         island 25.564683\n5:      body_mass 23.717214\n6:            sex  1.387641\n7:           year  1.005973\n\n\n\n\nEmbedded methods\n러너로 학습을 진행할 때, 러너는 예측에 도움이 되는 피처의 일부들을 선택하고 나머지 피처는 무시합니다. 이렇게 러너가 선택한 피처의 일부를 피처 선택으로 사용할 수가 있습니다. Embedded 방법이라고 불리는 이유가 바로 피처 선택이 러너 안에 포함되어 있기 때문입니다.\n선택된 피처의 경우 러너가 selected_features 라는 속성에 있는 경우 확인해볼 수 있습니다.\n\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\n\nstopifnot('selected_features' %in% learner$properties)\n\nlearner$train(task)\nlearner$selected_features()\n\n[1] \"flipper_length\" \"bill_length\"    \"island\"        \n\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\nfilter\n\n<FilterSelectedFeatures:selected_features>: Embedded Feature Selection\nTask Types: classif\nProperties: missings\nTask Properties: -\nPackages: mlr3, rpart\nFeature types: logical, integer, numeric, factor, ordered\n          feature score\n1: flipper_length     1\n2:    bill_length     1\n3:         island     1\n4:           year     0\n5:     bill_depth     0\n6:            sex     0\n7:      body_mass     0\n\n\n결과를 봤을 때, 학습시킨 모델에 의해 선택된 피처 점수는 1, 나머지는 0 즉 사용되지 않은 피처인 것을 알 수 있습니다.\n\n\nFilter-based feature selection\n필터를 통해 각 피처들의 점수가 계산이 되었다면, 다음 모델링 단계에서 피처 선택하여 학습을 진행해야 합니다. 위의 임베디드 방법에서 확인한 selected_features의 경우 1과 0으로 선택되는 변수를 구분할 수 있기 때문에 피처 선택에 있어 가장 직관적입니다. 따라서 여기서 선택된 피처들을 사용한다면 아래와 같이 수행할 수 있습니다.\n\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('selected_features', learner=learner)\nfilter$calculate(task)\n\nkeep <- names(which(filter$scores==1))\ntask$select(keep) # column을 선택하기 때문에 select\ntask$feature_names\n\n[1] \"bill_length\"    \"flipper_length\" \"island\"        \n\n\n위의 예시에서는 selected_features를 했기 때문에 0과 1로 구분이 되었기 때문에 피처들을 쉽게 선택할 수 있었습니다.\n연속형 점수로 피처의 점수가 출력되는 경우에서 피처 선택을 수행하는 경우는 다음과 같은 방법이 있습니다.\n\n위에서 top N개의 feature 선택하는 경우\n\n\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# top 3개 선택\nkeep <- names(head(filter$scores,3))\ntask$select(keep)\n\n\nscore가 특정 기준 k 보다 큰 경우\n\n\ntask <- tsk('penguins')\nlearner <- lrn('classif.rpart')\nfilter <- flt('information_gain')\nfilter$calculate(task)\n\n# information gain이 0.5보다 큰 경우\nkeep <- names(which(filter$scores>0.5))\ntask$select(keep)"
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html#래퍼-방식",
    "href": "blog/posts/mlr3_feature_selection/index.html#래퍼-방식",
    "title": "mlr3 피처 선택",
    "section": "래퍼 방식",
    "text": "래퍼 방식\n래퍼 방식은 피처들의 일부를 선택한 뒤 그 피처들로 모델링을 하고 그 모델의 성능을 비교하여, 최적의 성능을 나타낼 때의 피처들을 선택하게 됩니다. 한마디로 모델의 성능을 최적화하기 위해 반복적으로 피처들을 선택하는 것입니다.\n래퍼 방식은 피처들의 순위를 매기는 대신, 일부 피처 사용하여 학습한 뒤 선택된 성능 지표에 따라 평가하게 됩니다. 이는 하이퍼파라미터 최적화 과정과 굉장히 유사합니다. 즉 하이퍼파라미터 최적화를 통해 높은 성능을 내는 하이퍼파라미터를 찾는 것처럼 래퍼 방식을 통해 높은 성능을 내는 피처들을 찾는 것입니다.\n\n간단한 예시\nmlr3에서는 mlr3fselect 패키지의 FSelector 객체를 이용하여 위의 방법을 수행합니다.\n\nlibrary(mlr3fselect)\ninstance <- fselect(\n  fselector = fs(\"sequential\"),\n  task= tsk('penguins')$select(c(\"bill_depth\",\"bill_length\",\"body_mass\",\"flipper_length\")),\n  learner = lrn('classif.rpart'),\n  resampling = rsmp('holdout'),\n  measure = msr('classif.acc')\n)\n\n성능 비교를 위한 피처들의 모든 하위집단을 확인하기 위해선 아래의 코드로 확인 가능합니다.\n\ndt <- as.data.table(instance$archive)\ndt[batch_nr ==1, 1:5][order(-classif.acc)]\n\n\n\n  \n\n\n\n위의 결과를 확인해보니, flipper_length가 첫 번째 반복에서 가장 높은 예측 성능을 보였습니다(정확도 0.7478). 또한 두 번째 반복에서는 bill_length 피처가 더해지자가 무려 90% 이상의 예측 정확도가 나타났습니다.\n\ndt[batch_nr ==2, 1:5][order(-classif.acc)]\n\n\n\n  \n\n\n\n\ndt[batch_nr ==3, 1:5][order(-classif.acc)]\n\n\n\n  \n\n\n\n반면에 세 번째 반복에서 bill_depth가 추가되었지만, 예측 성능이 향상되지는 않았습니다.\n최적의 feature 들을 확인하기 위해서는 아래와 같이 입력할 수 있습니다.\n\ninstance$result_feature_set\n\n\n\nFSelectInstance 클래스\nmlr3fselect 에서 피처 선택을 수행할 경우, fselect() 함수는 FSelectInstanceSingleCrit 객체를 만들고, FSelector 객체를 통해 피처 선택을 실시합니다.\nFSelectInstanceSingleCrit 객체를 만들기 위해 fsi() sugar function을 사용할 수 있습니다.\n\ninstance <- fsi(\n  task = tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals=5)\n)\n\n이제 피처 선택을 위한 인스턴스가 만들어졌습니다. 다음으로 해야할 것은 이 인스턴스를 어떤 알고리즘을 통해 수행할 것인지입니다.\n\n\nFselector 클래스\nmlr3fselect::FSelector에는 다양한 종류의 피처 선택 알고리즘이 존재합니다.\n\nRandom Search (mlr3fselect::FSelectorRandomSearch)\nExhaustive Search (mlr3fselect::FSelectorExhaustiveSearch)\nSequential Search (mlr3fselect::FSelectorSequential)\nRecursive Feature Elimination (mlr3fselect::FSelectorRFE)\nDesign Points (mlr3fselect::FSelectorDesignPoints)\nGenetic Search (mlr3fselect::FSelectorGeneticSearch)\nShadow Variable Search (mlr3fselect::FSelectorShadowVariableSearch)\n\n이 알고리즘들을 활용하여 앞서 만들어놓은 인스턴스 (instance)에 대한 피처 선택을 수행합니다. 여기서는 간단한 random search 방식을 이용하도록 하겠습니다. FSelector 클래스는 mlr_fselectors 딕셔너리에서 확인할 수 있습니다. 또한 fs() sugar function을 통해 새로운 피처 선택 클래스를 만들어줄 수 있습니다.\n\nfselector <- fs(\"random_search\")\n\n\n\n피처 선택 수행\n피처 선택 역시 하이퍼파라미터 최적화에서 수행한 것처럼 $optimize() 메소드로 수행합니다. FSelector 클래스를 실행함으로써 최적의 분류 정확도를 갖는 피처들의 조합을 찾아낼 수 있습니다. 실행해보면 생각보다 오랜 시간이 걸리는 것을 알 수 있습니다.\n\nfselector$optimize(instance)\n\n\n\n  \n\n\n\n이 알고리즘은 다음과 같은 흐름으로 진행됩니다.\n피처 선택이 종료되었다면, 최적의 피처 세트는 인스턴스를 통해 접근할 수 있습니다.\n\nas.data.table(instance$result)[,.(features, classif.acc)]\n\n\n\n  \n\n\n\nFSelectInstanceMultiCrit 을 통해 여러 가지 조건으로 피처 선택을 진행할 수 있습니다. 앞서 만들었던 인스턴스와 달라지는 부분은 measure 뿐입니다. sugar function fsi()을 사용한다면 FSelectInstanceSingleCrit 과 FSelectInstanceMultiCrit을 구분 지을 필요 없이, measure의 개수로 자동으로 정해집니다.\n\ninstance = fsi(\n  task = tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msrs(c(\"classif.acc\",\"time_train\")),\n  terminator = trm(\"evals\", n_evals = 5)\n)\n\n\n\n피처 선택 자동화\nAutoFSelector 클래스는 FSelectInstanc 를 만들고 FSelector 를 만들어 $optimize()하는 과정을 합쳐놓았습니다.\n\nlibrary(mlr3learners)\naf = auto_fselector(\n  fselector = fs(\"random_search\"),\n  learner = lrn(\"classif.log_reg\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals=5)\n)\n\nAutoFSelector 역시 AutoTuner 와 마찬가지로 러너의 클래스를 상속받기 때문에 러너처럼 활용이 가능합니다. 그래서 피처 선택 이후 $train(),$predict() 메소드 사용이 가능합니다.\n이번에는 benchmark()를 통해 피처 선택을 통해 만든 모델과, 그렇지 않은 모델의 성능을 비교해보도록 하겠습니다. sonar 데이터셋을 활용해 로지스틱 회귀 모형을 추가로 만들어줍니다.\n\ngrid = benchmark_grid(\n  task = tsk(\"sonar\"),\n  learner = list(af, lrn(\"classif.log_reg\")),\n  resampling = rsmp(\"cv\", folds=3)\n)\n\nbmr = benchmark(grid)\n\n결과를 비교했을 때, 비록 모델 학습에 소요된 시간은 AutoFSelector 로 만든 모델이 더 오래 걸렸지만, 정확도 측면에서는 기본 로지스틱 회귀 모델보다 더 높은 성능을 보여주는 것을 알 수 있습니다.\n\naggr <- bmr$aggregate(msrs(c(\"classif.acc\",\"time_train\")))\nas.data.table(aggr)[,.(learner_id, classif.acc, time_train)]"
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html#요약",
    "href": "blog/posts/mlr3_feature_selection/index.html#요약",
    "title": "mlr3 피처 선택",
    "section": "요약",
    "text": "요약\n\n\n\n\n\n\n\n\nR6 Class\nSugar function\nSummary\n\n\n\n\nFilter\nflt()\n각 피처들의 점수 계산을 통한 피처 선택\n\n\nFSelectInstanceSingleCrit or FSelectInstanceMultiCrit\nfselect()\n피처 선택 구성 및 결과 저장\n\n\nFSelector\nfs()\n피처 선택 알고리즘 선택\n\n\nAutoFSelectoor\nauto_fselector()\n피처 선택 자동화"
  },
  {
    "objectID": "blog/posts/mlr3_feature_selection/index.html#참고자료",
    "href": "blog/posts/mlr3_feature_selection/index.html#참고자료",
    "title": "mlr3 피처 선택",
    "section": "참고자료",
    "text": "참고자료\n\nhttps://mlr3book.mlr-org.com/feature-selection.html#sec-multicrit-featsel"
  },
  {
    "objectID": "blog/posts/2023-02-02-dual-yaxis/index.html",
    "href": "blog/posts/2023-02-02-dual-yaxis/index.html",
    "title": "ggplot에서 두 번째 y축 그리기",
    "section": "",
    "text": "dat <- data.table(\n  day= as.factor(rep(c(1,2,3,7),each=2)),\n  transfusion=rep(c('Yes','No'), 4),\n  num=c(157,845,134,681,68,559, 27,342)\n)\ndat[,transfusion:=factor(transfusion, levels=c('No','Yes'))]\ndat[,y_pos := cumsum(num)-0.5*num,by=.(day)]\n\ndat[,prop :=num/sum(num)*100,by=day]\ndat\n\n\n\n  \n\n\nratio <- data.table(\n  day= as.factor(c(1,2,3,7)),\n  prob = c(15.8,32.2,43.0,50.4)\n)\nratio\n\n\n\n  \n\n\nggplot()+\n  geom_col(data=dat,aes(x=day,y=num, fill=transfusion),\n           color='black', alpha=.5,\n           width=0.5,\n    position = position_stack()) +\n  scale_y_continuous(limits=c(0,1010),expand=c(0,0),\n                     name='Number of patients',\n                     sec.axis = sec_axis(~.*0.1,\n                                         name='Proportion of patients received a transfusion (%)'))+\n  geom_point(data=ratio, aes(x=day, y=prob/0.1, \n                             color='Pateints received transfusion'), \n             size=3)+\n  geom_text(data=dat, aes(x=day, label=num, y=y_pos))+\n  geom_line(data=ratio, aes(x=day, y=prob/0.1, group=1,\n                            color='Pateints received transfusion'))+\n  theme_classic() +\n  scale_color_manual(values=c('Pateints received transfusion'='black'))+\n  scale_fill_discrete(direction=-1,\n                      breaks=c('Yes','No'))+\n  labs(x='Follow-up (days)',\n       fill='RBC transfusion',\n       color='')+\n  theme(legend.position='top',\n        legend.direction = 'vertical',\n        legend.box = 'horizontal',\n        legend.box.spacing = unit(1,units = 'mm'),\n        axis.text.x = element_text(size=12),\n        axis.text.y = element_text(size=12)\n        )"
  },
  {
    "objectID": "blog/posts/ggplot_correlation_heatmap/index.html",
    "href": "blog/posts/ggplot_correlation_heatmap/index.html",
    "title": "상관관계 시각화",
    "section": "",
    "text": "상관관계 히트맵 (Correlation heatmap)은 상관관계 매트릭스를 시각화한 것으로, 데이터세트에서 변수 간의 쌍(pair)별 상관관계를 시각화하는 데 사용됩니다. 상관관계 히트맵에서 각 셀은 두 변수 간의 상관계수를 나타내며 셀의 색상은 상관관계의 강도와 방향을 나타냅니다. 일반적으로 색상 그라데이션이 사용되는데, 한 색상은 양의 상관관계를 나타내고 다른 색상은 음의 상관관계를 나타냅니다. 색상의 강도는 상관관계의 강도를 나타내구요.\n상관관계 히트맵은 여러 경우에 쓰일 수 있습니다. 예를 들어 탐색적 데이터 분석 (EDA) 과정에서 상관관계 히트맵은 변수 간의 잠재적 관계 또는 연관성을 식별하는 데 도움이 될 수 있습니다. 추후 이 결과를 바탕으로 회귀분석 등 심화 분석을 진행할 수 있습니다.\n또한 머신러닝이나 통계 모델링에서 상관 히트맵을 사용하여 다중 공선성으로 이어질 수 있는 고도로 상관된 기능을 식별하고 제거하여 모델의 안정성과 해석 가능성을 향상시킬 수 있습니다.\n아무래도 상관계수를 숫자로 보여주는 테이블을 제시해주는 것보다는 상관관계 히트맵을 통해 각 변수 간 상관성을 보여주는 것이 더욱 직관적이고 시각적으로 매력적인 분석결과일 것입니다.\n그래서 이번 포스트에서는 R을 이용해 상관관계를 시각화하는 상관관계 히트맵을 만드는 방법을 살펴보겠습니다."
  },
  {
    "objectID": "blog/posts/ggplot_correlation_heatmap/index.html#ggplot2",
    "href": "blog/posts/ggplot_correlation_heatmap/index.html#ggplot2",
    "title": "상관관계 시각화",
    "section": "ggplot2",
    "text": "ggplot2\nR에서 데이터 시각화에 가장 많이 사용되는 패키지는 단연코 ggplot2일 것입니다. ggplot2를 이용해 상관관계 히트맵을 그리려면 특정 데이터의 상관행렬을 구해야 합니다.\nmtcars 데이터를 이용해 예시를 들어보겠습니다.\n\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(RColorBrewer)\n\ncor_matrix <- cor(mtcars[6:11])\ncormat <- round(cor(cor_matrix),2)\ncor_melt <- melt(cormat, na.rm = TRUE)\n\ncor() 함수를 이용해 상관행렬을 만들었고, 각 상관계수를 소수점 둘째 자리에서 반올림해주었습니다.\n또한 시각화를 용이하게 하기 위해 해당 매트릭스를 long 형태로 변경하였습니다.\n이 상태로 ggplot2을 이용해 히트맵을 그려보면 다음과 같습니다.\n\nggplot(cor_melt, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() + \n  scale_fill_gradient2(low = \"#abd9e9\", mid = \"#FFFFFF\", high = \"#FF0000\", midpoint = 0, space = \"Lab\", name=\"Correlation\") +\n  theme_minimal()\n\n\n\n\n그럴 듯한 상관관계 히트맵이 만들어졌습니다. 그런데 상관관계 히트맵에서는 대각행렬을 기준으로 위와 아래가 동일한 값을 갖게 됩니다. 자신과의 상관성을 나타내는 대각선의 1을 기준으로 아래와 위가 같은 것을 알 수 있습니다.\n따라서 대각행렬을 기준으로 위 또는 아래의 값만 시각화를 해주는 것이 더욱 깔끔한 그래프를 시각화하는 데 도움이 됩니다.\n아래의 예시에서는 대각선을 기준으로 아래의 값만 남겨두도록 하겠습니다. 위의 값을 없애주어야 하기 때문에 upper.tri() 함수를 이용합니다. (아래의 값을 없애주고 싶은 경우는 lower.tri() 를 사용하면 됩니다.)\n\nlower_tri <- cormat\nlower_tri[upper.tri(lower_tri)] <- NA #OR upper.tri function\ncor_melt_lower <- melt(lower_tri, na.rm = TRUE)\n\n그런 뒤에 다시 시각화를 해보면 다음과 같은 결과가 나타납니다.\n\ncor_heatmap <- ggplot(cor_melt_lower, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() + \n  scale_fill_gradient2(low = \"#abd9e9\", mid = \"#FFFFFF\", high = \"#FF0000\", midpoint = 0, space = \"Lab\", name=\"Correlation\") +\n  theme_minimal() +\n  theme(panel.grid.major = element_blank())\ncor_heatmap\n\n\n\n\n이제 이 그래프에 상관계수를 넣어주기만 하면 될 것 같습니다.\n\ncor_heatmap +\n  geom_text(aes(Var1, Var2, label = value), color = \"black\", size = 4)"
  },
  {
    "objectID": "blog/posts/ggplot_correlation_heatmap/index.html#ggcorrplot",
    "href": "blog/posts/ggplot_correlation_heatmap/index.html#ggcorrplot",
    "title": "상관관계 시각화",
    "section": "ggcorrplot",
    "text": "ggcorrplot\n위에서는 ggplot2 패키지를 이용해 상관관계 히트맵을 만드는 코드를 살펴보았습니다. 다만 위의 코드가 너무 길고 복잡하여, 이해하기 힘들 수 있다는 단점이 있습니다. 아무래도 ggplot2를 통해 일일이 옵션을 설정해주어야 하기 때문에 어쩔 수 없습니다.\n이러한 단점을 보완하고자 ggcorrplot이라는 패키지를 이용해 correlation heatmap 을 그릴 수 있습니다.\n\ninstall.packages(\"ggcorrplot\")\nlibrary(ggcorrplot)\n\n\n# p-value 유의미성 위한 계산\np.mat <- cor_pmat(mtcars)\n\nggcorrplot(cormat,\n          method = \"square\",\n          type = \"lower\",\n          colors = c(\"#6D9EC1\", \"white\", \"#E46726\"),\n          ggtheme = theme_minimal,\n          lab = T,\n          lab_col = \"black\",\n          lab_size = 3,\n          outline.color = \"white\",\n          hc.order = T,\n          p.mat = p.mat,\n          insig = \"blank\",\n          show.diag = T,\n          legend.title = \"\"\n          ) +\n  theme(panel.grid.major = element_blank())\n\n\n\n\nggcorrplot() 함수를 이용해 히트맵 작성이 가능합니다.\nggcorrplot()에 쓰이는 주요 인자들은 다음과 같습니다.\n\nmethod: heatmap 을 채워 줄 모양을 선택합니다. square, circle\ntype: heatmap 모양을 설정합니다. full, lower, upper\nggtheme: 그래프의 전반적인 테마. ggplot2 또는 테마 관련 패키지의 테마 함수 사용 (e.g., theme_())\ncolors: heatmap의 low, mid, high 순으로 색상을 지정합니다.\noutline.color: method에서 설정한 square 또는 circle의 테두리색상\nlab: 상관계수 표시 여부\n\nlab_size: 상관계수 글자 크기\nlab_col: 상관계수 글자 색상\n\nhc.order: 상관계수에 따른 정렬\np.mat : 상관계수의 p-value matrix\n\ninsig: 상관계수가 유의미하지 않을 경우 표시할 방법: ‘blank’\n\nshow.legend: 범례 표시 여부\n\nlegend.title: 범례 제목.\n\n\nggcorrplot을 통해 만들어지는 그래프 역시 ggplot2 기반이기 때문에, ggplot()에서 사용할 수 있는 옵션들을 사용할 수 있다는 장점이 있습니다."
  },
  {
    "objectID": "blog/posts/ggplot_correlation_heatmap/index.html#참고자료",
    "href": "blog/posts/ggplot_correlation_heatmap/index.html#참고자료",
    "title": "상관관계 시각화",
    "section": "참고자료",
    "text": "참고자료\nhttp://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2"
  },
  {
    "objectID": "blog/posts/sankey_plot/index.html",
    "href": "blog/posts/sankey_plot/index.html",
    "title": "Sankey plot 그리기",
    "section": "",
    "text": "Sankey(생키) diagram라고도 하는 Sankey plot은 범주형 데이터의 분배, 변환 또는 보존을 설명하는 데 활용되는 그래프 중 하나입니다. 그래프에서 노드 간 화살표 또는 링크의 너비는 흐름 양에 비례하므로 여러 흐름의 상대적 크기를 쉽게 비교할 수 있습니다.\n\n쉽게 말해, Sankey plot은 데이터의 변화 양상이나 응답 빈도의 양상 등을 확인할 수 있는 그래프입니다.\nR에서 Sankey plot을 시각화할 수 있는 패키지 중 하나는 ggalluvial 입니다. gg가 붙은 것에서 알 수 있듯이, ggplot2 의 확장 패키지 중 하나입니다.\n저 같은 경우, 시간의 흐름에 따른 범주의 양상을 Sankey plot으로 시각화한 경험이 있습니다. 예를 들면 첫 번째 건강검진부터 세 번째 건강검진 간 비만인 사람들의 분포를 시각화하는 것이죠.\nggalluvial은 Sankey plot을 그리기 위해 wide 데이터와 long 데이터를 모두 사용할 수 있습니다."
  },
  {
    "objectID": "blog/posts/sankey_plot/index.html#wide-데이터로-sankey-plot-그리기",
    "href": "blog/posts/sankey_plot/index.html#wide-데이터로-sankey-plot-그리기",
    "title": "Sankey plot 그리기",
    "section": "Wide 데이터로 Sankey plot 그리기",
    "text": "Wide 데이터로 Sankey plot 그리기\nwide 데이터는 쉽게 말해 여러 가지 column이 옆으로 붙어있는 데이터를 의미합니다.\nSankey plot을 그리기 위한 wide 데이터에서는 하나의 열에 범주가 와야 하고, 범주의 조합에 해당하는 빈도(Frequency) 열이 와야 합니다.\n예를 들어 HairEyeColor 데이터를 살펴보겠습니다.\n\nHairEyeColor |> as.data.frame() \n\n\n\n  \n\n\n\nHairEyeColor 데이터는 성별, 머리 색상, 눈동자 색상에 따른 빈도 데이터를 담고 있습니다. 이 데이터를 활용해 Sankey plot을 작성해보도록 하겠습니다.\n\nlibrary(ggalluvial)\nggplot(as.data.frame(HairEyeColor),\n       aes(y=Freq, axis1 = Hair, axis2 = Eye, axis3= Sex)) +\n  geom_alluvium(aes(fill=Eye),\n                width=1/8, knot.pos = 0, reverse = F) +\n  geom_stratum(alpha = .25, width=1/8, reverse=F) +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), reverse=F) +\n  scale_fill_manual(values=c(Brown=\"brown\", Hazel=\"#E2AC76\", Green=\"darkgreen\", Blue=\"lightblue2\")) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\ngeom_alluvial()은 Sankey plot에서 노드를 그려주고, geom_stratum()은 각 열의 범주를 나타내는 막대를 그려줍니다.\n성별 * 머리 색상 * 눈동자 색상에 따른 범주의 분포를 보여주는 그래프가 완성되었습니다."
  },
  {
    "objectID": "blog/posts/sankey_plot/index.html#long-데이터로-sankey-plot-그리기",
    "href": "blog/posts/sankey_plot/index.html#long-데이터로-sankey-plot-그리기",
    "title": "Sankey plot 그리기",
    "section": "Long 데이터로 Sankey plot 그리기",
    "text": "Long 데이터로 Sankey plot 그리기\n이번에는 긴 데이터를 활용해 Sankey plot을 그려보겠습니다. 긴 데이터는 열들의 조합에 따른 범주를 나타내는 넓은 데이터와 달리, 하나의 행이 보통 하나의 경우를 의미합니다. 긴 데이터는 주로 공통된 ID, 그룹 별 값에 따른 데이터를 보여줍니다.\n예를 들어보겠습니다. majors라는 데이터를 보면 student 라고 하는 학생 ID가 1번부터 15번 까지 있습니다. 또한 semester는 1학기부터 15학기까지 있는데, 각 학생들의 학기별 curriculum에 대한 데이터를 보여주고 있습니다.\n\nmajors$curriculum <- as.factor(majors$curriculum)\nggplot(majors,\n       aes(x=semester, stratum = curriculum, alluvium=student, fill=curriculum, label = curriculum)) +\n  geom_flow(stat = \"alluvium\", lode.guidance = lode_frontback,\n            color = \"darkgrey\") + \n  geom_stratum() +\n  scale_fill_brewer(type=\"qual\", palette = \"Set2\")\n\n\n\n\n위의 그래프를 통해, 학생들이 학기별로 어떤 커리큘럼을 선택했는지 흐름과 빈도를 알 수 있습니다.\n물론 긴 데이터에 빈도가 있는 경우도 이를 활용할 수 있습니다. 마찬가지로 ggalluvial 패키지의 vaccinations 데이터를 이용하겠습니다.\n\nvaccinations\n\n\n\n  \n\n\n\nvaccinations 데이터는 RAND American Life Panel 설문 조사 중 세 건을 합친 데이터입니다. 인플루엔자 예방을 위해 백신 접종을 할 의향을 묻고 있는 데이터입니다. Never, Sometimes, Always 등의 응답이 기록되어 있습니다.\n\n# 그래프에서 나타나는 범주 순서 변경 위해 \nvaccinations$response <- factor(vaccinations$response, levels = rev(levels(vaccinations$response)))\n\nggplot(vaccinations,\n       aes(x=survey, y=freq,\n           stratum = response, \n           alluvium = subject,\n           fill = response,\n           label = response)) +\n  geom_flow() +\n  geom_stratum(alpha=.5) +\n  geom_text(stat = \"stratum\", size=3) +\n  scale_x_discrete(expand = c(0.1,0.1)) +\n  theme_void() +\n  theme(legend.position = \"none\")\n\n\n\n\n위의 그래프에서 freq는 survey와 response별로 합해져 그래프에 나타나는 것을 알 수 있습니다."
  },
  {
    "objectID": "blog/posts/sankey_plot/index.html#참고자료",
    "href": "blog/posts/sankey_plot/index.html#참고자료",
    "title": "Sankey plot 그리기",
    "section": "참고자료",
    "text": "참고자료\nhttps://cran.r-project.org/web/packages/ggalluvial/vignettes/ggalluvial.html"
  },
  {
    "objectID": "blog/posts/ggplot_facet_label/index.html",
    "href": "blog/posts/ggplot_facet_label/index.html",
    "title": "ggplot2 facet label 설정",
    "section": "",
    "text": "다음과 같은 데이터가 있다고 가정해봅시다.\n\n\n\n\n  \n\n\n\n만성신질환 (Chronic kidney disease, CKD) 여부에 따라 급성신손상(Acute kidney injury, AKI)의 단계별로 ICU 입원기간 1일부터 7일 간 Cr의 수치를 비교하는 그래프를 그린다고 가정해봅시다.\n우선 CKD 여부에 따른 그래프를 각각 그릴 수 있을 것입니다. 이 때, patchwork 패키지를 이용해 두 개의 그래프를 붙여줄 수 있습니다.\n\nplot_non_ckd <- data[Chronic_kidney_ds==0]|> \n  ggplot(aes(x=Day, y=mean,\n             group = as.factor(AKI_stage)))+\n  geom_line(aes(color=as.factor(AKI_stage))) +\n  geom_ribbon(aes(y=mean, ymin=mean-sd ,ymax = mean+sd, fill=as.factor(AKI_stage)),\n              alpha=.2) +\n  geom_point(aes(color=as.factor(AKI_stage))) +\n  scale_y_continuous(expand=c(0,0),\n                     limits=c(0,8),\n                     breaks=seq(0,8,1))+\n  scale_color_discrete(name=\"AKI Stage\") + \n  scale_fill_discrete(name=\"AKI Stage\") + \n  theme_classic() + \n  labs(x=\"ICU Days\",\n       y= \"Average Cr (mg/dL)\") +\n  theme(legend.position = \"top\")\n\nplot_ckd <- data[Chronic_kidney_ds==1]|> \n  ggplot(aes(x=Day, y=mean,\n             group = as.factor(AKI_stage)))+\n  geom_line(aes(color=as.factor(AKI_stage))) +\n  geom_ribbon(aes(y=mean, ymin=mean-sd ,ymax = mean+sd, fill=as.factor(AKI_stage)),\n              alpha=.2) +\n  geom_point(aes(color=as.factor(AKI_stage))) +\n  scale_y_continuous(expand=c(0,0),\n                     limits=c(0,8),\n                     breaks=seq(0,8,1))+\n  scale_color_discrete(name=\"AKI Stage\") + \n  scale_fill_discrete(name=\"AKI Stage\") + \n  theme_classic() + \n  labs(x=\"ICU Days\",\n       y= \"Average Cr (mg/dL)\") +\n  theme(legend.position = \"top\")\n\n\nplot_non_ckd + plot_ckd\n\n\n\n\n물론 두 개의 그래프를 각각 보여주는 것도 나쁘지 않습니다. 다만 y축이나 legend는 공통된 값들이기 때문에 두 번 보여줄 필요는 없습니다.\n그럴 때는 facet_grid() 를 이용해주면 됩니다.\n\nplot <- data|> \n  ggplot(aes(x=Day, y=mean,\n             group = as.factor(AKI_stage)))+\n  geom_line(aes(color=as.factor(AKI_stage))) +\n  geom_ribbon(aes(y=mean, ymin=mean-sd ,ymax = mean+sd, fill=as.factor(AKI_stage)),\n              alpha=.2) +\n  geom_point(aes(color=as.factor(AKI_stage))) +\n  scale_y_continuous(expand=c(0,0),\n                     limits=c(0,8),\n                     breaks=seq(0,8,1))+\n  scale_color_discrete(name=\"AKI Stage\") + \n  scale_fill_discrete(name=\"AKI Stage\") + \n  theme_classic() + \n  labs(x=\"ICU Days\",\n       y= \"Average Cr (mg/dL)\") +\n  theme(legend.position = \"top\")\n\nplot + facet_grid(~Chronic_kidney_ds)\n\n\n\n\n여기서 CKD 변수의 코딩이 0과 1로 되어있어, 그래프에도 0과 1로 나타났습니다. facet의 label을 수정하려면 아래와 같이 labeller를 추가해주면 됩니다.\n\nckd_label <- c(\"Non-CKD\",\"CKD\")\nnames(ckd_label) <- c(0,1)\nplot + \n  facet_grid(~Chronic_kidney_ds,\n             labeller = labeller(Chronic_kidney_ds=ckd_label))\n\n\n\n\nlabeller 사용이 어렵다면 데이터의 자체를 리코딩(변경)해주면 됩니다."
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html",
    "title": "dplyr group_by()",
    "section": "",
    "text": "group_by()는 특정 집단의 범주별로 어떤 값들을 요약하거나 계산할 때 많이 활용되는 함수입니다.\n\nlibrary(dplyr)\nstarwars |> \n  group_by(species, sex) |> \n  select(height, mass) |> \n  summarise(\n    height = mean(height, na.rm = TRUE),\n    mass = mean(mass, na.rm = TRUE)\n  )\n\n\n\n  \n\n\n\n\nby_species <- starwars |> group_by(species)\nby_sex_gender <- starwars |> group_by(sex,gender)\n\n출력했을 때, 데이터가 그룹으로 묶인 것을 알 수 있습니다.\n\nBy speciesBy sex gender\n\n\n\nby_species\n\n\n\n  \n\n\n\n\n\n\nby_sex_gender\n\n\n\n  \n\n\n\n\n\n\ntally() 또는 count()를 이용해 그룹별 n수를 파악할 수도 있습니다.\nsort 인자를 통해서 그룹 수가 많은 순서대로 정렬할 수 있습니다.\n\nby_species |> tally()\n\n\n\n  \n\n\nby_sex_gender |> \n  tally(sort=T)\n\n\n\n  \n\n\n\n또한 group_by() 안에서 새로운 변수를 만들어 그 변수에 따른 계산을 할 수도 있습니다.\n\nstarwars |> \n  group_by(bmi_cat = cut(mass/(height/100)^2, breaks = c(0,18,23,25,30,Inf))) |> \n  tally()"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-정보-확인",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-정보-확인",
    "title": "dplyr group_by()",
    "section": "그룹 변수 정보 확인",
    "text": "그룹 변수 정보 확인\n그룹 변수를 지정해준 경우, group_keys() 를 통해 그룹 변수의 범주(종류)를 확인할 수 있습니다.\n\nby_species |> group_keys()\n\n\n\n  \n\n\n\n한편 group_vars() 는 group_by() 에 사용된 변수의 이름을 확인할 때 사용할 수 있습니다.\n\nby_sex_gender |> group_vars()\n\n[1] \"sex\"    \"gender\""
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-변경-및-추가",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-변경-및-추가",
    "title": "dplyr group_by()",
    "section": "그룹 변수 변경 및 추가",
    "text": "그룹 변수 변경 및 추가\n기존의 그룹처리 되어있는 데이터에 group_by() 를 진행할 시, 새로운 그룹 변수로 덮어씌워집니다.\nby_species는 앞서 group_by(species)를 통해 species별로 묶은 데이터죠. 여기서 group_by(homeworld)를 추가한다면 기존의 species에서 homeworld를 기준으로 그룹이 묶이게 됩니다.\n\nby_species |> \n  group_by(homeworld) |> \n  tally()\n\n\n\n  \n\n\n\n만약 기존의 그룹을 대체하는 것이 아니라 추가를 할 수도 있을텐데요. group_by()에 .add=T 인자를 추가할 경우, 기존에 있던 그룹 변수에 새로운 그룹 변수를 추가할 수도 있습니다.\n\nby_species |> \n  group_by(homeworld, .add=T) |> \n  tally()"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-제거",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html#그룹-변수-제거",
    "title": "dplyr group_by()",
    "section": "그룹 변수 제거",
    "text": "그룹 변수 제거\n그룹 처리된 데이터를 제거하고 싶은 경우, ungroup() 을 사용합니다.\n\nby_species |> \n  ungroup() |> \n  tally()\n\n\n\n  \n\n\n\n물론 특정 그룹 변수만 제외할 수도 있습니다. by_sex_gender에서 sex와 gender로 묶여있던 것을 ungroup()을 통해 sex만 제거하여 gender만 그룹으로 남아있게 되었습니다.\n\nby_sex_gender |> \n  ungroup(sex) |> \n  tally()"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html#group_by와-함께-쓰는-동사",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html#group_by와-함께-쓰는-동사",
    "title": "dplyr group_by()",
    "section": "group_by()와 함께 쓰는 동사",
    "text": "group_by()와 함께 쓰는 동사\n다음으로 dplyr에서 group_by()와 자주 쓰이는 함수들을 알아보겠습니다.\n\n1. summarise(), summarize()\nsummarise()와 group_by()가 함께 사용될 경우, 그룹별로 요약 계산을 수행합니다. 평균, 표준편차, 중앙값, 최소, 최대값과 같은 값을 출력할 수 있습니다.\n\nby_species |> \n  summarise(\n    n=n(),\n    mean_height = mean(height,na.rm=T)\n  )\n\n\n\n  \n\n\n\n.groups인자는 출력되는 결과의 그룹 구조를 다루는 인자입니다. keep은 그룹으로 묶인 상태를 유지하는 반면, drop은 그룹으로 묶인 상태를 해제합니다.\n\nby_sex_gender |> \n  summarise(n=n(), .groups = 'keep')\n\n\n\n  \n\n\nby_sex_gender |> \n  summarise(n=n(), .groups = 'drop')\n\n\n\n  \n\n\n\n\n\n2. select(), rename(), relocate()\nrename()과 relocate()는 group_by()와 관계없이 동일한 작업을 수행합니다. 각각 열의 이름과 위치에만 영향을 미치게 때문입니다.\nselect()가 group_by()와 함께 쓰이면 그룹 변수가 항상 함께 선택됩니다.\n\nby_species |> select(mass)\n\n\n\n  \n\n\n\n그룹 변수가 함께 선택되는 것을 원치 않는다면 ungroup()을 먼저 사용해야 합니다.\n\nby_species |> ungroup() |> select(mass)\n\n\n\n  \n\n\n\n\n\n3. arrange()\narrange()가 group_by()와 함께 쓰일 경우, 그냥 사용하게 되면 group_by() 가 없을 때와 동일합니다. 그러나 .by_group=T 인자를 통해 그룹 변수를 기준으로 정렬되어 결과가 출력됩니다.\n\nby_species |> \n  arrange(desc(mass)) |> \n  relocate(species, mass)\n\n\n\n  \n\n\nby_species |> \n  arrange(desc(mass), .by_group = T) |> \n  relocate(species, mass)\n\n\n\n  \n\n\n\n\n\n4. mutate(), transmute()\ngroup_by()와 함께 새로운 변수를 생성할 경우, 그룹 별로 동일한 값이 들어가게 됩니다.\n\nstarwars |> \n  group_by(species) |> \n  mutate(mean_height = mean(height, na.rm=T)) |> \n  arrange(desc(species)) |> \n  select(species, height, mean_height)\n\n\n\n  \n\n\n\n\n\n5. filter()\nfilter()와 group_by()를 함께 사용하여, 특정 그룹의 수를 기준으로 선택할 때 사용할 수 있습니다.\n아래의 예시에서는 그룹 별 수가 1인 경우를 모두 제외하고 선택한 결과입니다.\n\nby_species |> \n  filter(n()!=1) |> \n  tally()\n\n\n\n  \n\n\n\n\n\n6. slice()\nslice()와 slice() 계열 함수들이 group_by() 와 함께 사용될 경우, 그룹별로 해당되는 값이 출력됩니다.\n\n# 그룹별 첫번째 값 \nby_species |> \n  slice(1) \n\n\n\n  \n\n\nby_species |>\n  filter(!is.na(height)) |> \n  slice_min(height,n=2)"
  },
  {
    "objectID": "blog/posts/2023-01-26-dplyr_groupby/index.html#update-dplyr-1.1.0",
    "href": "blog/posts/2023-01-26-dplyr_groupby/index.html#update-dplyr-1.1.0",
    "title": "dplyr group_by()",
    "section": "Update: dplyr 1.1.0",
    "text": "Update: dplyr 1.1.0\ndplyr 1.1.0 업데이트 이후에는 group_by()를 사용하지 않아도 다른 함수들에서 그룹 별 계산을 수행할 수 있습니다. 바로 .by 인자를 통해서 말이죠.\n\nstarwars |> \n  mutate(bmi = mass/(height/100)^2) |> \n  summarise(mean_bmi = mean(bmi, na.rm=T),.by=species)"
  },
  {
    "objectID": "blog/posts/2023-02-13-dplyr_pivot/index.html",
    "href": "blog/posts/2023-02-13-dplyr_pivot/index.html",
    "title": "tidyr로 Pivoting하기",
    "section": "",
    "text": "이번 시간에는 데이터의 형태를 변환시키는 pivoting을 배워보도록 하겠습니다.\n오늘 사용할 패키지는 다음과 같습니다.\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\n\n\n\n\nNote\n\n\n\ntidyr의 ‘tidy’는 ’정돈하다’ 라는 뜻입니다. 데이터를 정돈하기 위해 사용하는 패키지라고 이해하시면 되겠습니다.\n\n\n\n1. Longer: 데이터 길게 늘이기\n\n1) 열 이름에 문자 포함된 경우\npivot_longer() 는 데이터 내의 행의 수를 늘리고, 열의 개수를 줄임으로서 데이터를 길게 만듭니다. 즉 여러 개의 열을 줄여 행으로 바꾸는 과정에서 자연스럽게 데이터의 형태가 길어지는 것을 의미합니다.\nrelig_income 은 tidyr 패키지에 포함되어 있는 데이터셋으로, 사람들의 종교와 연소득 데이터가 들어있습니다.\npivot_longer() 간단한 예시를 살펴보겠습니다.\n\nrelig_income |> \n  pivot_longer(!religion,\n               names_to=\"income\",\n               values_to = \"values\")\n\n\n\n  \n\n\n\npivot_longer()에서 사용된 인자는 다음과 같습니다.\n\nrelig_income: 형태를 변환시킬 데이터\ncols: 데이터 내에서 길게 변환시킬 열들\nnames_to: 열들이 길게 변환된 뒤, 새로운 열의 이름\nvalues_to: 길게 변환된 열들의 데이터가 갖게되는 새로운 열 이름\n\n정리하자면 cols에서는 기존 데이터셋에서 열들의 이름이 들어가는데, 이 열들은 names_to에서 정해준 income이라고 하는 하나의 열로 변환되고, cols에서 선택된 열들의 값들은 values_to에서 정해준 values라고 하는 하나의 열로 변환되는 것입니다.\n\n\n2) 열 이름에 숫자가 포함된 경우\n다음으로 billboard 데이터로 pivot_longer를 진행해보겠습니다. billboard 데이터에는 2000년도의 빌보드 순위 주(week) 단위로 들어있습니다.\n이 데이터를 길게 바꾸면 다음과 같습니다.\n\nbillboard |> \n  pivot_longer(\n    cols=starts_with('wk'),\n    names_to='week',\n    values_to='rank',\n    values_drop_na = T\n  )\n\n\n\n  \n\n\n\ncols에서 열들을 선택할 때, 규칙이 있는 열들은 dplyr의 select()에서 사용하던 starts_with() 등을 사용할 수 있습니다.\n또한 데이터를 길게 변환할 때, values_drop_na를 통해 NA인 데이터들은 제외하였습니다.\n데이터를 변환하니 wk~ 열들이 전부 week라는 하나의 열로 변환되었습니다. 그런데 week 열의 데이터는 전부 wk가 들어가있습니다. 몇 주차인지 표현하기 위해서는 숫자만 있어도 될 것 같아 보입니다.\n이럴 때, names_prefix 인자를 통해 wk 접두사(prefix)를 없애주도록 하겠습니다.\n이 때, names_prefix가 추가되어도, week 의 데이터 유형은 그대로 character입니다. week를 숫자형태로 변환해주려면 names_transform을 이용하면 됩니다.\n\nbillboard |> \n  pivot_longer(\n    cols=starts_with('wk'),\n    names_to='week',\n    names_prefix = 'wk',\n    names_transform = as.integer,\n    values_to='rank',\n    values_drop_na = T\n  )\n\n\n\n  \n\n\n\n\n\n3) 열들을 세부적으로 나누기\n다음으로는 좀더 까다로운 데이터를 길게 변환해보도록 하겠습니다. tidyr 의 who 데이터에는 new_sp_m014 부터 newrel_f65 까지, 네 종류의 값들이 열의 이름으로 들어가 있습니다.\n\nnew_ / new: 새로운 경우들을 값들을 나타냅니다.\nsp / rel / ep : 진단명\nm / f: 성별\n014 / 1524 / 1524 / 3544 / 4554 / 65 : 나이 범위\n\n이 값들은 규칙을 띈 채로 열 이름으로 되어있기 때문에, name_pattern을 이용해 열 이름에서 추출할 수 있습니다.\n\nwho |> \n  pivot_longer(\n    cols=new_sp_m014:newrel_f65,\n    names_to= c('diagnosis','gender','age'),\n    names_pattern = \"new_?(.*)_(.)(.*)\", # 정규표현식\n    values_to=\"count\"\n  )\n\n\n\n  \n\n\n\n\nhousehold <- tibble(\n  family = 1:5,\n  dob_child1 = c('1998-11-26','1996-06-22','2002-07-11','2004-10-10','2000-12-05'),\n  dob_child2 = c('2000-01-29',NA,'2004-04-05','2009-08-27','2005-02-28'),\n  name_child1 = c('Susan','Mark','Sam','Craig','Parker'),\n  name_child2 = c('Jose',NA,'Seth','Khai','Gracie')\n)\n\nhousehold |> \n  pivot_longer(\n    cols = !family,\n    names_to=c('.value',\"child\"),\n    names_sep=\"_\",\n    values_drop_na = T,\n  )\n\n\n\n  \n\n\n\n\n\n\n2. Wider: 데이터 넓게 펼치기\n\n1) Capture-recapture\n\nfish_encounters |> \n  pivot_wider(\n    names_from = station,\n    values_from = seen\n  )\n\n\n\n  \n\n\n\n이 데이터에서 결측 값 NA 는 0과 같습니다. 다시 말해, 물고기가 발견된 적이 없다는 것이죠. 그렇기 때문에 NA를 0으로 채워넣을 수 있습니다.\n\nfish_encounters |> \n  pivot_wider(\n    names_from = station,\n    values_from = seen,\n    values_fill = 0\n  )\n\n\n\n  \n\n\n\n\n\n2) Aggregation\n\nwarpbreaks |> \n  pivot_wider(\n    names_from = c(wool),\n    values_from = breaks,\n    values_fn = mean\n  )"
  },
  {
    "objectID": "blog/posts/mlr3_basic/index.html",
    "href": "blog/posts/mlr3_basic/index.html",
    "title": "mlr3 기초",
    "section": "",
    "text": "Important\n\n\n\n이 글은 mlr3book1을 참고하여 작성되었습니다. 국내 R 사용자들에게 잘 알려지지 않은 mlr32 패키지를 통해, R에서도 손쉽게 머신러닝을 수행할 수 있다는 것을 보여드리고자 합니다."
  },
  {
    "objectID": "blog/posts/mlr3_basic/index.html#r6",
    "href": "blog/posts/mlr3_basic/index.html#r6",
    "title": "mlr3 기초",
    "section": "R6",
    "text": "R6\nR6는 객체지향 프로그래밍 (OOP)을 위한 R의 최근 패러다임 중 하나입니다. R6는 S3와 같이 R에 존재하던 기존의 객체지향성의 단점을 해결하는것이 특징입니다. 아마 다른 프로그래밍 언어에서 객체 지향 개념을 다루어 보셨다면, R6가 더 익숙하게 느껴질 것입니다.\nR6에서 객체(object)는 R6Class() 생성자 객체와 더불어 $new()메소드를 통해 생성됩니다.\n예를 들어 보겠습니다.\n\nlibrary(R6)\nFoo = R6Class()\nfoo = Foo$new()\n\n이 객체들은 자신들의 필드 안에서 변환 가능하도록 압축된 상태를 띄고 있는데, 우리는 이 객체들에 $ 기호를 통해 접근할 수 있습니다.\n필드 뿐만 아니라, 객체들이 갖고 있는 메소드를 통해 각 객체의 상태를 파악하고 정보를 검색하거나, 객체의 내부 상태를 변경할 수 있습니다. 예를 들어 mlr3 의 학습모델(learner) 의 $train() 메소드를 통해, 모델을 학습된 상태로 변경할 수 있고, 이를 통해 예측을 할 수 있게 됩니다.\n\n\n\n\n\n\nNote\n\n\n\nR6 객체의 내부 요소는 다음과 같이 부릅니다.\n\n$field: 필드, 정보\n$method(): 메소드, 특정 동작 실행\n\n\n\nR6 객체들은 각각의 환경(environment)로서, 참조 특성 (reference semantics)을 갖습니다. 예를 들어, foo2 = foo 를 실행할 시, foo2 는 foo가 복사된 것은 아니지만, 하나의 객체를 참조하고 있는 것입니다. 따라서, foo$bar = 3 을 실행할 시, foo2$bar 역시 3이 됩니다.\n객체를 복사할 경우 $clone() 메소드와 deep = TRUE 인자를 사용해야 합니다.\n\nfoo2 = foo$clone(deep=TRUE)\n\n\nlibrary(mlr3verse)\n\n\n\n\n\n\n\nTip\n\n\n\nR6에 대해 더 자세히 알고 싶다면, R6 vignettes, 특히 introduction부분을 참고하세요. 포괄적인 R6의 정보를 얻고 싶다면, Advanced R의 R6 챕터를 참고하세요.\n\n\n\n\nmlr3의 필수 활용요소\n\nSugar functions\n대부분의 mlr3 객체들은 sugar function으로 불리는 간편한 함수들을 제공합니다. 다시 말해, sugar function은 원래의 코드에 대한 단축키(shortcut)로서 사용자가 입력해야 하는 코드를 줄여줍니다. 예를 들어 lrn(\"regr.rpart\") 는 LearnerRegrRpart$new() 의 sugar 버전입니다.\n\n\nDictionaries\nmlr3는 러너(learners)나 태스크(tasks) 객체들을 저장하기 위해 dictionary 구조를 사용합니다. dictionary 구조는 key와 value로 이루어져 있어 key와 value를 연관시켜주는데, 이는 실제 사전의 단어와 단어의 설명과 같다고 이해하시면 됩니다.\ndictionary는 연관된 객체들을 묶어 나열하고 검색하기 쉽게 하기 위해 사용됩니다. 예를 들어 특정 학습모델을 검색할 때, mlr_learners dictionary에 원하는 러너(key)를 입력하면 검색이 가능합니다.\n예를 들면 다음과 같습니다.\n\nrequire(mlr3)\nmlr_learners$get('classif.rpart')\n\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\n또한 as.data.table(mlr3_learners) 모든 학습 모델의 정보를 확인할 수도 있습니다.\n\n\nmlr3viz\nmlr3viz는 mlr3 생태계 안에서 시각화를 담당하는 패키지입니다. ggplot2의 theme_minimal()을 적용시킨 동일한 배경의 그래프들을 생성합니다. mlr3viz는 ggplot2 를 기반으로 하고 있으며, fortify와 autoplot 라는 확장 패키지를 통해 예측, 학습모델, 벤치마크 객체 등 mlr3의 결과물들을 시각화하는 데 사용됩니다. mlr3viz에서 가장 많이 사용되는 것은 autoplot()으로, 객체의 타입에 따라 그래프의 출력 결과가 결정됩니다."
  },
  {
    "objectID": "blog/posts/mlr3_basic/index.html#tasks",
    "href": "blog/posts/mlr3_basic/index.html#tasks",
    "title": "mlr3 기초",
    "section": "Tasks",
    "text": "Tasks\n태스크(task)은 데이터와 머신러닝 문제들을 정의한 메타데이터를 갖고 있는 객체입니다. 예를 들면 머신러닝의 분류에서 타겟 피처의 이름이 메타 데이터 입니다.\n한마디로, 태스크는 우리가 활용하는 기본 데이터와 머신러닝을 위해 필요한 데이터들을 담아둔 정보 등이 포함된 객체입니다.\n이 메타 데이터는 사용자가 모덜이 학습될 때 예측 타겟을 다시 지정해줄 필요 없도록 태스크와 함께 작동하게 됩니다.\n\n\n\n\n\n\nNote\n\n\n\nTask와 Learner 등 영어로 지정된 단어들은 따로 번역하지 않고, 소리 그대로 “태스크”와 “러너”로 부르겠습니다.\n\n\n\n내장 태스크\nmlr3에는 mlr_tasks라고 하는 R6 Dictionary 를 통해 미리 지정된 머신러닝 태스크를 제공하고 있습니다.\n\nmlr_tasks\n\n<DictionaryTask> with 20 stored values\nKeys: bike_sharing, boston_housing, breast_cancer, german_credit, ilpd,\n  iris, kc_housing, moneyball, mtcars, optdigits, penguins,\n  penguins_simple, pima, ruspini, sonar, spam, titanic, usarrests,\n  wine, zoo\n\n\nmlr_tasks에 내장된 태스크를 가져오기 위해선, tsk() 함수와 불러오고자 하는 태스크의 이름을 입력하면 됩니다.\n\ntask_mtcars <- tsk(\"mtcars\")\ntask_mtcars\n\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\n특정 태스크에 대해 보다 많은 정보가 필요하다면, help()메소드를 이용하시면 됩니다.\n\ntask_mtcars$help()\n\n\n\n\n\n\n\nTip\n\n\n\nR을 사용하셨던 분들이라면 위와 같은 구조의 코드가 낯설 것입니다. 보통 도움말을 보기 위해선 help()나 ?를 이용했을테니까요.\ntask_mtcars$help() 와 같은 구조가 바로 R6 객체 구조입니다. Introduction에서 간단히 소개했었죠? 앞으로 mlr3를 배우며, 계속 사용하게 될 것입니다.\n\n\n\ntask_sonar <- tsk('sonar')\nsplit <- partition(task_sonar, ratio=.7)\n\n\n\n외부 데이터 태스크로 변환\nmlr3에서 제공하는 데이터가 아닌, 외부의 데이터셋을 mlr3 패키지와 사용하려면 아래와 같이 데이터셋을 태스크로 변환해야 합니다. 예를 들어, survival 패키지의 lung 데이터셋을 mlr3의 태스크 객체로 변환한다면, 아래와 같이 진행할 수 있습니다.\n\nlibrary(survival)\nlibrary(data.table)\ntask_lung = as_task_classif(\n  as.data.table(lung)[,.(age, sex, wt.loss, ph.ecog,status)],\n             target = \"status\",\n  \n             id = \"lung\")\ntask_lung\n\n<TaskClassif:lung> (228 x 5)\n* Target: status\n* Properties: twoclass\n* Features (4):\n  - dbl (4): age, ph.ecog, sex, wt.loss\n\n\n\n\n\n\n\n\nTip\n\n\n\n태스크 생성자인 as_task_regr()과 as_task_classif()는 각각 머신러닝의 회귀와 분류를 수행하기 위한 태스크를 만드는 함수입니다.\n외부의 데이터를 태스크로 변환할 때, UTF8 이름을 따르지 않는 경우, 머신러닝 학습과정에서 오류가 발생합니다. 예를 들면,\nTask 'lung' has missing values in column(s) 'ph.ecog', 'wt.loss', but learner 'classif.ranger' does not support this\n따라서 make.names() 함수를 이용해 데이터의 열 이름을 변경하는 것을 권장합니다.\n\n\n\nlibrary(mlr3viz)\nautoplot(task_lung, type = \"pairs\")\n\n\n\n\n\n\n데이터 살펴보기\n태스크 객체는 테이블 형태의 데이터와 함께, 메타 데이터를 포함하고 있습니다. 예를 들면 행과 열의 개수, 피처(feature) 변수, 타겟 변수와 각 변수의 데이터유형 등을 확인할 수 있습니다.\n이런 메타 데이터들은 field를 통해 확인이 가능합니다.\n\ntask_lung$nrow\n\n[1] 228\n\n\n\ntask_lung$ncol\n\n[1] 5\n\n\n피처와 타겟변수의 이름은 각각 $feature_names 와 $target_names 에 저장되어 있습니다. 여기서 target 은 머신러닝을 통해 예측하고자 하는 변수를 의미합니다.\n\ntask_lung$feature_names\n\n[1] \"age\"     \"ph.ecog\" \"sex\"     \"wt.loss\"\n\n\n\ntask_lung$target_names\n\n[1] \"status\"\n\n\n한편 태스크 안에 들어있는 데이터는 data.table 객체로, $data() 메소드를 통해 확인할 수 있습니다.\n\ntask_lung$data()\n\n\n\n  \n\n\n\n$data() 메소드 안에서 rows와 cols를 통해 원하는 데이터를 확인할 수 있습니다.\n\ntask_lung$data(rows=1:10, cols=\"status\")\n\n\n\n  \n\n\n\n태스크를 data.table 객체로 바꾼다면, 데이터에 있는 모든 변수를 확인할 수도 있습니다.\n\nsummary(as.data.table(task_lung))\n\n status       age           ph.ecog            sex           wt.loss       \n 1: 63   Min.   :39.00   Min.   :0.0000   Min.   :1.000   Min.   :-24.000  \n 2:165   1st Qu.:56.00   1st Qu.:0.0000   1st Qu.:1.000   1st Qu.:  0.000  \n         Median :63.00   Median :1.0000   Median :1.000   Median :  7.000  \n         Mean   :62.45   Mean   :0.9515   Mean   :1.395   Mean   :  9.832  \n         3rd Qu.:69.00   3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.: 15.750  \n         Max.   :82.00   Max.   :3.0000   Max.   :2.000   Max.   : 68.000  \n                         NA's   :1                        NA's   :14       \n\n\n\n\n태스크 변환자(Mutators)\n머신러닝을 수행하며, 행과 열들을 선택하는 경우가 종종 있습니다. 예를 들면 train-test split을 위해 행을 선택하는 경우, 모델링에 넣을 피처들을 선택하는 경우가 있겠죠.\nmlr3의 태스크는 행을 선택하는 $filter(), 열을 선택하는 $select() 를 이용해 원하는 조건의 데이터를 추출할 수 있습니다.\n한 가지 주의해야 할 것이 있습니다. $select()와 $filter()는 변환자이기 때문에, 기존의 태스크를 수정하게 됩니다. 다시 말해, 처음에 있던 원래 데이터를 바꿔버리는 것이죠.\n\ntask_iris <- tsk(\"iris\")\ntask_iris$select(c(\"Sepal.Length\",\"Petal.Width\"))\ntask_iris$filter(2:4)\ntask_iris$data()\n\n\n\n  \n\n\n\n이를 방지하기 위해서는 $clone() 메소드를 이용해 새로운 태스크로 복사한 뒤에 행이나 열을 선택하는 작업하시면 됩니다.\n\ntask_iris_copy <- task_iris$clone()\ntask_iris_copy$filter(2)\ntask_iris_copy$data()\n\n\n\n  \n\n\n\n\ntask_iris$data()\n\n\n\n  \n\n\n\ntask_iris_copy는 task_iris를 복사한 뒤, 2번째 행을 $filter() 했지만, task_iris의 데이터는 아무 변화가 없는 것을 확인할 수 있습니다."
  },
  {
    "objectID": "blog/posts/mlr3_basic/index.html#learner",
    "href": "blog/posts/mlr3_basic/index.html#learner",
    "title": "mlr3 기초",
    "section": "Learner",
    "text": "Learner\n러너는 클래스는 널리 알려진 많은 머신러닝 패키지들을 통일된 형태로 제공합니다. 태스크와 마찬가지로 mlr_learners dictionary를 통해 확인할 수 있습니다.\n러너는 머신러닝 모델을 학습(train)하고 예측하는 역할을 수행합니다. 태스크에서는 외부 데이터도 태스크로 만들 수 있었던 것과 달리, 러너는 mlr3에서 지원하는 것만 사용할 수 있습니다. mlr3에서 제공하는 러너들은 다음과 같습니다.\n\nas.data.table(mlr_learners) |> head()\n\n\n\n  \n\n\n\n러너의 기본 형태는 Learner입니다. Learner로 시작하는 다양한 러너들이 존재합니다. mlr3의 러너들은 mlr3learners와 mlr3extralearners패키지를 통해 확인 가능합니다.\n\nmlr3learners::LearnerClassifLogReg\n\n<LearnerClassifLogReg> object generator\n  Inherits from: <LearnerClassif>\n  Public:\n    initialize: function () \n    loglik: function () \n    clone: function (deep = FALSE) \n  Private:\n    .train: function (task) \n    .predict: function (task) \n  Parent env: <environment: namespace:mlr3learners>\n  Locked objects: TRUE\n  Locked class: FALSE\n  Portable: TRUE\n\n\n러너의 sugar function은 lrn() 입니다.\n\n\n\n\n\n\nDanger\n\n\n\nmlr3의 러너를 실행하기 위한 패키지가 없다면 아래와 같은 경고 메시지가 뜹니다.\nWarning: Package 'ranger' required but not installed for Learner 'classif.ranger\n패키지를 설치해주시면 간단히 해결됩니다.\n\ninstall.packages(\"ranger\")\n\n\n\n\nlearner_rf <- lrn('classif.ranger', predict_type='prob')\nlearner_rf\n\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n각 러너들은 아래의 메타데이터를 갖고 있습니다.\n\n$feature_types: 피처들의 유형\n$packages: 모델을 학습시키고 예측하기 위해 필요한 패키지들\n$properties: 해당 러너가 갖고 있는 추가적인 특성. 예를 들어 importance 특성이 있다면 학습 후, 각 피처들의 importance를 추출할 수 있습니다.\n$predict_types: 해당 러너를 활용해 가능한 예측 유형입니다. 분류 유형의 러너는 response와 prob을 출력합니다.\n\n모든 러너들은 두 단계에 걸쳐 진행됩니다.\n\n학습 (Training): $train() 메소드를 통해 학습시키고자 하는 태스크를 전달합니다.\n예측 (Prediction): $predict() 메소드에 학습 때 사용하지 않은 데이터를 사용합니다. 학습 데이터를 기반으로 훈련된 모델이 새로운 데이터를 받아 예측값을 반환합니다.\n\n\n\n\n\n\n\nWarning\n\n\n\n러너가 학습되지 않았다면 $predict() 실행 시 에러가 발생합니다.\n\n\n\ntrain: 학습시키기\n앞서 말했듯이, mlr3에서는 러너에 태스크를 투입하여 모델을 학습시킵니다. 머신러닝에서는 태스크를 투입하기 전, 훈련에 사용할 데이터와 예측에 사용할 데이터를 나누는 것이 일반적입니다.\nmlr3 에서는 $partition() 메소드를 이용해 두 개의 데이터로 나눌 수 있습니다. 기본값은 전체 데이터의 67%를 훈련에, 나머지 33%를 예측에 사용합니다. 물론 이 비율은 ratio 인자 (범위: 0~1)를 통해 변경 가능합니다.\n\ntask_breast <- tsk(\"breast_cancer\")\nsplits <- partition(task_breast, ratio = 0.7)\n\n데이터를 나눠줬으니, 이제 모델을 학습시켜보도록 하겠습니다. 위에서 선언한 랜덤포레스트 러너에서 $train() 메소드를 실행시킵니다. 태스크와 row_ids에는 split 중 train에 해당하는 부분을 입력합니다.\n\nlearner_rf$train(task_breast, \n                 row_ids = split$train)\nlearner_rf$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      146 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.03578763 \n\n\n학습을 시킨 이후, $model 필드를 통해 학습된 결과를 확인할 수 있습니다. 어떤 하이퍼파라미터가 사용되었는지 등의 정보를 확인할 수 있습니다.\n각 러너들은 하이퍼파라미터들을 조정해줄 수 있습니다. 러너들의 파라미터를 확인하는 명령어는 다음과 같습니다.\n\nlearner_rf$param_set\n\n\n\n\n\n  \n\n\n\n현재 학습에 사용된 하이퍼 파라미터들은 values 필드에 저장되어있습니다.\n\nlearner_rf$param_set$values\n\n$num.threads\n[1] 1\n\n\n위와 같이 하이퍼 파라미터에 접근한 뒤, 기존의 값을 원하는 값으로 변경하거나, 새로운 하이퍼파라미터에 값을 설정해줄 수 있습니다.\n\nlearner_rf$param_set$values$num.threads = 10\nlearner_rf$param_set$values$num.trees = 20\nlearner_rf$param_set$values\n\n$num.threads\n[1] 10\n\n$num.trees\n[1] 20\n\n\n또는 lrn() 함수로 러너를 생성할 때, 원하는 하이퍼파라미터를 설정해줄 수 있습니다.\n\nlearner_rf <- lrn('classif.ranger', num.trees= 30)\nlearner_rf$param_set$values\n\n$num.threads\n[1] 1\n\n$num.trees\n[1] 30\n\n\n\nlearner_rf$train(task_breast, row_ids = split$train)\nlearner_rf\n\n<LearnerClassifRanger:classif.ranger>\n* Model: ranger\n* Parameters: num.threads=1, num.trees=30\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n\n\n변경된 파라미터가 적용된 것을 확인할 수 있습니다.\n\n\n\n\n\n\nNote\n\n\n\n하이퍼 파라미터에 대한 자세한 설명은 하이퍼파라미터 튜닝에서 더 자세히 다루도록 하겠습니다.\n\n\n\n\npredict: 예측하기\n모델 학습이 완료되었다면, 예측값을 만들어볼 수 있습니다. split의 test 를 이용해 $predict() 메소드를 실행합니다.\n\nprediction <- learner_rf$predict(task_breast, \n                                 row_ids = split$test)\nprediction\n\n<PredictionClassif> for 62 observations:\n    row_ids     truth  response\n          3    benign    benign\n          6 malignant malignant\n          9    benign    benign\n---                            \n        204    benign    benign\n        206 malignant malignant\n        207    benign    benign\n\n\n예측값을 살펴보니, 예측값이 범주로 나타났습니다. 만약 범주에 대한 예측 확률값을 구하고 싶을 경우, lrn()의 predict_type을 prob으로 조정해주면 됩니다.\n\nlearner_rf <- lrn(\"classif.ranger\", \n                  predict_type=\"prob\")\nlearner_rf$train(task_breast, \n                 row_ids = split$train)\nprediction <- learner_rf$predict(task_breast, \n                                 row_ids = split$test)\nprediction\n\n<PredictionClassif> for 62 observations:\n    row_ids     truth  response prob.malignant prob.benign\n          3    benign    benign     0.00000000 1.000000000\n          6 malignant malignant     0.99750556 0.002494444\n          9    benign    benign     0.07133333 0.928666667\n---                                                       \n        204    benign    benign     0.01348889 0.986511111\n        206 malignant malignant     0.94362302 0.056376984\n        207    benign    benign     0.00000000 1.000000000\n\n\nprediction 객체에서는 $confusion 을 이용해 confusion matrix를 확인할 수 있습니다.\n\nprediction$confusion\n\n           truth\nresponse    malignant benign\n  malignant        27      1\n  benign            0     34\n\n\n우리가 만든 모델이 내놓은 예측값에 대한 시각화를 진행해줄 수 있습니다. mlr3viz 패키지의 autoplot() 기능을 이용해주면 됩니다.\nautoplot()에서 type을 어떻게 지정해주는지에 따라 예측값에 시각화가 다르게 나타납니다.\n\nlibrary(mlr3viz)\nautoplot(prediction)\n\n\n\n\n위의 그림은 test 데이터의 실제 target 값과 머신러닝 모델이 예측한 target을 비교한 값입니다.\n그 외에도 분류 계열의 모델에는 AUROC(type=\"roc\" )와 AUPRC(type=\"prc\") 등을 시각화할 수 있습니다.\n\n# install.packages(\"precrec\")\nautoplot(prediction, type=\"roc\")\nautoplot(prediction, type=\"prc\")\n\n\n\n\n\n\nAUROC\n\n\n\n\n \n\n\n\n\n\nAUPRC\n\n\n\n\n\n\n\n\nEvaluation: 성능 평가\n성능 평가는 머신러닝 모델링 과정에서 중요한 단계입니다. 앞서 예측 부분에서 예측값을 시각화로 표현했습니다. 그와 더불에 mlr3에서는 msr() 함수를 이용해 다양한 성능 지표들을 계산하고 비교할 수 있습니다. mlr_measures 딕셔너리를 보면 계산 가능한 지표들이 나와있습니다.\n예를 들어, 분류(classification) 모델에 대한 지표들을 살펴보면 다음과 같습니다.\n\nmlr_measures$keys(\"classif\")\n\n [1] \"classif.acc\"         \"classif.auc\"         \"classif.bacc\"       \n [4] \"classif.bbrier\"      \"classif.ce\"          \"classif.costs\"      \n [7] \"classif.dor\"         \"classif.fbeta\"       \"classif.fdr\"        \n[10] \"classif.fn\"          \"classif.fnr\"         \"classif.fomr\"       \n[13] \"classif.fp\"          \"classif.fpr\"         \"classif.logloss\"    \n[16] \"classif.mauc_au1p\"   \"classif.mauc_au1u\"   \"classif.mauc_aunp\"  \n[19] \"classif.mauc_aunu\"   \"classif.mbrier\"      \"classif.mcc\"        \n[22] \"classif.npv\"         \"classif.ppv\"         \"classif.prauc\"      \n[25] \"classif.precision\"   \"classif.recall\"      \"classif.sensitivity\"\n[28] \"classif.specificity\" \"classif.tn\"          \"classif.tnr\"        \n[31] \"classif.tp\"          \"classif.tpr\"         \"debug_classif\"      \n\n\n모델의 성능을 계산하기 위한 지표는 한 가지만 사용할 수 있고, 여러 가지를 사용할수도 있습니다.\n\nmeasure <- msr(\"classif.auc\")\nmeasures <- msrs(c(\"classif.auc\",\"classif.auc\"))\n\n모든 성능 지표들은 예측값과 test 데이터의 실제 값의 차이를 통해 수치화된 값입니다. 이 말은 모델의 성능을 평가하기 위해선, 예측값을 비교하기 위한, 모델 학습에 사용되지 않은 실제값이 필요하다는 것입니다.\n이제, Measure 객체 중 하나인 classif.acc를 통해 위에서 만든 랜덤포레스트 모델의 성능을 평가해보도록 하겠습니다. 지표(measure)를 생성한 뒤, 모델의 예측값 객체(prediction)의 $score() 메소드에 넘겨주면 해당 지표의 성능이 평가됩니다.\n\nmeasure <- msr(\"classif.acc\")\nmeasure\n\n<MeasureClassifSimple:classif.acc>: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n\n\n\nprediction$score(measure)\n\nclassif.acc \n   0.983871 \n\n\n\n\n\n\n\n\nNote\n\n\n\n$score()는 measure 없이 사용될 수 있습니다. 이럴 경우, 분류의 경우 기본값인 분류오차(classif.ce)가, 회귀는 평균제곱오차(regr.mse)가 적용됩니다.\n\n\n$score()를 이용해 여러 종류의 성능 지표를 한 번에 계산하는 것도 가능합니다. 예를 들어, 민감도(classif.sensitivity)와 특이도(classif.specificity)를 한번에 계산할 경우,\n\nmeasures <- msrs(c(\"classif.sensitivity\",\"classif.specificity\"))\n\nprediction$score(measures)\n\nclassif.sensitivity classif.specificity \n          1.0000000           0.9714286 \n\n\n뿐만 아니라, mlr3에서는 예측 모델의 품질을 정량화하는 지표로도 모델을 평가할 수 있습니다. 예를 들어, 학습하는 시간과 예측하는 시간을 평가하기 위해선\n\nmeasures <- msrs(c(\"time_train\", \"time_predict\"))\nprediction$score(measures,learner = learner_rf)\n\n  time_train time_predict \n       0.015        0.006 \n\n\n참고로, 위처럼 학습시간과 예측 시간을 평가하기 위해선 학습된 러너를 $score()에 입력해주어야 합니다.\n한편, 일부 지표들은 스스로 하이퍼파라미터를 가지고 있습니다. 대표적인 예로 selected_features 지표가 있습니다. 이 지표는 “selected_features” 속성이 있는 러너에만 사용 가능한 지표로서, 모델이 학습 시 사용한 피처에 대한 정보를 제공해줍니다. 이런 지표의 경우, $score() 에 태스크와 러너를 추가로 입력해주어야 합니다.\n\ntask_boston <- tsk(\"boston_housing\")\nsplits <- partition(task_boston)\nlearner_rpart <- lrn(\"regr.rpart\")\n\nlearner_rpart$train(task_boston, splits$train)\nprediction <- learner_rpart$predict(task_boston, splits$test)\nmeasure <- msr(\"selected_features\")\nprediction$score(measure, task = task_boston, learner = learner_rpart)\n\nselected_features \n                1 \n\n\nselected_features 는 선택된 피처의 수를 정규화해줄 수 있는 하이퍼파라미터를 설정할 수 있습니다.\n\nmeasure <- msr(\"selected_features\", normalize=T)\nprediction$score(measure, task=task_boston, learner = learner_rpart)\n\nselected_features \n       0.05555556"
  },
  {
    "objectID": "blog/posts/mlr3_basic/index.html#레퍼런스",
    "href": "blog/posts/mlr3_basic/index.html#레퍼런스",
    "title": "mlr3 기초",
    "section": "레퍼런스",
    "text": "레퍼런스\n\nhttps://mlr3.mlr-org.com/\nhttps://mlr3book.mlr-org.com/basics.html"
  },
  {
    "objectID": "blog/posts/gtsummary/index.html",
    "href": "blog/posts/gtsummary/index.html",
    "title": "통계 테이블 끝판왕 gtsummary",
    "section": "",
    "text": "데이터 요약 & 회귀분석 결과 테이블 만들기"
  },
  {
    "objectID": "blog/posts/gtsummary/index.html#필요한-패키지-설치",
    "href": "blog/posts/gtsummary/index.html#필요한-패키지-설치",
    "title": "통계 테이블 끝판왕 gtsummary",
    "section": "필요한 패키지 설치",
    "text": "필요한 패키지 설치\n\nlibrary(gtsummary)\nlibrary(gt)"
  },
  {
    "objectID": "blog/posts/gtsummary/index.html#baseline-characteristics",
    "href": "blog/posts/gtsummary/index.html#baseline-characteristics",
    "title": "통계 테이블 끝판왕 gtsummary",
    "section": "1. Baseline characteristics",
    "text": "1. Baseline characteristics\n\ndata의 일반 특성 요약\n집단 별 통계량 표시\n\n\n기본 테이블\n\nlibrary(gtsummary)\nlibrary(gt)\nlibrary(survival)\nlibrary(flextable)\ntbl_summary(\n  data = trial,\n  by = trt\n) |> \n  add_overall() |> \n  as_flex_table()\n\n\nCharacteristicOverall, N = 2001Drug A, N = 981Drug B, N = 1021Age47 (38, 57)46 (37, 59)48 (39, 56)Unknown1174Marker Level (ng/mL)0.64 (0.22, 1.39)0.84 (0.24, 1.57)0.52 (0.19, 1.20)Unknown1064T StageT153 (26%)28 (29%)25 (25%)T254 (27%)25 (26%)29 (28%)T343 (22%)22 (22%)21 (21%)T450 (25%)23 (23%)27 (26%)GradeI68 (34%)35 (36%)33 (32%)II68 (34%)32 (33%)36 (35%)III64 (32%)31 (32%)33 (32%)Tumor Response61 (32%)28 (29%)33 (34%)Unknown734Patient Died112 (56%)52 (53%)60 (59%)Months to Death/Censor22.4 (16.0, 24.0)23.5 (17.4, 24.0)21.2 (14.6, 24.0)1Median (IQR); n (%)\n\n\n\n\n통계량 변경\n테이블에 표시되는 숫자의 양식을 변경해줄 수 있습니다. 예를 들어, 연속형(continuous) 변수의 경우 기본적으로 median(IQR)로 표시가 됩니다. tbl_summary()의 statistic 인자를 통해 mean ± sd 형태로 표시할 수 있습니다.\n주의해야 할 점은 항상 {}가 붙은 문자 형태(\"\")로 사용해야 한다는 것입니다.\n\ntbl_summary(\n  data = trial,\n  by = trt,\n  include = c(age, marker, stage, grade, response, death, ttdeath),\n  type = list(\n    c(stage, grade)~ \"categorical\"\n  ),\n  statistic = list(\n    all_continuous() ~\"{mean} ± {sd}\",\n    all_categorical() ~ \"{n} ({p})\"\n  ),\n  digits = list(\n    all_continuous() ~ 1,\n    all_categorical() ~ c(0,1)\n  ),\n  missing = \"no\",\n) |> \n  add_overall() |> \n  as_flex_table()\n\n\nCharacteristicOverall, N = 2001Drug A, N = 981Drug B, N = 1021Age47.2 ± 14.347.0 ± 14.747.4 ± 14.0Marker Level (ng/mL)0.9 ± 0.91.0 ± 0.90.8 ± 0.8T StageT153 (26.5)28 (28.6)25 (24.5)T254 (27.0)25 (25.5)29 (28.4)T343 (21.5)22 (22.4)21 (20.6)T450 (25.0)23 (23.5)27 (26.5)GradeI68 (34.0)35 (35.7)33 (32.4)II68 (34.0)32 (32.7)36 (35.3)III64 (32.0)31 (31.6)33 (32.4)Tumor Response61 (31.6)28 (29.5)33 (33.7)Patient Died112 (56.0)52 (53.1)60 (58.8)Months to Death/Censor19.6 ± 5.320.2 ± 5.019.0 ± 5.51Mean ± SD; n (%)\n\n\n\n\nP-value\n\ntbl_summary(\n   data = trial,\n  by = trt,\n  include = c(age, marker, stage, grade, response, death, ttdeath),\n  statistic = list(\n    all_continuous() ~\"{mean} ± {sd}\",\n    # statistic = \"{median} ({p25}-{p75})\",\n    all_categorical() ~ \"{n} ({p})\"\n  ),\n  digits = list(\n    all_continuous() ~ 1,\n    all_categorical() ~ c(0,1)\n  ),\n  missing = \"no\",\n) |> \n  add_overall() |> \n  add_p(\n    test = list(\n      all_continuous() ~ \"t.test\",\n      all_categorical() ~ \"chisq.test\"\n    ),\n    pvalue_fun = ~style_pvalue(., digits = 3)\n  ) |> \n    as_flex_table()\n\n\nCharacteristicOverall, N = 2001Drug A, N = 981Drug B, N = 1021p-value2Age47.2 ± 14.347.0 ± 14.747.4 ± 14.00.834Marker Level (ng/mL)0.9 ± 0.91.0 ± 0.90.8 ± 0.80.116T Stage0.866T153 (26.5)28 (28.6)25 (24.5)T254 (27.0)25 (25.5)29 (28.4)T343 (21.5)22 (22.4)21 (20.6)T450 (25.0)23 (23.5)27 (26.5)Grade0.871I68 (34.0)35 (35.7)33 (32.4)II68 (34.0)32 (32.7)36 (35.3)III64 (32.0)31 (31.6)33 (32.4)Tumor Response61 (31.6)28 (29.5)33 (33.7)0.637Patient Died112 (56.0)52 (53.1)60 (58.8)0.498Months to Death/Censor19.6 ± 5.320.2 ± 5.019.0 ± 5.50.1081Mean ± SD; n (%)2Welch Two Sample t-test; Pearson's Chi-squared test"
  },
  {
    "objectID": "blog/posts/gtsummary/index.html#regression-table-만들기",
    "href": "blog/posts/gtsummary/index.html#regression-table-만들기",
    "title": "통계 테이블 끝판왕 gtsummary",
    "section": "2. Regression table 만들기",
    "text": "2. Regression table 만들기\n\n단순 회귀분석(Univariable regression)\n단순회귀분석의 경우,\n\nlibrary(survival)\nuni_tbl <- tbl_uvregression(\n  data = trial |> select(age, marker, stage, grade, response, death, ttdeath),\n  method = coxph,\n  y = Surv(ttdeath, death),\n  exponentiate = T,\n  hide_n = T\n  ) |> \n  modify_footnote(everything() ~ NA)\nuni_tbl |> as_flex_table()\n\n\nCharacteristicHR195% CI1p-valueAge1.010.99, 1.020.3Marker Level (ng/mL)0.910.72, 1.150.4T StageT1——T21.180.68, 2.040.6T31.230.69, 2.200.5T42.481.49, 4.14<0.001GradeI——II1.280.80, 2.050.3III1.691.07, 2.660.024Tumor Response0.500.31, 0.780.0031HR = Hazard Ratio, CI = Confidence Interval\n\n\n\n\n다변량 회귀분석(Multivariable regression)\n다중 회귀분석 테이블을 만드는 함수는 tbl_regression()입니다. 단순 회귀분석을 만드는 tbl_uvregression()과 다르게, 먼저 회귀식을 작성한 뒤에 tbl_regression()의 인자로 사용해야 합니다.\n\nfit <- coxph(Surv(ttdeath, death) ~ age +  marker + stage + grade + response, data = trial)\nmult_tbl <- tbl_regression(\n  x = fit,\n  exponentiate = T,\n  pvalue_fun = ~ style_pvalue(., digits = 3),\n  estimate_fun = ~style_ratio(., digits = 2)\n) |> \n  bold_p() |> \n  # modify_column_merge(pattern = \"{estimate} ({conf.low}-{conf.high})\",\n  #                     rows = !is.na(estimate)) |> \n  add_significance_stars(hide_ci = T, hide_se = T) |> \n  modify_header(\n    label = \"**Variable**\",\n    estimate = \"**OR (95% CI)**\",\n    p.value = \"**P value**\"\n  ) |> \n  modify_footnote(everything() ~ NA)\nmult_tbl |> as_flex_table()\n\n\nVariableOR (95% CI)1P valueAge1.02*0.023Marker Level (ng/mL)0.890.398T StageT1—T21.330.376T31.600.183T43.65***<0.001GradeI—II1.270.395III1.75*0.025Tumor Response0.41***<0.0011HR = Hazard Ratio\n\n\n\n\n두 개의 테이블 합치기\n이제 단순 회귀분석 테이블과 다중 회귀분석 테이블을 합쳐보도록 하겠습니다.\n\ntbl_merge(\n  list(uni_tbl, mult_tbl),\n  tab_spanner = \n    c(\"**Univariable**\", \"**Multivariable**\")\n) |> \n  as_flex_table()\n\n\n UnivariableMultivariableCharacteristicHR195% CI1p-valueOR (95% CI)1P valueAge1.010.99, 1.020.31.02*0.023Marker Level (ng/mL)0.910.72, 1.150.40.890.398T StageT1———T21.180.68, 2.040.61.330.376T31.230.69, 2.200.51.600.183T42.481.49, 4.14<0.0013.65***<0.001GradeI———II1.280.80, 2.050.31.270.395III1.691.07, 2.660.0241.75*0.025Tumor Response0.500.31, 0.780.0030.41***<0.0011HR = Hazard Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "blog/posts/gtsummary/index.html#참고자료",
    "href": "blog/posts/gtsummary/index.html#참고자료",
    "title": "통계 테이블 끝판왕 gtsummary",
    "section": "참고자료",
    "text": "참고자료\nhttps://www.danieldsjoberg.com/gtsummary/reference/tbl_summary.html"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nmlr3 피처 선택\n\n\n\n\n\n\n\nR\n\n\nmlr3\n\n\nmachine learning\n\n\n\n\n머신러닝에서 최적의 피처 선택하기\n\n\n\n\n\n\nApr 28, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgroup analysis (feat. forester)\n\n\n\n\n\n\n\nR\n\n\nSubgroup analysis\n\n\nPublish\n\n\nforester\n\n\n\n\nR을 활용한 하위그룹 분석\n\n\n\n\n\n\nApr 22, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\n성향점수 매칭 (PSM)\n\n\n\n\n\n\n\nR\n\n\nPSM\n\n\nmatchit\n\n\ncobalt\n\n\n\n\nR을 활용한 성향점수 매칭하는 방법\n\n\n\n\n\n\nApr 19, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nSankey plot 그리기\n\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\n\n\nggalluvial을 통한 Sankey plot 그리기\n\n\n\n\n\n\nApr 18, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n상관관계 시각화\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nvisualization\n\n\n\n\nggplot2, ggcorrplot 를 통해 상관관계 히트맵 만들기\n\n\n\n\n\n\nApr 17, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nMICE를 이용한 multiple imputation\n\n\n\n\n\n\n\nR\n\n\nimputation\n\n\nmice\n\n\n\n\nmultiple imputation을 활용한 분석방법\n\n\n\n\n\n\nApr 11, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n주요 패키지의 성능비교\n\n\n\n\n\n\n\nR\n\n\ngroup_by\n\n\ntest\n\n\ndplyr\n\n\ndata.table\n\n\n\n\n주요 패키지들의 그룹별 계산 속도 비교\n\n\n\n\n\n\nApr 10, 2023\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMIMIC-IV 분석 준비\n\n\n\n\n\n\n\ndataset\n\n\nMIMIC-IV\n\n\nPostgreSQL\n\n\n\n\nMIMIC-IV 데이터 DB에 저장하기\n\n\n\n\n\n\nApr 5, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\n통계 테이블 끝판왕 gtsummary\n\n\n\n\n\n\n\nR\n\n\ntable\n\n\ngtsummary\n\n\n\n\n분석결과 보고를 위한 최고의 테이블 패키지\n\n\n\n\n\n\nApr 3, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nGT\n\n\n\n\n\n\n\nR\n\n\ntable\n\n\ngt\n\n\n\n\n원하는대로 테이블 꾸며주는 패키지\n\n\n\n\n\n\nApr 1, 2023\n\n\nJYH\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 하이퍼파라미터 최적화\n\n\n\n\n\n\n\nR\n\n\nmlr3\n\n\nmachine learning\n\n\n\n\nmlr3를 이용한 하이퍼파라미터 튜닝 학습\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n학습관점에서 비교하는 dplyr과 data.table\n\n\n\n\n\n\n\ndata.table\n\n\ndplyr\n\n\nR\n\n\n\n\n처음 배우는 사람에게 추천하는 패키지\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 리샘플링 벤치마킹\n\n\n\n\n\n\n\nmlr3\n\n\nR\n\n\nmachine learning\n\n\n\n\n여러 모델 동시 학습 및 성능비교\n\n\n\n\n\n\nFeb 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nstringr을 이용한 문자 추출하기\n\n\n\n\n\n\n\nR\n\n\nregex\n\n\nstringr\n\n\n\n\n정규표현식을 사용한 규칙 찾기\n\n\n\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot2 facet label 설정\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nfacet\n\n\n\n\nggplot에서 facet을 사용할 때 label을 변경하는 방법에 대해 알아봅시다.\n\n\n\n\n\n\nFeb 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 1.14.9 업데이트\n\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\n\n\ndata.table 1.14.9 버전 업데이트 내용 살펴보기.\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmlr3 기초\n\n\n\n\n\n\n\nmlr3\n\n\nR\n\n\nmachine learning\n\n\n\n\nmlr3에 대한 소개 및 mlr3를 사용하기 위한 필수문법에 대해 소개합니다.\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nImputation의 종류\n\n\n\n\n\n\n\nR\n\n\nimputation\n\n\nmice\n\n\n\n\nmice를 이용한 multiple imputation\n\n\n\n\n\n\nFeb 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggsignif: 통계적 유의성 시각화\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nvisualization\n\n\nggsignif\n\n\n\n\n그래프에 통계적 유의성(p-value) 출력\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot 세부 조정: 축 조정\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nvisualization\n\n\n\n\n그래프와 축 간격을 조정하거나 tick 간격을 변경하는 방법\n\n\n\n\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataExplorer을 활용한 EDA\n\n\n\n\n\n\n\nEDA\n\n\nR\n\n\n\n\nDataExplorer를 통한 탐색적 데이터 분석\n\n\n\n\n\n\nFeb 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidyr로 Pivoting하기\n\n\n\n\n\n\n\nR\n\n\ndplyr\n\n\ntidyr\n\n\n\n\ntidyr을 이용해 데이터의 형태를 바꾸는 pivoting에 대해 알아봅시다.\n\n\n\n\n\n\nFeb 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr 심화\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\nacross()로 column 동시 처리\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot에서 두 번째 y축 그리기\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nVisualization\n\n\naxis\n\n\n\n\nggplot으로 그래프에 두 개의 y축을 활용하는 방법을 배우기\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr 기초 문법 이해하기\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\ndplyr 필수함수 및 문법 익히기\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr group_by()\n\n\n\n\n\n\n\ndplyr\n\n\nR\n\n\n\n\ndplyr를 활용한 그룹 별 계산\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR에서 색상 다루기\n\n\n\n\n\n\n\ncolor\n\n\nR\n\n\nvisualization\n\n\n\n\n데이터 시각화를 진행할 때, 그래프에 적절한 색상을 선택하는 방법을 살펴봅시다.\n\n\n\n\n\n\nJan 25, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 심화\n\n\n\n\n\n\n\ndata.table\n\n\nR\n\n\n\n\n특수 기호, 조인, 피봇 등 data.table에서 다루는 심화내용을 살펴봅시다.\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nggplot boxplot 그리기\n\n\n\n\n\n\n\nggplot2\n\n\nR\n\n\nVisualization\n\n\nboxplot\n\n\n\n\nggplot으로 boxplot 그리는 방법\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR 기초 이해\n\n\n\n\n\n\n\nR\n\n\ndata.frame\n\n\n\n\nR 사용을 위한 필수 개념 및 함수을 배워봅시다.\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nQuarto 사용법\n\n\nQuarto, a substitute of Rmarkdown\n\n\n\n\nQuarto\n\n\nR\n\n\n\n\nBrief intorduction of Quarto\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 기초 문법\n\n\n\n\n\n\n\ndata.table\n\n\nR\n\n\n\n\ndata.table 문법, 연산자, 함수\n\n\n\n\n\n\nJan 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservable.js\n\n\n\n\n\n\n\nOJS\n\n\nVisualization\n\n\njavascript\n\n\n\n\nInteractive chart with ojs\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny에서 DB 다루기\n\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n서울 R 밋업 자료\n\n\n\n\n\n\n\n\n\n\nR을 활용한 머신러닝\n\n\n\n\n\n\nJan 1, 2022\n\n\nJYH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그래프 범례(legend) 통합하기\n\n\n\n\n\n두 가지 이상의 변수 통합\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\nKaplan-Meier 곡선\n\n\n\n\n\n\n\nR\n\n\nsurvival\n\n\nvisualization\n\n\n\n\nggsurvplot을 통한 생존분석 시각화\n\n\n\n\n\n\nJan 1, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "헬스케어 빅데이터를 다루는 데이터 사이언티스트입니다. 😃\n\nDataSkillsPapersLanguagesEducationExperience\n\n\n\nKorean national health insurance system: NHIS-NSC, NHIS-HEALS\nMedical information mart for intensive care-iv (MIMIC-IV)\nKorean sepsis alliance (KSA)\nSNUBH Clinical data warehouse (CDW)\nKorean national health and nutrition examination survey (KNHANES)\nCommunity health survey (CHS)\n\n\n\n\n\n\n\n\n\n\n\n\nSong MJ, Jang Y, Lee JH, et al. Association of Dexmedetomidine With New-Onset Atrial Fibrillation in Patients With Critical Illness. JAMA Netw Open. 2023;6(4):e239955. https://doi:10.1001/jamanetworkopen.2023.9955\nMoon, J. H., Jang, Y. H., Oh, T. J., Jung, S. Y. (2023). The Risk of Type 2 Diabetes Mellitus according to Changes in Obesity Status in Late Middle-Aged Adults: A Nationwide Cohort Study of Korea. Diabetes & Metabolism Journal, https://doi.org/10.4093/dmj.2022.0159\nJung, S. Y., Jang, Y. H., Bae, W. K., & Han, J. S. (2022). Changes of Body Mass Index and the Incidence of Hypertension in Late Middle Age: A Nationwide Cohort Study in South Korea. Korean Journal of Health Promotion, 22(4), 175–182.\nJang, Y., You, M., Lee, H. et al. Burnout and peritraumatic distress of healthcare workers in the COVID-19 pandemic. BMC Public Health 21, 2075 (2021). https://doi.org/10.1186/s12889-021-11978-0\nJang, Y., You, M., Lee, S., & Lee, W. (2021). Factors Associated With the Work Intention of Hospital Workers’ in South Korea During the Early Stages of the COVID-19 Outbreak. Disaster Medicine and Public Health Preparedness, 15(3), E23-E30. doi:10.1017/dmp.2020.221\n\n\n\n\nKOR🇰🇷: Native\nENG🇺🇸 : Intermediate\nCHN🇨🇳: Intermediate\nFRE🇫🇷: Beginner\n\n\n\n\nMPH, Graduate School of Public Health, Seoul National University\nBS, Chinese language and literature, Konkuk University\n\n\n\n\nComento | Mentor | Sept 2022 - present\nSNUBH | Data scientist | June 2021 - present"
  }
]